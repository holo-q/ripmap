
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 examples/boost_demo.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Demonstration of the boost calculator in action.
//!
//! This example shows how contextual boosts dramatically affect symbol ranking.
//! We'll create a simple scenario where multiple signals align to boost a symbol.

use std::collections::{HashMap, HashSet};
use std::sync::Arc;

use ripmap::ranking::BoostCalculator;
use ripmap::types::{RankingConfig, Tag, TagKind};

fn main() {
    println!("ğŸš€ Boost Calculator Demo\n");

    // Setup configuration with default boost multipliers
    let config = RankingConfig::default();
    let calculator = BoostCalculator::new(config.clone());

    // Create some sample tags
    let mut tags_by_file = HashMap::new();

    // File: src/auth.rs
    tags_by_file.insert(
        "/src/auth.rs".to_string(),
        vec![
            make_tag("src/auth.rs", "authenticate", TagKind::Def),
            make_tag("src/auth.rs", "validate_token", TagKind::Def),
        ],
    );

    // File: src/main.rs
    tags_by_file.insert(
        "/src/main.rs".to_string(),
        vec![
            make_tag("src/main.rs", "main", TagKind::Def),
            make_tag("src/main.rs", "handle_request", TagKind::Def),
        ],
    );

    // Base PageRank scores (from graph structure)
    let mut file_ranks = HashMap::new();
    file_ranks.insert("src/auth.rs".to_string(), 0.3);
    file_ranks.insert("src/main.rs".to_string(), 0.5);

    let mut symbol_ranks = HashMap::new();
    symbol_ranks.insert((Arc::from("src/auth.rs"), Arc::from("authenticate")), 0.4);
    symbol_ranks.insert((Arc::from("src/auth.rs"), Arc::from("validate_token")), 0.2);
    symbol_ranks.insert((Arc::from("src/main.rs"), Arc::from("main")), 0.6);
    symbol_ranks.insert((Arc::from("src/main.rs"), Arc::from("handle_request")), 0.5);

    println!("ğŸ“Š Base PageRank Scores:");
    println!("  authenticate:     0.4");
    println!("  validate_token:   0.2");
    println!("  main:             0.6");
    println!("  handle_request:   0.5\n");

    // Scenario 1: No contextual signals
    println!("ğŸ”µ Scenario 1: No Contextual Signals");
    println!("   (Just base PageRank)\n");

    let result1 = calculator.apply_boosts(
        &tags_by_file,
        &file_ranks,
        Some(&symbol_ranks),
        &HashSet::new(),
        &HashSet::new(),
        &HashSet::new(),
        &HashSet::new(),
        None,
        None,
        None,
    );

    print_results(&result1);

    // Scenario 2: User mentions "authenticate" in query
    println!("\nğŸŸ¢ Scenario 2: User Asks About 'authenticate'");
    println!("   Boost: 10x (mentioned_ident)\n");

    let mut mentioned_idents = HashSet::new();
    mentioned_idents.insert("authenticate".to_string());

    let result2 = calculator.apply_boosts(
        &tags_by_file,
        &file_ranks,
        Some(&symbol_ranks),
        &HashSet::new(),
        &HashSet::new(),
        &mentioned_idents,
        &HashSet::new(),
        None,
        None,
        None,
    );

    print_results(&result2);

    // Scenario 3: Editing auth.rs, mentions "authenticate"
    println!("\nğŸŸ¡ Scenario 3: Editing auth.rs + Mentions 'authenticate'");
    println!("   Boosts: 10x (mentioned_ident) Ã— 20x (chat_file) = 200x!\n");

    let mut chat_fnames = HashSet::new();
    chat_fnames.insert("/src/auth.rs".to_string());

    let result3 = calculator.apply_boosts(
        &tags_by_file,
        &file_ranks,
        Some(&symbol_ranks),
        &chat_fnames,
        &HashSet::new(),
        &mentioned_idents,
        &HashSet::new(),
        None,
        None,
        None,
    );

    print_results(&result3);

    // Scenario 4: All signals align!
    println!("\nğŸ”´ Scenario 4: ALL SIGNALS ALIGN!");
    println!("   - Editing auth.rs (20x chat_file)");
    println!("   - Mentions 'authenticate' (10x mentioned_ident)");
    println!("   - Mentions auth.rs (5x mentioned_file)");
    println!("   - auth.rs co-changes with chat (3x temporal_coupling)");
    println!("   - Recent git activity (2x git_weight)");
    println!("   - Caller analysis boost (1.5x caller_weight)");
    println!("   Total: 20 Ã— 10 Ã— 5 Ã— 3 Ã— 2 Ã— 1.5 = 9000x BASE RANK!\n");

    let mut mentioned_fnames = HashSet::new();
    mentioned_fnames.insert("src/auth.rs".to_string());

    let mut temporal_boost_files = HashSet::new();
    temporal_boost_files.insert("src/auth.rs".to_string());

    let mut git_weights = HashMap::new();
    git_weights.insert("src/auth.rs".to_string(), 2.0);

    let mut caller_weights = HashMap::new();
    caller_weights.insert("src/auth.rs".to_string(), 1.5);

    let result4 = calculator.apply_boosts(
        &tags_by_file,
        &file_ranks,
        Some(&symbol_ranks),
        &chat_fnames,
        &mentioned_fnames,
        &mentioned_idents,
        &temporal_boost_files,
        Some(&git_weights),
        Some(&caller_weights),
        None,
    );

    print_results(&result4);

    println!("\nâœ¨ Key Insight:");
    println!("   When multiple contextual signals align, the boost system creates");
    println!("   EXPLOSIVE amplification, ensuring the most relevant symbol rises");
    println!("   to the top regardless of its structural PageRank.");
    println!("\n   This is the magic of multiplicative boosts! ğŸ¯");
}

/// Helper to create a test tag
fn make_tag(rel_fname: &str, name: &str, kind: TagKind) -> Tag {
    Tag {
        rel_fname: Arc::from(rel_fname),
        fname: Arc::from(format!("/{}", rel_fname)),
        line: 1,
        name: Arc::from(name),
        kind,
        node_type: Arc::from("function"),
        parent_name: None,
        parent_line: None,
        signature: None,
        fields: None,
        metadata: None,
    }
}

/// Pretty-print ranked results
fn print_results(results: &[ripmap::types::RankedTag]) {
    println!("   Ranking:");
    for (i, ranked) in results.iter().enumerate() {
        println!(
            "   {}. {:20} (rank: {:.2})",
            i + 1,
            ranked.tag.name.as_ref(),
            ranked.rank
        );
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 examples/demo_discovery.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Demo of the file discovery module.

use std::path::Path;
use ripmap::discovery::find_source_files;

fn main() -> anyhow::Result<()> {
    let cwd = Path::new(".");
    
    println!("ğŸ” Discovering source files in ripmap codebase...\n");
    
    let files = find_source_files(cwd, false)?;
    
    println!("âœ“ Found {} source files\n", files.len());
    
    // Group by extension
    let mut by_ext = std::collections::HashMap::new();
    for file in &files {
        if let Some(ext) = file.extension() {
            if let Some(ext_str) = ext.to_str() {
                *by_ext.entry(ext_str.to_string()).or_insert(0) += 1;
            }
        }
    }
    
    println!("Files by extension:");
    let mut exts: Vec<_> = by_ext.iter().collect();
    exts.sort_by_key(|(_, count)| std::cmp::Reverse(**count));
    for (ext, count) in exts {
        println!("  .{}: {}", ext, count);
    }
    
    println!("\nFirst 15 files:");
    for (i, file) in files.iter().take(15).enumerate() {
        println!("  {}. {}", i + 1, file.strip_prefix(cwd).unwrap_or(file).display());
    }
    
    if files.len() > 15 {
        println!("  ... and {} more", files.len() - 15);
    }
    
    // Verify gitignore is working
    println!("\nğŸ”’ Gitignore verification:");
    let has_target = files.iter().any(|f| f.to_string_lossy().contains("target/"));
    let has_lock = files.iter().any(|f| f.to_string_lossy().ends_with("Cargo.lock"));
    println!("  Excludes target/: {}", !has_target);
    println!("  Excludes Cargo.lock: {}", !has_lock);
    
    Ok(())
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 examples/demo_focus.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Demo: Focus resolver - fuzzy matching and graph expansion
//!
//! This example demonstrates how the FocusResolver enables targeted
//! exploration of large codebases through natural queries.
//!
//! Run with: cargo run --example demo_focus

use std::collections::HashMap;
use std::sync::Arc;

use ripmap::ranking::FocusResolver;
use ripmap::types::{Tag, TagKind};

fn main() {
    println!("=== Focus Resolver Demo ===\n");

    // Create a mock codebase with authentication and parsing modules
    let (tags_by_file, symbol_graph) = create_mock_codebase();

    let resolver = FocusResolver::new("/project");

    // Demo 1: Fuzzy symbol matching
    println!("1. Fuzzy Symbol Matching");
    println!("   Query: 'auth'");
    let (_files, idents) = resolver.resolve(&["auth".to_string()], &tags_by_file);
    println!("   Matched identifiers:");
    for ident in &idents {
        println!("     - {}", ident);
    }
    println!();

    // Demo 2: File matching
    println!("2. File Matching");
    println!("   Query: 'auth.rs'");
    let (files, _idents) = resolver.resolve(&["auth.rs".to_string()], &tags_by_file);
    println!("   Matched files:");
    for file in &files {
        println!("     - {}", file);
    }
    println!();

    // Demo 3: Comma-separated targets
    println!("3. Multi-target Query");
    println!("   Query: 'parse,config.rs'");
    let (files, idents) = resolver.resolve(&["parse,config.rs".to_string()], &tags_by_file);
    println!("   Matched files: {}", files.len());
    println!("   Matched identifiers: {}", idents.len());
    for ident in &idents {
        println!("     - {}", ident);
    }
    println!();

    // Demo 4: Graph expansion
    println!("4. Graph Expansion (BFS with decay)");
    println!("   Starting from: 'authenticate'");
    let mut seed_idents = std::collections::HashSet::new();
    seed_idents.insert("authenticate".to_string());

    let expanded = resolver.expand_via_graph(&seed_idents, &symbol_graph, 2, 0.5);

    println!("   Expanded symbols (weight = decay^hop_distance):");
    let mut expanded_vec: Vec<_> = expanded.iter().collect();
    expanded_vec.sort_by(|a, b| b.1.partial_cmp(a.1).unwrap());
    for ((file, symbol), weight) in expanded_vec {
        println!("     - {} in {} (weight: {:.2})", symbol, file, weight);
    }
    println!();

    // Demo 5: Typo tolerance
    println!("5. Typo Tolerance");
    println!("   Query: 'authentcate' (missing 'i')");
    let (_files, idents) = resolver.resolve(&["authentcate".to_string()], &tags_by_file);
    println!("   Matched identifiers:");
    for ident in &idents {
        println!("     - {}", ident);
    }
    println!();

    // Demo 6: Stem matching
    println!("6. Stem Matching (morphological variants)");
    println!("   Query: 'valid'");
    let (_files, idents) = resolver.resolve(&["valid".to_string()], &tags_by_file);
    println!("   Matched identifiers:");
    for ident in &idents {
        println!("     - {}", ident);
    }
    println!();

    println!("=== Demo Complete ===");
}

/// Create a mock codebase for demonstration purposes.
///
/// Returns:
/// - tags_by_file: Map of file paths to tags
/// - symbol_graph: Call graph edges (from_file, from_sym, to_file, to_sym)
fn create_mock_codebase() -> (
    HashMap<String, Vec<Tag>>,
    Vec<(Arc<str>, Arc<str>, Arc<str>, Arc<str>)>,
) {
    let mut tags_by_file = HashMap::new();

    // auth.rs - Authentication module
    tags_by_file.insert(
        "/project/src/auth.rs".to_string(),
        vec![
            Tag {
                rel_fname: "src/auth.rs".into(),
                fname: "/project/src/auth.rs".into(),
                line: 10,
                name: "authenticate".into(),
                kind: TagKind::Def,
                node_type: "function".into(),
                parent_name: None,
                parent_line: None,
                signature: None,
                fields: None,
                metadata: None,
            },
            Tag {
                rel_fname: "src/auth.rs".into(),
                fname: "/project/src/auth.rs".into(),
                line: 20,
                name: "authorize".into(),
                kind: TagKind::Def,
                node_type: "function".into(),
                parent_name: None,
                parent_line: None,
                signature: None,
                fields: None,
                metadata: None,
            },
            Tag {
                rel_fname: "src/auth.rs".into(),
                fname: "/project/src/auth.rs".into(),
                line: 30,
                name: "validate_token".into(),
                kind: TagKind::Def,
                node_type: "function".into(),
                parent_name: None,
                parent_line: None,
                signature: None,
                fields: None,
                metadata: None,
            },
        ],
    );

    // parser.rs - Parsing module
    tags_by_file.insert(
        "/project/src/parser.rs".to_string(),
        vec![
            Tag {
                rel_fname: "src/parser.rs".into(),
                fname: "/project/src/parser.rs".into(),
                line: 10,
                name: "parse".into(),
                kind: TagKind::Def,
                node_type: "function".into(),
                parent_name: None,
                parent_line: None,
                signature: None,
                fields: None,
                metadata: None,
            },
            Tag {
                rel_fname: "src/parser.rs".into(),
                fname: "/project/src/parser.rs".into(),
                line: 20,
                name: "parser_error".into(),
                kind: TagKind::Def,
                node_type: "function".into(),
                parent_name: None,
                parent_line: None,
                signature: None,
                fields: None,
                metadata: None,
            },
        ],
    );

    // config.rs - Configuration module
    tags_by_file.insert(
        "/project/src/config.rs".to_string(),
        vec![
            Tag {
                rel_fname: "src/config.rs".into(),
                fname: "/project/src/config.rs".into(),
                line: 10,
                name: "load_config".into(),
                kind: TagKind::Def,
                node_type: "function".into(),
                parent_name: None,
                parent_line: None,
                signature: None,
                fields: None,
                metadata: None,
            },
            Tag {
                rel_fname: "src/config.rs".into(),
                fname: "/project/src/config.rs".into(),
                line: 20,
                name: "validate_config".into(),
                kind: TagKind::Def,
                node_type: "function".into(),
                parent_name: None,
                parent_line: None,
                signature: None,
                fields: None,
                metadata: None,
            },
        ],
    );

    // validator.rs - Validation module
    tags_by_file.insert(
        "/project/src/validator.rs".to_string(),
        vec![Tag {
            rel_fname: "src/validator.rs".into(),
            fname: "/project/src/validator.rs".into(),
            line: 10,
            name: "validate".into(),
            kind: TagKind::Def,
            node_type: "function".into(),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        }],
    );

    // Build symbol graph - who calls whom?
    // authenticate -> validate_token -> validate
    // load_config -> validate_config -> validate
    // parse -> parser_error
    let symbol_graph = vec![
        // authenticate calls validate_token
        (
            Arc::from("/project/src/auth.rs"),
            Arc::from("authenticate"),
            Arc::from("/project/src/auth.rs"),
            Arc::from("validate_token"),
        ),
        // validate_token calls validate
        (
            Arc::from("/project/src/auth.rs"),
            Arc::from("validate_token"),
            Arc::from("/project/src/validator.rs"),
            Arc::from("validate"),
        ),
        // load_config calls validate_config
        (
            Arc::from("/project/src/config.rs"),
            Arc::from("load_config"),
            Arc::from("/project/src/config.rs"),
            Arc::from("validate_config"),
        ),
        // validate_config calls validate
        (
            Arc::from("/project/src/config.rs"),
            Arc::from("validate_config"),
            Arc::from("/project/src/validator.rs"),
            Arc::from("validate"),
        ),
        // parse calls parser_error
        (
            Arc::from("/project/src/parser.rs"),
            Arc::from("parse"),
            Arc::from("/project/src/parser.rs"),
            Arc::from("parser_error"),
        ),
    ];

    (tags_by_file, symbol_graph)
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 examples/extract_tags.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Example demonstrating tag extraction from source files.
//!
//! This shows how to use the extraction pipeline to parse files
//! and extract symbol definitions using regex-based patterns.

use ripmap::extraction::{Parser, extract_tags};
use std::path::Path;

fn main() {
    let parser = Parser::new();
    
    // Example 1: Extract from a Python file
    println!("=== Python Example ===");
    if let Ok(tags) = extract_tags(Path::new("examples/sample.py"), "examples/sample.py", &parser) {
        for tag in &tags {
            println!("  {}:{} - {} ({})", tag.rel_fname, tag.line, tag.name, tag.node_type);
        }
        println!("Total tags: {}\n", tags.len());
    }
    
    // Example 2: Extract from a Rust file
    println!("=== Rust Example (this file) ===");
    if let Ok(tags) = extract_tags(Path::new("examples/extract_tags.rs"), "examples/extract_tags.rs", &parser) {
        for tag in &tags {
            println!("  {}:{} - {} ({})", tag.rel_fname, tag.line, tag.name, tag.node_type);
        }
        println!("Total tags: {}\n", tags.len());
    }
    
    // Example 3: Extract from JavaScript file
    println!("=== JavaScript Example ===");
    if let Ok(tags) = extract_tags(Path::new("examples/sample.js"), "examples/sample.js", &parser) {
        for tag in &tags {
            println!("  {}:{} - {} ({})", tag.rel_fname, tag.line, tag.name, tag.node_type);
        }
        println!("Total tags: {}", tags.len());
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 examples/test_pagerank.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

use ripmap::ranking::PageRanker;
use ripmap::types::{RankingConfig, Tag, TagKind};
use std::collections::HashMap;
use std::sync::Arc;

fn make_tag(rel_fname: &str, name: &str, kind: TagKind) -> Tag {
    Tag {
        rel_fname: Arc::from(rel_fname),
        fname: Arc::from(format!("/{}", rel_fname)),
        line: 1,
        name: Arc::from(name),
        kind,
        node_type: Arc::from("function"),
        parent_name: None,
        parent_line: None,
        signature: None,
        fields: None,
        metadata: None,
    }
}

fn main() {
    let config = RankingConfig::default();
    let ranker = PageRanker::new(config);

    // Create a simple graph:
    // a.rs defines "foo"
    // b.rs references "foo" (b -> a)
    // c.rs references "foo" (c -> a)
    // a.rs should have highest rank (referenced by both)

    let mut tags_by_file = HashMap::new();
    tags_by_file.insert(
        "/a.rs".to_string(),
        vec![make_tag("a.rs", "foo", TagKind::Def)],
    );
    tags_by_file.insert(
        "/b.rs".to_string(),
        vec![make_tag("b.rs", "foo", TagKind::Ref)],
    );
    tags_by_file.insert(
        "/c.rs".to_string(),
        vec![make_tag("c.rs", "foo", TagKind::Ref)],
    );

    let chat_fnames = vec![];
    let ranks = ranker.compute_ranks(&tags_by_file, &chat_fnames);

    println!("PageRank results:");
    for (file, rank) in &ranks {
        println!("  {}: {:.6}", file, rank);
    }

    let total: f64 = ranks.values().sum();
    println!("\nTotal rank: {:.6}", total);
    println!("Expected: ~1.0");

    // Verify rankings
    if ranks["a.rs"] > ranks["b.rs"] && ranks["a.rs"] > ranks["c.rs"] {
        println!("\nâœ“ a.rs has highest rank (as expected)");
    } else {
        println!("\nâœ— ERROR: a.rs should have highest rank!");
    }

    if (total - 1.0).abs() < 0.01 {
        println!("âœ“ Total rank sums to 1.0 (as expected)");
    } else {
        println!("âœ— ERROR: Total rank should sum to 1.0!");
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 examples/test_ripmap_itself.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Test extraction on the ripmap codebase itself.

use ripmap::extraction::{Parser, extract_tags};
use std::path::Path;

fn main() {
    let parser = Parser::new();
    
    println!("=== Extracting from ripmap's own types.rs ===\n");
    
    if let Ok(tags) = extract_tags(
        Path::new("src/types.rs"),
        "src/types.rs",
        &parser
    ) {
        // Group by node type
        let mut by_type: std::collections::HashMap<&str, Vec<_>> = std::collections::HashMap::new();
        for tag in &tags {
            by_type.entry(tag.node_type.as_ref()).or_default().push(tag);
        }
        
        // Display by category
        for (node_type, tags) in by_type.iter() {
            println!("{} (count: {}):", node_type, tags.len());
            for tag in tags.iter().take(5) {
                println!("  - {} (line {})", tag.name, tag.line);
            }
            if tags.len() > 5 {
                println!("  ... and {} more", tags.len() - 5);
            }
            println!();
        }
        
        println!("Total symbols extracted: {}", tags.len());
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 examples/training_extraction.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Benchmark the extraction performance.

use ripmap::extraction::{Parser, extract_tags};
use std::path::PathBuf;
use std::time::Instant;

fn main() {
    let parser = Parser::new();
    
    // Collect all Rust files in src/
    let files: Vec<PathBuf> = walkdir::WalkDir::new("src")
        .into_iter()
        .filter_map(|e| e.ok())
        .filter(|e| e.path().extension().and_then(|s| s.to_str()) == Some("rs"))
        .map(|e| e.path().to_path_buf())
        .collect();
    
    println!("Found {} Rust files in src/", files.len());
    println!("Starting extraction training...\n");
    
    let start = Instant::now();
    let mut total_tags = 0;
    
    for path in &files {
        if let Ok(tags) = extract_tags(path, path.to_str().unwrap(), &parser) {
            total_tags += tags.len();
        }
    }
    
    let elapsed = start.elapsed();
    
    println!("Results:");
    println!("  Files processed: {}", files.len());
    println!("  Total tags extracted: {}", total_tags);
    println!("  Time elapsed: {:.2?}", elapsed);
    println!("  Avg per file: {:.2?}", elapsed / files.len() as u32);
    println!("  Tags per second: {:.0}", total_tags as f64 / elapsed.as_secs_f64());
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/bin/ripmap-mcp.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! ripmap MCP server binary.
//!
//! Runs the ripmap codebase cartography tool as an MCP server over stdio.
//! This enables AI assistants like Claude to invoke ripmap for understanding
//! codebase structure.
//!
//! # Usage
//!
//! ```bash
//! ripmap-mcp
//! ```
//!
//! The server communicates via JSON-RPC over stdio and provides the
//! `grep_map` tool for generating topology-aware structural maps.

use anyhow::Result;
use rmcp::{transport::stdio, ServiceExt};
use ripmap::mcp::RipmapServer;

#[tokio::main]
async fn main() -> Result<()> {
    // Initialize and run the MCP server over stdio
    let service = RipmapServer::new().serve(stdio()).await?;

    // Wait for the service to complete (runs until client disconnects)
    service.waiting().await?;

    Ok(())
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/bin/ripmap-train-outer.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! ripmap-train-outer: L2 Promptgram Optimizer
//!
//! The outer loop that evolves prompts for the inner reasoning-based optimizer.
//!
//! ## Usage
//!
//! ```bash
//! # Run Stage 0: baseline outer loop (no prompt edits, just recording)
//! ripmap-train-outer --outer-steps 10 --inner-episodes 20 --run-name test_outer
//!
//! # Run with a specific inner promptgram
//! ripmap-train-outer --promptgram training-outer/prompts/inner/explorer.toml
//!
//! # Full outer loop with prompt editing (Stage 1+)
//! ripmap-train-outer --outer-steps 50 --inner-episodes 30 --edit-prompts
//! ```
//!
//! ## Directory Structure
//!
//! ```text
//! training-outer/runs/<run-name>/
//!   config.toml           # Outer run configuration
//!   outer_scratchpad.json # L2 scratchpad
//!   inner_runs/
//!     step_001/           # Inner run for outer step 1
//!       scratchpad.json
//!       results.json
//!       progress.png
//!     step_002/
//!       ...
//! ```

use std::path::PathBuf;
use clap::Parser;
use ripmap::training::reasoning::Agent;
use ripmap::training_outer::{
    OuterLoop, OuterScratchpad, OuterConfig, RunConfig, RunContext,
    Promptgram, baseline_promptgram,
};

#[derive(Parser)]
#[command(name = "ripmap-train-outer")]
#[command(about = "L2 Promptgram Optimizer - evolves prompts for the inner optimizer")]
struct Args {
    /// Number of outer steps to run
    #[arg(long, default_value = "10")]
    outer_steps: usize,

    /// Number of inner episodes per outer step
    #[arg(long, default_value = "20")]
    inner_episodes: usize,

    /// Name for this outer run
    #[arg(long, default_value = "outer_run")]
    run_name: String,

    /// Agent to use for inner loop (claude, gemini, codex)
    #[arg(long, default_value = "claude")]
    inner_agent: String,

    /// Agent to use for outer loop reasoning
    #[arg(long, default_value = "codex")]
    outer_agent: String,

    /// Path to inner promptgram (optional, uses default if not specified)
    #[arg(long)]
    promptgram: Option<PathBuf>,

    /// Enable prompt editing (Stage 1+). Without this, runs Stage 0 (recording only).
    #[arg(long)]
    edit_prompts: bool,

    /// Resume from existing outer scratchpad
    #[arg(long)]
    resume: bool,

    /// Corpus to use: quick, curated, or full
    #[arg(long, default_value = "curated")]
    corpus: String,

    /// Save interval for checkpoints
    #[arg(long, default_value = "1")]
    save_interval: usize,
}

fn main() -> Result<(), Box<dyn std::error::Error>> {
    use owo_colors::OwoColorize;

    let args = Args::parse();

    println!();
    println!("{}", " RIPMAP L2 OUTER LOOP OPTIMIZER ".bold().on_magenta());
    println!();

    // Parse agents
    let inner_agent: Agent = args.inner_agent.parse()
        .map_err(|e: String| e)?;
    let _outer_agent: Agent = args.outer_agent.parse()
        .map_err(|e: String| e)?;

    // Set up run configuration
    let run_config = RunConfig {
        run_name: args.run_name.clone(),
        base_dir: PathBuf::from("training-outer/runs"),
        episodes: args.inner_episodes,
        agent: inner_agent,
        model: None,
        save_interval: args.save_interval,
    };

    // Create run context
    let ctx = RunContext {
        config: run_config.clone(),
        episode: 0,
        is_outer: true,
        parent_step: None,
    };

    // Set up directories
    ctx.setup_dirs()?;

    // Load or create outer scratchpad
    let scratchpad_path = ctx.config.output_dir().join("outer_scratchpad.json");
    let mut outer_scratchpad = if args.resume && scratchpad_path.exists() {
        println!("ğŸ“‚ Resuming from existing scratchpad...");
        let content = std::fs::read_to_string(&scratchpad_path)?;
        serde_json::from_str(&content)?
    } else {
        OuterScratchpad::default()
    };

    // Load or create inner promptgram
    let inner_promptgram = if let Some(path) = &args.promptgram {
        println!("ğŸ“„ Loading promptgram from {:?}", path);
        Promptgram::load(path)?
    } else {
        println!("ğŸ“„ Using default inner promptgram");
        baseline_promptgram()
    };

    // Set up outer loop
    let mut outer_loop = OuterLoop::new();
    outer_loop.population = vec![inner_promptgram];
    outer_loop.inner_config = OuterConfig {
        inner_episodes: args.inner_episodes,
        inner_agent: args.inner_agent.clone(),
        outer_agent: args.outer_agent.clone(),
        max_outer_steps: args.outer_steps,
        exploration_quota: 0.2,
        corpus: args.corpus.clone(),
        edit_prompts: args.edit_prompts,
        ..Default::default()
    };

    // Print configuration
    println!("Configuration:");
    println!("  Run name:       {}", args.run_name);
    println!("  Outer steps:    {}", args.outer_steps);
    println!("  Inner episodes: {}", args.inner_episodes);
    println!("  Inner agent:    {}", args.inner_agent);
    println!("  Outer agent:    {}", args.outer_agent);
    println!("  Corpus:         {}", args.corpus);
    println!("  Edit prompts:   {}", args.edit_prompts);
    println!("  Output dir:     {:?}", ctx.config.output_dir());
    println!();

    // Determine starting step
    let start_step = outer_scratchpad.episodes.len();
    if start_step > 0 {
        println!("ğŸ“Š Resuming from step {} (best NDCG: {:.4})",
            start_step, outer_scratchpad.best_ndcg);
    }

    // Save initial config
    let config_path = ctx.config.output_dir().join("config.toml");
    let config_content = format!(r#"# L2 Outer Loop Configuration
[run]
name = "{}"
outer_steps = {}
inner_episodes = {}
inner_agent = "{}"
outer_agent = "{}"
corpus = "{}"
edit_prompts = {}
"#,
        args.run_name,
        args.outer_steps,
        args.inner_episodes,
        args.inner_agent,
        args.outer_agent,
        args.corpus,
        args.edit_prompts,
    );
    std::fs::write(&config_path, config_content)?;

    // Run outer loop
    println!("{}", "â”€".repeat(65));
    println!("Starting outer loop ({} steps)...\n", args.outer_steps - start_step);

    for step in start_step..args.outer_steps {
        println!("{}", format!(" OUTER STEP {}/{} ", step + 1, args.outer_steps).bold().on_cyan());

        // Run one outer episode
        match outer_loop.run_outer_episode(step + 1, &ctx, &mut outer_scratchpad) {
            Ok(summary) => {
                // Print summary
                println!("\n  ğŸ“ˆ Results:");
                println!("     NDCG: {:.4} â†’ {:.4} (Î”{:+.4})",
                    summary.baseline_metrics.ndcg,
                    summary.final_metrics.ndcg,
                    summary.delta.ndcg);
                println!("     Failures: {} â†’ {}",
                    summary.baseline_metrics.failures,
                    summary.final_metrics.failures);
                println!("     Duration: {:.1}s", summary.duration_secs);
                println!("     Stability: {} collapses, {:.4} variance",
                    summary.stability.collapse_events,
                    summary.stability.ndcg_variance);

                // Print meta-levers
                println!("     Meta-levers: {}", summary.meta_levers_estimate.summary());

                // Print strategy capsules (if any)
                if !summary.strategy_capsules.is_empty() {
                    println!("     Strategy capsules:");
                    for capsule in summary.strategy_capsules.iter().take(3) {
                        println!("       â€¢ {}", capsule.chars().take(60).collect::<String>());
                    }
                }

                // Update best if improved
                if summary.final_metrics.ndcg > outer_scratchpad.best_ndcg {
                    println!("\n  ğŸ† NEW BEST! {:.4} â†’ {:.4}",
                        outer_scratchpad.best_ndcg, summary.final_metrics.ndcg);
                }
            }
            Err(e) => {
                eprintln!("\n  âŒ Error in outer step: {}", e);
                // Continue to next step
            }
        }

        // Save scratchpad after each step
        let scratchpad_json = serde_json::to_string_pretty(&outer_scratchpad)?;
        std::fs::write(&scratchpad_path, &scratchpad_json)?;
        println!("\n  ğŸ’¾ Checkpoint saved\n");
    }

    // Print final summary
    println!();
    println!("{}", "â”€".repeat(65));
    println!("{}", " OUTER LOOP COMPLETE ".bold().on_green());
    println!("{}", "â”€".repeat(65));
    println!();

    println!("Total outer steps: {}", outer_scratchpad.episodes.len());
    println!("Best NDCG: {:.4} (prompt: {})",
        outer_scratchpad.best_ndcg, outer_scratchpad.best_prompt_id);

    if !outer_scratchpad.episodes.is_empty() {
        let first = outer_scratchpad.episodes.first().unwrap();
        let last = outer_scratchpad.episodes.last().unwrap();
        println!("NDCG trajectory: {:.4} â†’ {:.4}",
            first.baseline_metrics.ndcg, last.final_metrics.ndcg);

        // Print strategy capsule diversity
        let all_capsules: Vec<_> = outer_scratchpad.episodes.iter()
            .flat_map(|e| e.strategy_capsules.iter())
            .collect();
        println!("Total strategy capsules: {}", all_capsules.len());

        // Print structural insights
        let all_insights: std::collections::HashSet<_> = outer_scratchpad.episodes.iter()
            .flat_map(|e| e.structural_insights.iter())
            .collect();
        if !all_insights.is_empty() {
            println!("\nStructural insights discovered:");
            for insight in all_insights.iter().take(5) {
                println!("  â€¢ {}", insight);
            }
        }
    }

    println!("\nOutputs saved to: {:?}", ctx.config.output_dir());

    Ok(())
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/bin/ripmap-train.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Benchmark CLI for hyperparameter optimization.
//!
//! # Usage
//!
//! ```bash
//! # Single repo for testing
//! ripmap-train --repo /path/to/repo --budget 50
//!
//! # Quick corpus (5 repos) for dev iteration
//! ripmap-train --corpus quick --strategy bayesian --budget 100
//!
//! # Full curated corpus for training
//! ripmap-train --corpus curated --strategy bayesian --budget 500
//!
//! # Reasoning-based training (uses LLM to analyze failures)
//! ripmap-train --corpus curated --reason --prompt training-outer/prompts/inner/v001.md --episodes 50
//!
//! # Sensitivity analysis on best config
//! ripmap-train --repo /path/to/repo --sensitivity --config best_config.json
//! ```
//!
//! # Output
//!
//! The benchmark produces:
//! - `results.json`: Full results with all parameter evaluations
//! - `best_config.json`: Optimal RankingConfig for production use
//! - `sensitivity.json`: Parameter importance analysis
//! - Console summary with top configurations and insights

use std::fs::File;
use std::path::{Path, PathBuf};
use std::sync::Arc;
use std::time::Instant;

use clap::Parser;
use rand::SeedableRng;
use rayon::prelude::*;
use chrono::{DateTime, Local, TimeZone};
use serde::{Deserialize, Serialize};

use ripmap::training::{
    extract_cases, compute_coupling_weights, weight_cases,
    WeightedCase, EvalMetrics, CaseMetrics,
    ParameterPoint, ParameterGrid, SearchStrategy, sample_points, bayesian_next_sample,
    SensitivityAnalysis, full_analysis, print_summary,
    CURATED_REPOS, quick_repos, RepoSpec,
    // Reasoning-based training
    Agent, RankingFailure, Scratchpad,
    reason_about_failures, update_scratchpad, apply_changes,
    distill_scratchpad, print_scratchpad_summary,
};

#[derive(Parser, Debug)]
#[command(name = "ripmap-bench")]
#[command(about = "Hyperparameter optimization for ripmap ranking")]
struct Args {
    /// Path to a single repository to benchmark
    #[arg(long)]
    repo: Option<PathBuf>,

    /// Corpus to use: quick (5 repos), curated (full set), or a path to custom list
    /// Examples: --corpus quick, --corpus curated, --corpus ./my_repos.txt
    #[arg(long)]
    corpus: Option<String>,

    /// Search strategy: grid, lhs, random, bayesian
    #[arg(long, default_value = "lhs")]
    strategy: String,

    /// Number of parameter configurations to evaluate
    #[arg(long, default_value = "100")]
    budget: usize,

    /// Random seed for reproducibility
    #[arg(long, default_value = "42")]
    seed: u64,

    /// Output file for results
    #[arg(long, default_value = "training/runs/default/results.json")]
    output: PathBuf,

    /// Run sensitivity analysis after optimization
    #[arg(long)]
    sensitivity: bool,

    /// Load existing config for sensitivity analysis
    #[arg(long)]
    config: Option<PathBuf>,

    /// Only extract training cases, don't optimize
    #[arg(long)]
    extract_only: bool,

    /// Maximum commits to analyze per repo
    #[arg(long, default_value = "500")]
    max_commits: usize,

    /// Minimum files per commit for training case
    #[arg(long, default_value = "2")]
    min_files: usize,

    /// Maximum files per commit for training case
    #[arg(long, default_value = "12")]
    max_files: usize,

    /// Directory to clone curated repos into
    #[arg(long, default_value = "./training/corpus")]
    clone_dir: PathBuf,

    /// Verbose output
    #[arg(short, long)]
    verbose: bool,

    /// Path to semantic distractors JSON (generated by Claude)
    /// Format: [{"repo": "name", "cases": [{"seed_file": "...", "semantic_distractors": [...]}]}]
    #[arg(long)]
    distractors: Option<PathBuf>,

    // === Reasoning-Based Training ===

    /// Path to prompt template file (required for --reason mode)
    /// Contains placeholders: {current_ndcg:.4}, {episode_num}, {episode_history}, {params_desc}, {failure_desc}
    #[arg(long)]
    prompt: Option<PathBuf>,

    /// Use reasoning-based training via Claude (universal function approximator)
    /// Instead of blind parameter search, Claude analyzes WHY rankings fail
    #[arg(long)]
    reason: bool,

    /// Number of reasoning episodes to run
    #[arg(long, default_value = "10")]
    episodes: usize,

    /// NDCG threshold below which a case is considered a failure
    #[arg(long, default_value = "0.5")]
    failure_threshold: f64,

    /// Path to scratchpad file (accumulated insights)
    #[arg(long, default_value = "training/runs/default/scratchpad.json")]
    scratchpad: PathBuf,

    /// Distill scratchpad into operator wisdom (presets, heuristics, warnings)
    #[arg(long)]
    distill: bool,

    /// Generate training progress chart (requires --features plotters)
    #[arg(long)]
    plot: Option<PathBuf>,

    /// LLM agent to use for reasoning: 'claude' (default), 'gemini', or 'codex'
    #[arg(long, default_value = "claude")]
    agent: String,

    /// Model to use for the agent (e.g., 'opus', 'o3', 'gemini-2.0-flash')
    /// Defaults: claude=sonnet, gemini=gemini-2.0-flash, codex=o3
    #[arg(long, short = 'm')]
    model: Option<String>,

    /// Save scratchpad/params after every N episodes (for crash recovery)
    /// Default: 1 (save every episode)
    #[arg(long, default_value = "1")]
    save_interval: usize,

    /// Name for this training run (creates training/runs/<name>/)
    /// If provided, overrides --output, --scratchpad, --plot paths
    #[arg(long)]
    run_name: Option<String>,

    /// Show a rich visualization of a past training run
    /// Usage: --show <run-name> (looks in training/runs/<run-name>/)
    /// Or: --show <path-to-scratchpad.json>
    #[arg(long)]
    show: Option<String>,

    /// Pivot view: show structural insights as primary axis (insight-first, not episode-first)
    #[arg(long)]
    show_insights: Option<String>,

    /// Pivot view: show parameter interactions as primary axis
    #[arg(long)]
    show_interactions: Option<String>,

    /// List all available training runs
    #[arg(long)]
    list: bool,
}

/// Semantic distractors generated by Claude.
/// These are plausible-but-wrong file paths that create a realistic ranking challenge.
#[derive(Debug, Clone, Serialize, Deserialize)]
struct SemanticDistractorCase {
    seed_file: String,
    #[serde(default)]
    expected_related: Vec<String>,
    #[serde(default)]
    commit_message: String,
    semantic_distractors: Vec<String>,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
struct SemanticDistractorRepo {
    repo: String,
    n_cases: usize,
    cases: Vec<SemanticDistractorCase>,
}

/// Lookup table: seed_file -> semantic distractors
type DistractorLookup = std::collections::HashMap<String, Vec<String>>;

fn load_distractors(path: &Path) -> anyhow::Result<DistractorLookup> {
    let file = File::open(path)?;
    let repos: Vec<SemanticDistractorRepo> = serde_json::from_reader(file)?;

    let mut lookup = DistractorLookup::new();
    for repo in repos {
        for case in repo.cases {
            lookup.insert(case.seed_file, case.semantic_distractors);
        }
    }

    println!("Loaded {} semantic distractor entries", lookup.len());
    Ok(lookup)
}

/// Full benchmark results.
#[derive(Debug, Serialize, Deserialize)]
struct BenchmarkResults {
    /// All evaluated configurations with scores
    evaluations: Vec<(ParameterPoint, EvalMetrics)>,

    /// Best configuration found
    best_config: ParameterPoint,
    best_score: f64,

    /// Sensitivity analysis (if run)
    sensitivity: Option<SensitivityAnalysis>,

    /// Metadata
    n_cases: usize,
    n_repos: usize,
    total_time_secs: f64,
    strategy: String,
}

/// Summary info for a training run, used for sorting and display.
struct RunInfo {
    name: String,
    episodes: usize,
    first_ndcg: f64,
    last_ndcg: f64,
    delta: f64,
    /// First episode timestamp (for sorting by start date)
    start_ts: i64,
    /// Last episode timestamp (for end date display)
    end_ts: i64,
}

/// Format a Unix timestamp as a human-readable date string.
fn format_timestamp(ts: i64) -> String {
    if ts == 0 {
        return "â€”".to_string();
    }
    Local.timestamp_opt(ts, 0)
        .single()
        .map(|dt: DateTime<Local>| dt.format("%b %d %H:%M").to_string())
        .unwrap_or_else(|| "â€”".to_string())
}

/// List all available training runs with summary stats, sorted by date.
fn list_training_runs() -> anyhow::Result<()> {
    let runs_dir = PathBuf::from("training/runs");
    if !runs_dir.exists() {
        println!("No training runs found. Run with --run-name to create one.");
        return Ok(());
    }

    // Collect run info with timestamps
    let mut runs: Vec<RunInfo> = Vec::new();
    let mut empty_runs: Vec<(String, i64)> = Vec::new();  // (name, mtime) for runs with no data

    for entry in std::fs::read_dir(&runs_dir)?.filter_map(|e| e.ok()) {
        if !entry.path().is_dir() {
            continue;
        }
        let name = entry.file_name().to_string_lossy().to_string();
        let scratchpad_path = entry.path().join("scratchpad.json");

        // Fallback: use file modification time if no episode timestamps
        let file_mtime = scratchpad_path.metadata()
            .and_then(|m| m.modified())
            .ok()
            .and_then(|t| t.duration_since(std::time::UNIX_EPOCH).ok())
            .map(|d| d.as_secs() as i64)
            .unwrap_or(0);

        if scratchpad_path.exists() {
            if let Ok(file) = File::open(&scratchpad_path) {
                if let Ok(sp) = serde_json::from_reader::<_, Scratchpad>(file) {
                    let eps = sp.episodes.len();
                    if eps > 0 {
                        let first_ndcg = sp.episodes.first().map(|e| e.ndcg_before).unwrap_or(0.0);
                        let last_ndcg = sp.episodes.last().map(|e| e.ndcg_before).unwrap_or(0.0);
                        let delta = last_ndcg - first_ndcg;

                        // Get timestamps from episodes (fall back to file mtime if 0)
                        let start_ts = sp.episodes.first()
                            .map(|e| if e.timestamp > 0 { e.timestamp } else { file_mtime })
                            .unwrap_or(file_mtime);
                        let end_ts = sp.episodes.last()
                            .map(|e| if e.timestamp > 0 { e.timestamp } else { file_mtime })
                            .unwrap_or(file_mtime);

                        runs.push(RunInfo {
                            name,
                            episodes: eps,
                            first_ndcg,
                            last_ndcg,
                            delta,
                            start_ts,
                            end_ts,
                        });
                        continue;
                    }
                }
            }
        }
        empty_runs.push((name, file_mtime));
    }

    // Sort by start timestamp (oldest first, so most recent at bottom)
    runs.sort_by_key(|r| r.start_ts);
    empty_runs.sort_by_key(|(_, ts)| *ts);

    // Build table without box drawing for cleaner output
    use owo_colors::OwoColorize;

    println!();
    println!("{}", "TRAINING RUNS".bold());
    println!("{}", "â”€".repeat(90));
    println!("{:26} {:>4}  {:^17}  {:>8}   {:>24}",
        "NAME", "EPS", "NDCG TRAJECTORY", "DELTA", "STARTED -> LAST");
    println!("{}", "â”€".repeat(90));

    for run in &runs {
        let (trend, delta_colored) = if run.delta > 0.01 {
            ("+", format!("{:>+8.4}", run.delta).green().to_string())
        } else if run.delta < -0.01 {
            ("-", format!("{:>+8.4}", run.delta).red().to_string())
        } else {
            ("=", format!("{:>+8.4}", run.delta).dimmed().to_string())
        };
        let start_str = format_timestamp(run.start_ts);
        let end_str = format_timestamp(run.end_ts);
        let time_range = format!("{} -> {}", start_str, end_str);

        println!("{:26} {:>4}  {:.3} {} {:.3}  {}   {}",
            run.name, run.episodes, run.first_ndcg, trend, run.last_ndcg, delta_colored, time_range.dimmed());
    }

    // Show empty runs at the bottom
    for (name, mtime) in &empty_runs {
        let ts_str = format_timestamp(*mtime);
        println!("{:26}    -  (no data)                          {:>24}", name, ts_str);
    }

    println!("{}", "â”€".repeat(90));
    println!("\nUse --show <run-name> to see detailed visualization.\n");
    Ok(())
}

/// Rich text visualization of a past training run.
/// Shows the full optimization journey: trajectory, strategies, insights.
fn show_training_run(path: &str) -> anyhow::Result<()> {
    use comfy_table::{Table, Cell, ContentArrangement, presets::UTF8_FULL_CONDENSED};
    use owo_colors::OwoColorize;

    // Resolve path: either a run name or direct path to scratchpad
    let scratchpad_path = if path.ends_with(".json") {
        PathBuf::from(path)
    } else {
        PathBuf::from(format!("training/runs/{}/scratchpad.json", path))
    };

    if !scratchpad_path.exists() {
        anyhow::bail!("Scratchpad not found: {}", scratchpad_path.display());
    }

    let file = File::open(&scratchpad_path)?;
    let scratchpad: Scratchpad = serde_json::from_reader(file)?;

    if scratchpad.episodes.is_empty() {
        println!("No episodes found in scratchpad.");
        return Ok(());
    }

    // Header
    let run_name = scratchpad_path.parent()
        .and_then(|p| p.file_name())
        .map(|s| s.to_string_lossy().to_string())
        .unwrap_or_else(|| "unknown".to_string());

    println!();
    println!("{}", format!(" TRAINING RUN: {} ", run_name).bold().on_blue());
    println!("  Episodes: {}", scratchpad.episodes.len());
    println!();

    // NDCG trajectory sparkline
    let ndcgs: Vec<f64> = scratchpad.episodes.iter().map(|e| e.ndcg_before).collect();
    let min_ndcg = ndcgs.iter().cloned().fold(f64::INFINITY, f64::min);
    let max_ndcg = ndcgs.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
    let range = (max_ndcg - min_ndcg).max(0.001);

    let spark_chars = ['â–', 'â–‚', 'â–ƒ', 'â–„', 'â–…', 'â–†', 'â–‡', 'â–ˆ'];
    let sparkline: String = ndcgs.iter().map(|&n| {
        let normalized = ((n - min_ndcg) / range * 7.0).round() as usize;
        spark_chars[normalized.min(7)]
    }).collect();

    let delta = ndcgs.last().unwrap_or(&0.0) - ndcgs.first().unwrap_or(&0.0);
    let delta_str = if delta > 0.0 {
        format!("{:+.4}", delta).green().to_string()
    } else if delta < 0.0 {
        format!("{:+.4}", delta).red().to_string()
    } else {
        format!("{:+.4}", delta).dimmed().to_string()
    };

    println!("{}", "NDCG TRAJECTORY".bold());
    println!("  {:.3} {} {:.3}  Î” = {}", min_ndcg, sparkline.cyan(), max_ndcg, delta_str);
    println!();

    // Episode-by-episode breakdown as a table
    println!("{}", "EPISODES".bold());

    let mut table = Table::new();
    table.load_preset(UTF8_FULL_CONDENSED);
    table.set_content_arrangement(ContentArrangement::Dynamic);
    table.set_header(vec!["#", "Trend", "NDCG", "Fail", "Conf", "Strategy / Changes / Insight"]);

    for (i, ep) in scratchpad.episodes.iter().enumerate() {
        let ep_num = i + 1;
        let trend = if i == 0 {
            "Â·".to_string()
        } else {
            let prev = scratchpad.episodes[i - 1].ndcg_before;
            if ep.ndcg_before > prev + 0.005 { "â†—".green().to_string() }
            else if ep.ndcg_before < prev - 0.005 { "â†˜".red().to_string() }
            else { "â†’".dimmed().to_string() }
        };

        // Build the description column with strategy, changes, insight
        let mut desc_parts: Vec<String> = Vec::new();

        // Strategy capsule
        if !ep.strategy_capsule.is_empty() {
            let capsule = if ep.strategy_capsule.len() > 60 {
                format!("âŸ¨{}...âŸ©", &ep.strategy_capsule[..57])
            } else {
                format!("âŸ¨{}âŸ©", &ep.strategy_capsule)
            };
            desc_parts.push(capsule);
        }

        // Parameter changes (compact format)
        if !ep.proposed_changes.is_empty() {
            let changes: Vec<String> = ep.proposed_changes.iter()
                .map(|(k, (dir, mag, _))| {
                    let arrow = if dir == "increase" { "â†‘" } else { "â†“" };
                    let mag_short = mag.chars().next().map(|c| c.to_uppercase().to_string()).unwrap_or_default();
                    format!("{}{}{}", k, arrow, mag_short)
                })
                .collect();
            desc_parts.push(changes.join(", "));
        }

        // Key insight from this episode
        if !ep.structural_insights.is_empty() {
            let insight = &ep.structural_insights[0];
            let truncated = if insight.len() > 55 {
                format!("ğŸ’¡ {}...", &insight[..52])
            } else {
                format!("ğŸ’¡ {}", insight)
            };
            desc_parts.push(truncated);
        }

        table.add_row(vec![
            Cell::new(format!("E{:02}", ep_num)),
            Cell::new(&trend),
            Cell::new(format!("{:.4}", ep.ndcg_before)),
            Cell::new(format!("{}", ep.failures.len())),
            Cell::new(format!("{:.2}", ep.confidence)),
            Cell::new(desc_parts.join("\n")),
        ]);
    }

    println!("{table}");
    println!();

    // Final parameters (from last episode) as a compact table
    if let Some(last_ep) = scratchpad.episodes.last() {
        println!("{}", "FINAL PARAMETERS".bold());

        let mut ptable = Table::new();
        ptable.load_preset(UTF8_FULL_CONDENSED);
        ptable.set_content_arrangement(ContentArrangement::Dynamic);
        ptable.set_header(vec!["Category", "Parameters"]);

        let p = &last_ep.params;
        ptable.add_row(vec![
            "PageRank",
            &format!("Î±={:.3}  chat_mult={:.1}", p.pagerank_alpha, p.pagerank_chat_multiplier),
        ]);
        ptable.add_row(vec![
            "Depth",
            &format!("root={:.2}  mod={:.2}  deep={:.2}  vendor={:.4}",
                p.depth_weight_root, p.depth_weight_moderate, p.depth_weight_deep, p.depth_weight_vendor),
        ]);
        ptable.add_row(vec![
            "Boosts",
            &format!("ident={:.1}  file={:.1}  chat={:.1}  temp={:.2}  focus={:.2}",
                p.boost_mentioned_ident, p.boost_mentioned_file, p.boost_chat_file,
                p.boost_temporal_coupling, p.boost_focus_expansion),
        ]);
        ptable.add_row(vec![
            "Git",
            &format!("decay={:.0}d  recency_max={:.1}  churn_th={:.0}  churn_max={:.1}",
                p.git_recency_decay_days, p.git_recency_max_boost,
                p.git_churn_threshold, p.git_churn_max_boost),
        ]);
        ptable.add_row(vec![
            "Focus",
            &format!("decay={:.2}  max_hops={:.0}", p.focus_decay, p.focus_max_hops),
        ]);

        println!("{ptable}");
        println!();
    }

    Ok(())
}

/// Resolve a run name or path to a scratchpad file path.
fn resolve_scratchpad_path(path: &str) -> PathBuf {
    if path.ends_with(".json") {
        PathBuf::from(path)
    } else {
        PathBuf::from(format!("training/runs/{}/scratchpad.json", path))
    }
}

/// Pivot view: insights as primary axis.
/// Shows each unique insight and which episodes produced it.
fn show_insights_pivot(path: &str) -> anyhow::Result<()> {
    use std::collections::HashMap;

    let scratchpad_path = resolve_scratchpad_path(path);
    if !scratchpad_path.exists() {
        anyhow::bail!("Scratchpad not found: {}", scratchpad_path.display());
    }

    let file = File::open(&scratchpad_path)?;
    let scratchpad: Scratchpad = serde_json::from_reader(file)?;

    // Build insight -> [episode indices] map
    let mut insight_episodes: HashMap<String, Vec<usize>> = HashMap::new();
    for (i, ep) in scratchpad.episodes.iter().enumerate() {
        for insight in &ep.structural_insights {
            insight_episodes.entry(insight.clone()).or_default().push(i + 1);
        }
    }

    // Sort by frequency (most common first)
    let mut sorted: Vec<_> = insight_episodes.iter().collect();
    sorted.sort_by(|a, b| b.1.len().cmp(&a.1.len()));

    println!("\nSTRUCTURAL INSIGHTS (pivot view)");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    println!("{} unique insights from {} episodes\n", sorted.len(), scratchpad.episodes.len());

    for (insight, episodes) in sorted.iter().take(30) {
        let ep_list = if episodes.len() <= 5 {
            episodes.iter().map(|e| e.to_string()).collect::<Vec<_>>().join(", ")
        } else {
            format!("{}, ... (+{} more)",
                episodes[..3].iter().map(|e| e.to_string()).collect::<Vec<_>>().join(", "),
                episodes.len() - 3)
        };
        println!("[E{}] {}", ep_list, insight);
        println!();
    }

    if sorted.len() > 30 {
        println!("... and {} more insights", sorted.len() - 30);
    }

    Ok(())
}

/// Pivot view: parameter interactions as primary axis.
/// Shows each unique interaction and which episodes discovered it.
fn show_interactions_pivot(path: &str) -> anyhow::Result<()> {
    use std::collections::HashMap;

    let scratchpad_path = resolve_scratchpad_path(path);
    if !scratchpad_path.exists() {
        anyhow::bail!("Scratchpad not found: {}", scratchpad_path.display());
    }

    let file = File::open(&scratchpad_path)?;
    let scratchpad: Scratchpad = serde_json::from_reader(file)?;

    // Build interaction -> [episode indices] map
    let mut interaction_episodes: HashMap<String, Vec<usize>> = HashMap::new();
    for (i, ep) in scratchpad.episodes.iter().enumerate() {
        for interaction in &ep.param_interactions {
            interaction_episodes.entry(interaction.clone()).or_default().push(i + 1);
        }
    }

    // Sort by frequency (most common first)
    let mut sorted: Vec<_> = interaction_episodes.iter().collect();
    sorted.sort_by(|a, b| b.1.len().cmp(&a.1.len()));

    println!("\nPARAMETER INTERACTIONS (pivot view)");
    println!("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€");
    println!("{} unique interactions from {} episodes\n", sorted.len(), scratchpad.episodes.len());

    for (interaction, episodes) in sorted.iter().take(30) {
        let ep_list = if episodes.len() <= 5 {
            episodes.iter().map(|e| e.to_string()).collect::<Vec<_>>().join(", ")
        } else {
            format!("{}, ... (+{} more)",
                episodes[..3].iter().map(|e| e.to_string()).collect::<Vec<_>>().join(", "),
                episodes.len() - 3)
        };
        println!("[E{}] {}", ep_list, interaction);
        println!();
    }

    if sorted.len() > 30 {
        println!("... and {} more interactions", sorted.len() - 30);
    }

    Ok(())
}

fn main() -> anyhow::Result<()> {
    let mut args = Args::parse();

    // Handle --list: show available training runs and exit
    if args.list {
        return list_training_runs();
    }

    // Handle --show: visualize a past training run and exit
    if let Some(ref path) = args.show {
        return show_training_run(path);
    }

    // Handle --show-insights: pivot view with insights as primary axis
    if let Some(ref path) = args.show_insights {
        return show_insights_pivot(path);
    }

    // Handle --show-interactions: pivot view with interactions as primary axis
    if let Some(ref path) = args.show_interactions {
        return show_interactions_pivot(path);
    }

    // Handle --run-name: create run directory and override paths
    if let Some(ref name) = args.run_name {
        let run_dir = PathBuf::from(format!("training/runs/{}", name));
        std::fs::create_dir_all(&run_dir)?;
        std::fs::create_dir_all(run_dir.join("checkpoints"))?;

        args.output = run_dir.join("results.json");
        args.scratchpad = run_dir.join("scratchpad.json");
        if args.plot.is_none() {
            args.plot = Some(run_dir.join("progress.png"));
        }

        println!("Training run: {}", name);
        println!("  Output dir: {}", run_dir.display());
    }

    // Determine repos to use
    let repos: Vec<PathBuf> = if let Some(ref repo) = args.repo {
        vec![repo.clone()]
    } else if let Some(ref corpus) = args.corpus {
        let specs = match corpus.as_str() {
            "quick" => quick_repos(),
            "curated" => CURATED_REPOS.iter().collect(),
            // Future: could load from file path
            other => {
                eprintln!("Error: Unknown corpus '{}'. Use 'quick' or 'curated'", other);
                std::process::exit(1);
            }
        };
        ensure_repos_cloned(&specs, &args.clone_dir)?
    } else {
        eprintln!("Error: Specify --repo or --corpus");
        std::process::exit(1);
    };

    println!("Benchmarking {} repositories", repos.len());

    // Extract training cases from all repos
    let start = Instant::now();
    let mut all_cases: Vec<WeightedCase> = Vec::new();

    for repo_path in &repos {
        println!("\nProcessing {}...", repo_path.display());

        let cases = extract_cases(repo_path, args.max_commits, args.min_files, args.max_files);
        println!("  Extracted {} raw cases", cases.len());

        let coupling = compute_coupling_weights(repo_path, args.max_commits);
        println!("  Computed {} coupling pairs", coupling.len());

        let weighted = weight_cases(cases, &coupling);
        println!("  Weighted cases: {}", weighted.len());

        all_cases.extend(weighted);
    }

    println!("\nTotal training cases: {}", all_cases.len());

    // Load semantic distractors if provided
    let distractors: Option<Arc<DistractorLookup>> = if let Some(ref path) = args.distractors {
        match load_distractors(path) {
            Ok(lookup) => Some(Arc::new(lookup)),
            Err(e) => {
                eprintln!("Warning: Failed to load distractors: {}. Using synthetic.", e);
                None
            }
        }
    } else {
        None
    };

    if args.extract_only {
        // Just save cases and exit
        let file = File::create(&args.output)?;
        serde_json::to_writer_pretty(file, &all_cases)?;
        println!("Saved cases to {}", args.output.display());
        return Ok(());
    }

    if all_cases.is_empty() {
        eprintln!("Error: No training cases extracted. Check repository paths.");
        std::process::exit(1);
    }

    // === REASONING-BASED TRAINING ===
    // Uses Claude as universal function approximator to understand WHY rankings fail
    if args.reason {
        return run_reasoning_training(&args, &all_cases, distractors.as_deref());
    }

    // === DISTILLATION ===
    // Crystallize accumulated insights into operator wisdom
    if args.distill {
        return run_distillation(&args);
    }

    // === CLASSICAL OPTIMIZATION ===
    // Parse search strategy
    let strategy = match args.strategy.as_str() {
        "grid" => SearchStrategy::Grid { points_per_dim: 3 },
        "lhs" => SearchStrategy::LatinHypercube,
        "random" => SearchStrategy::Random,
        "bayesian" => SearchStrategy::Bayesian,
        _ => {
            eprintln!("Unknown strategy: {}. Using LHS.", args.strategy);
            SearchStrategy::LatinHypercube
        }
    };

    // Generate initial parameter points
    let grid = ParameterGrid::default();
    let mut points = sample_points(&grid, strategy, args.budget, args.seed);

    use indicatif::{ProgressBar, ProgressStyle};

    // Shared case data for parallel evaluation
    let cases = Arc::new(all_cases);
    let distractors_ref = distractors.clone();

    // Evaluate all points in parallel with progress bar
    let pb = ProgressBar::new(points.len() as u64);
    pb.set_style(ProgressStyle::with_template(
        "{prefix:.bold} {bar:40.cyan/dim} {pos}/{len} [{elapsed}<{eta}] {msg}"
    ).unwrap());
    pb.set_prefix("Evaluating");

    let best_ndcg = std::sync::atomic::AtomicU64::new(0);
    let evaluations: Vec<(ParameterPoint, EvalMetrics)> = points
        .par_iter()
        .map(|point| {
            let metrics = evaluate_point(point, &cases, distractors_ref.as_deref());
            // Track best score seen so far (atomic for thread safety)
            let current = (metrics.ndcg_at_10 * 10000.0) as u64;
            best_ndcg.fetch_max(current, std::sync::atomic::Ordering::Relaxed);
            let best = best_ndcg.load(std::sync::atomic::Ordering::Relaxed) as f64 / 10000.0;
            pb.set_message(format!("best={:.4}", best));
            pb.inc(1);
            (point.clone(), metrics)
        })
        .collect();
    pb.finish_with_message(format!("best={:.4}", best_ndcg.load(std::sync::atomic::Ordering::Relaxed) as f64 / 10000.0));

    // For Bayesian strategy, do iterative refinement
    let mut evaluations = evaluations; // make mutable for Bayesian
    if matches!(strategy, SearchStrategy::Bayesian) && evaluations.len() < args.budget {
        let remaining = args.budget - evaluations.len();
        let pb = ProgressBar::new(remaining as u64);
        pb.set_style(ProgressStyle::with_template(
            "{prefix:.bold} {bar:40.green/dim} {pos}/{len} [{elapsed}<{eta}] {msg}"
        ).unwrap());
        pb.set_prefix("Bayesian");

        let mut rng = rand::rngs::StdRng::seed_from_u64(args.seed);
        let history: Vec<_> = evaluations.iter()
            .map(|(p, m)| (p.clone(), m.ndcg_at_10))
            .collect();

        let mut best_so_far = evaluations.iter()
            .map(|(_, m)| m.ndcg_at_10)
            .fold(0.0_f64, f64::max);

        for _ in evaluations.len()..args.budget {
            let next = bayesian_next_sample(&grid, &history, &mut rng);
            let metrics = evaluate_point(&next, &cases, distractors.as_deref());
            if metrics.ndcg_at_10 > best_so_far {
                best_so_far = metrics.ndcg_at_10;
            }
            pb.set_message(format!("best={:.4}", best_so_far));
            pb.inc(1);
            evaluations.push((next, metrics));
        }
        pb.finish_with_message(format!("best={:.4}", best_so_far));
    }

    // Find best configuration
    let (best_config, best_metrics) = evaluations
        .iter()
        .max_by(|a, b| a.1.ndcg_at_10.partial_cmp(&b.1.ndcg_at_10).unwrap())
        .cloned()
        .expect("No evaluations");

    let elapsed = start.elapsed().as_secs_f64();

    println!("\n=== Results ===\n");
    println!("Best NDCG@10: {:.4}", best_metrics.ndcg_at_10);
    println!("Best NDCG@5:  {:.4}", best_metrics.ndcg_at_5);
    println!("Best MRR:     {:.4}", best_metrics.mrr);
    println!("Best P@10:    {:.4}", best_metrics.precision_at_10);
    println!("\nTotal time: {:.1}s", elapsed);

    // Print best config
    println!("\n=== Best Configuration ===\n");
    print_config(&best_config);

    // Sensitivity analysis if requested
    let sensitivity = if args.sensitivity {
        println!("\n=== Running Sensitivity Analysis ===\n");
        let distractors_ref = distractors.as_deref();
        let evaluator = |p: &ParameterPoint| evaluate_point(p, &cases, distractors_ref).ndcg_at_10;
        let analysis = full_analysis(&best_config, evaluator);
        print_summary(&analysis);
        Some(analysis)
    } else {
        None
    };

    // Save results
    let results = BenchmarkResults {
        evaluations: evaluations.clone(),
        best_config: best_config.clone(),
        best_score: best_metrics.ndcg_at_10,
        sensitivity,
        n_cases: cases.len(),
        n_repos: repos.len(),
        total_time_secs: elapsed,
        strategy: args.strategy,
    };

    let file = File::create(&args.output)?;
    serde_json::to_writer_pretty(file, &results)?;
    println!("\nResults saved to {}", args.output.display());

    // Also save just the best config for easy loading
    let config_path = args.output.with_extension("best.json");
    let config_file = File::create(&config_path)?;
    serde_json::to_writer_pretty(config_file, &best_config)?;
    println!("Best config saved to {}", config_path.display());

    Ok(())
}

/// Evaluate a parameter point against training cases.
fn evaluate_point(
    point: &ParameterPoint,
    cases: &[WeightedCase],
    distractors: Option<&DistractorLookup>,
) -> EvalMetrics {
    // For each case, simulate running ripmap with this config
    // and compute metrics against ground truth

    let per_case: Vec<CaseMetrics> = cases
        .iter()
        .map(|case| {
            // Simulate ranking: use coupling weights as proxy
            // (In full implementation, would actually run the ranking pipeline)
            let ranking = simulate_ranking(point, case, distractors);

            // Ground truth
            let ground_truth: Vec<_> = case.expected_related
                .iter()
                .cloned()
                .collect();

            CaseMetrics::compute(&ranking, &ground_truth, 0.1)
        })
        .collect();

    // Weight by case quality
    let weighted: Vec<_> = per_case
        .iter()
        .zip(cases.iter())
        .map(|(m, c)| (m.clone(), c.case_weight))
        .collect();

    EvalMetrics::aggregate_weighted(&weighted)
}

/// Simulate ranking for a case with given parameters.
///
/// CRITICAL: This simulation includes **distractors** - files that
/// are NOT in ground truth but compete for ranking. This tests whether the
/// parameters can distinguish signal (coupled files) from noise.
///
/// Two modes:
/// 1. Semantic distractors (Claude-generated): Plausible-but-wrong files
///    that share keywords and structure with ground truth. Realistic challenge.
/// 2. Synthetic distractors (fallback): Simple generated paths like
///    "src/distractor_0.rs". Easy baseline.
///
/// The challenge: ground truth files have coupling > 0, distractors have 0.
/// Good configs should rank ground truth above distractors.
fn simulate_ranking(
    point: &ParameterPoint,
    case: &WeightedCase,
    distractors: Option<&DistractorLookup>,
) -> Vec<String> {
    use rand::Rng;
    let mut rng = rand::thread_rng();

    // Score ground truth files (have coupling signal)
    let mut scored: Vec<_> = case.expected_related
        .iter()
        .map(|(file, coupling_weight)| {
            let score = score_file(point, file, *coupling_weight, &mut rng);
            (file.clone(), score, true) // true = is ground truth
        })
        .collect();

    // Get distractor file paths - either semantic (Claude) or synthetic (fallback)
    let distractor_paths: Vec<String> = if let Some(lookup) = distractors {
        // Try to find semantic distractors for this case by seed file
        lookup.get(&case.seed_file)
            .cloned()
            .unwrap_or_else(|| generate_synthetic_distractors(case.expected_related.len()))
    } else {
        generate_synthetic_distractors(case.expected_related.len())
    };

    // Score and add distractors (all have ZERO coupling - they're noise)
    for distractor in distractor_paths {
        let score = score_file(point, &distractor, 0.0, &mut rng);
        scored.push((distractor, score, false));
    }

    // Sort by score descending
    scored.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));

    // Return just filenames (ground truth + distractors mixed by score)
    scored.into_iter().map(|(f, _, _)| f).collect()
}

/// Generate synthetic distractor paths (fallback when no Claude-generated ones).
fn generate_synthetic_distractors(n_ground_truth: usize) -> Vec<String> {
    let n_distractors = (n_ground_truth * 3).max(10);
    (0..n_distractors)
        .map(|i| match i % 5 {
            0 => format!("src/distractor_{}.rs", i),           // depth 1
            1 => format!("src/utils/helper_{}.rs", i),         // depth 2
            2 => format!("src/core/internal/deep_{}.rs", i),   // depth 3
            3 => format!("lib/mod_{}.rs", i),                  // depth 1
            _ => format!("tests/test_{}.rs", i),               // depth 1
        })
        .collect()
}

/// Score a single file based on parameters.
///
/// The score combines:
/// - Base coupling weight (0.0 for distractors, >0 for ground truth)
/// - Depth weighting (shallower = higher by default)
/// - Simulated recency boost
/// - Random noise
fn score_file(
    point: &ParameterPoint,
    file: &str,
    coupling_weight: f64,
    rng: &mut impl rand::Rng,
) -> f64 {
    // Base score from coupling (this IS the signal)
    // Ground truth: coupling > 0
    // Distractors: coupling = 0
    let mut score = coupling_weight;

    // Depth weighting - shallower files get higher base weight
    // This is a heuristic that should help, but distractors at shallow
    // depths will also get boosted, testing if coupling dominates
    let depth = file.matches('/').count();
    let depth_mult = if depth <= 2 {
        point.depth_weight_root
    } else if depth <= 4 {
        point.depth_weight_moderate
    } else {
        point.depth_weight_deep
    };

    // For files with coupling > 0, depth acts as multiplier
    // For distractors (coupling = 0), depth gives them a small base score
    // This creates the challenge: can coupling beat depth heuristic?
    if coupling_weight > 0.0 {
        score *= depth_mult;
    } else {
        // Distractors get only depth-based score (0.01 - 0.1 range)
        // This simulates "PageRank gives some score to all files"
        score = 0.05 * depth_mult;
    }

    // Simulated recency boost
    // Ground truth files: assume 70% are "recent" (last 30 days)
    // Distractors: assume only 20% are "recent"
    let is_recent = if coupling_weight > 0.0 {
        rng.r#gen::<f64>() < 0.7
    } else {
        rng.r#gen::<f64>() < 0.2
    };

    if is_recent {
        // Recent files get boosted based on recency config
        let recency_boost = 1.0 + (point.git_recency_max_boost - 1.0)
            * (1.0 - rng.r#gen::<f64>() * 0.3);  // 70-100% of max boost
        score *= recency_boost;
    }

    // Simulated churn boost
    // Ground truth files: assume 40% are "high churn"
    // Distractors: assume only 10% are "high churn"
    let is_high_churn = if coupling_weight > 0.0 {
        rng.r#gen::<f64>() < 0.4
    } else {
        rng.r#gen::<f64>() < 0.1
    };

    if is_high_churn {
        score *= 1.0 + (point.git_churn_max_boost - 1.0) * 0.5;
    }

    // Random noise (Â±15%)
    score *= 0.85 + 0.3 * rng.r#gen::<f64>();

    score
}

/// Ensure curated repos are cloned locally.
fn ensure_repos_cloned(specs: &[&RepoSpec], base_dir: &Path) -> anyhow::Result<Vec<PathBuf>> {
    use indicatif::{ProgressBar, ProgressStyle};

    std::fs::create_dir_all(base_dir)?;

    let mut paths = Vec::new();

    for spec in specs {
        let repo_dir = base_dir.join(&spec.name);

        if !repo_dir.exists() {
            let spinner = ProgressBar::new_spinner();
            spinner.set_style(ProgressStyle::with_template(
                "{spinner:.green} {msg}"
            ).unwrap().tick_strings(&["â ‹", "â ™", "â ¹", "â ¸", "â ¼", "â ´", "â ¦", "â §", "â ‡", "â "]));
            spinner.set_message(format!("Cloning {}...", spec.name));
            spinner.enable_steady_tick(std::time::Duration::from_millis(80));

            let status = std::process::Command::new("git")
                .args([
                    "clone",
                    "--depth", &spec.estimated_commits().to_string(),
                    &spec.url,
                    repo_dir.to_str().unwrap(),
                ])
                .stdout(std::process::Stdio::null())
                .stderr(std::process::Stdio::null())
                .status()?;

            if !status.success() {
                spinner.finish_with_message(format!("âœ— Failed to clone {}", spec.name));
                continue;
            }
            spinner.finish_with_message(format!("âœ“ Cloned {}", spec.name));
        }

        paths.push(repo_dir);
    }

    Ok(paths)
}

/// Print a parameter configuration in a readable format.
fn print_config(point: &ParameterPoint) {
    println!("PageRank:");
    println!("  alpha:           {:.2}", point.pagerank_alpha);
    println!("  chat_multiplier: {:.1}", point.pagerank_chat_multiplier);
    println!();
    println!("Depth Weights:");
    println!("  root:     {:.2}", point.depth_weight_root);
    println!("  moderate: {:.2}", point.depth_weight_moderate);
    println!("  deep:     {:.2}", point.depth_weight_deep);
    println!("  vendor:   {:.4}", point.depth_weight_vendor);
    println!();
    println!("Boosts:");
    println!("  mentioned_ident: {:.1}", point.boost_mentioned_ident);
    println!("  mentioned_file:  {:.1}", point.boost_mentioned_file);
    println!("  chat_file:       {:.1}", point.boost_chat_file);
    println!("  temporal:        {:.1}", point.boost_temporal_coupling);
    println!("  focus_expand:    {:.1}", point.boost_focus_expansion);
    println!();
    println!("Git:");
    println!("  recency_decay_days: {:.1}", point.git_recency_decay_days);
    println!("  recency_max_boost:  {:.1}", point.git_recency_max_boost);
    println!("  churn_threshold:    {:.0}", point.git_churn_threshold);
    println!("  churn_max_boost:    {:.1}", point.git_churn_max_boost);
    println!();
    println!("Focus Expansion:");
    println!("  decay:    {:.2}", point.focus_decay);
    println!("  max_hops: {:.0}", point.focus_max_hops);
}

// =============================================================================
// REASONING-BASED TRAINING
// =============================================================================

/// Run reasoning-based training loop.
///
/// This is where Claude acts as a universal function approximator:
/// - Evaluate current parameters, collect failures
/// - Ask Claude to reason about WHY failures occurred
/// - Apply proposed changes, update scratchpad
/// - Repeat for N episodes
fn run_reasoning_training(
    args: &Args,
    cases: &[WeightedCase],
    distractors: Option<&DistractorLookup>,
) -> anyhow::Result<()> {
    use ripmap::training::LiveProgress;

    // Load prompt template (required)
    let prompt_path = args.prompt.as_ref()
        .ok_or_else(|| anyhow::anyhow!("--prompt is required for reasoning mode. Example: --prompt training-outer/prompts/inner/v001.md"))?;
    let prompt_template = std::fs::read_to_string(prompt_path)
        .map_err(|e| anyhow::anyhow!("Failed to read prompt template '{}': {}", prompt_path.display(), e))?;

    // Parse agent type
    let agent: Agent = args.agent.parse()
        .map_err(|e: String| anyhow::anyhow!(e))?;
    let model = args.model.as_deref();

    println!("\n=== REASONING-BASED TRAINING ===");
    println!("Prompt: {}", prompt_path.display());
    println!("Using {} as universal function approximator\n", agent);

    // Load or create scratchpad
    let mut scratchpad = if args.scratchpad.exists() {
        let file = File::open(&args.scratchpad)?;
        serde_json::from_reader(file).unwrap_or_default()
    } else {
        Scratchpad::default()
    };

    // Start with default or loaded parameters
    let mut current_params = if let Some(ref config_path) = args.config {
        let file = File::open(config_path)?;
        serde_json::from_reader(file)?
    } else {
        ParameterPoint::default()
    };

    println!("Starting parameters:");
    print_config(&current_params);
    println!("\nRunning {} reasoning episodes...\n", args.episodes);

    // Live progress visualization for terminal sparklines
    let mut progress = LiveProgress::new();

    for episode_num in 0..args.episodes {
        // Evaluate current parameters
        let metrics = evaluate_point(&current_params, cases, distractors);

        // Collect failures (cases where NDCG < threshold)
        let failures = collect_failures(
            &current_params,
            cases,
            distractors,
            args.failure_threshold,
        );

        if failures.is_empty() {
            println!("No failures below threshold {:.2}. Training converged!", args.failure_threshold);
            break;
        }

        // Ask LLM to reason about failures with spinner
        use indicatif::{ProgressBar, ProgressStyle};
        let spinner = ProgressBar::new_spinner();
        spinner.set_style(ProgressStyle::with_template(
            "{prefix:.bold} {spinner:.cyan} {msg}"
        ).unwrap().tick_strings(&["â ‹", "â ™", "â ¹", "â ¸", "â ¼", "â ´", "â ¦", "â §", "â ‡", "â "]));
        spinner.set_prefix(format!("E{:02}", episode_num + 1));
        spinner.set_message(format!("{} reasoning ({} failures)...", agent, failures.len()));
        spinner.enable_steady_tick(std::time::Duration::from_millis(80));

        match reason_about_failures(&prompt_template, &failures, &current_params, &scratchpad, metrics.ndcg_at_10, agent, model) {
            Ok(episode) => {
                spinner.finish_and_clear();

                // Record metrics for live progress visualization
                progress.record(
                    metrics.ndcg_at_10,
                    failures.len(),
                    episode.confidence,
                    current_params.pagerank_alpha,
                );

                // Display live sparklines during training
                progress.display(episode_num + 1, args.episodes);

                println!(); // Newline after sparkline for subsequent output
                println!("Confidence: {:.2}  â± {:.1}s", episode.confidence, episode.duration_secs);
                println!("Proposed {} changes:", episode.proposed_changes.len());
                for (param, (dir, mag, rationale)) in &episode.proposed_changes {
                    println!("  {} {} {} - \"{}\"", param, dir, mag, rationale);
                }

                if !episode.structural_insights.is_empty() {
                    println!("\nStructural insights:");
                    for insight in &episode.structural_insights {
                        println!("  â€¢ {}", insight);
                    }
                }

                // Apply changes if confidence is high enough
                if episode.confidence >= 0.3 {
                    current_params = apply_changes(&current_params, &episode.proposed_changes);
                    println!("\nApplied changes. New params:");
                    print_config(&current_params);
                } else {
                    println!("\nConfidence too low ({:.2}), skipping changes", episode.confidence);
                }

                // Update scratchpad
                update_scratchpad(&mut scratchpad, &episode);
            }
            Err(e) => {
                spinner.finish_with_message("failed");
                eprintln!("Warning: Reasoning failed: {}", e);
                // Still record progress even on failure
                progress.record(metrics.ndcg_at_10, failures.len(), 0.0, current_params.pagerank_alpha);
                progress.display(episode_num + 1, args.episodes);
                println!();
            }
        }

        // Incremental save for crash recovery (every save_interval episodes)
        if (episode_num + 1) % args.save_interval == 0 {
            // Save scratchpad
            std::fs::create_dir_all(args.scratchpad.parent().unwrap_or(Path::new(".")))?;
            let scratchpad_file = File::create(&args.scratchpad)?;
            serde_json::to_writer_pretty(scratchpad_file, &scratchpad)?;

            // Save current params to checkpoints dir if using run_name, otherwise alongside output
            let checkpoint_path = if args.run_name.is_some() {
                args.output.parent().unwrap().join("checkpoints").join(format!("ep{:03}.json", episode_num + 1))
            } else {
                args.output.with_extension(format!("ep{}.json", episode_num + 1))
            };
            let checkpoint_file = File::create(&checkpoint_path)?;
            serde_json::to_writer_pretty(checkpoint_file, &current_params)?;

            // Generate interim chart if plotters enabled
            #[cfg(feature = "plotters")]
            if let Some(plot_path) = &args.plot {
                use ripmap::training::plots::plot_training_progress;
                let _ = plot_training_progress(&scratchpad, plot_path.to_str().unwrap_or("training.png"));
            }

            println!("  [checkpoint saved at episode {}]", episode_num + 1);
        }

        println!();
    }

    // Final evaluation
    let final_metrics = evaluate_point(&current_params, cases, distractors);

    // Display final training summary with full sparklines
    progress.final_summary();

    println!("Final NDCG@10: {:.4}", final_metrics.ndcg_at_10);
    println!("Final MRR:     {:.4}", final_metrics.mrr);

    // Timing statistics
    let durations: Vec<f64> = scratchpad.episodes.iter()
        .map(|e| e.duration_secs)
        .filter(|&d| d > 0.0)  // Filter out old episodes without timing
        .collect();
    if !durations.is_empty() {
        let total_agent_time: f64 = durations.iter().sum();
        let avg_time = total_agent_time / durations.len() as f64;
        let min_time = durations.iter().cloned().fold(f64::INFINITY, f64::min);
        let max_time = durations.iter().cloned().fold(0.0, f64::max);
        println!("\nâ± Agent timing ({} episodes with timing data):", durations.len());
        println!("  Total: {:.1}s ({:.1}m)  Avg: {:.1}s  Min: {:.1}s  Max: {:.1}s",
            total_agent_time, total_agent_time / 60.0, avg_time, min_time, max_time);
    }

    // Save scratchpad
    std::fs::create_dir_all(args.scratchpad.parent().unwrap_or(Path::new(".")))?;
    let scratchpad_file = File::create(&args.scratchpad)?;
    serde_json::to_writer_pretty(scratchpad_file, &scratchpad)?;
    println!("\nScratchpad saved to {}", args.scratchpad.display());

    // Print scratchpad summary
    print_scratchpad_summary(&scratchpad);

    // Save final params
    let config_path = args.output.with_extension("trained.json");
    let config_file = File::create(&config_path)?;
    serde_json::to_writer_pretty(config_file, &current_params)?;
    println!("\nTrained config saved to {}", config_path.display());

    // Generate training progress chart if requested
    if let Some(plot_path) = &args.plot {
        #[cfg(feature = "plotters")]
        {
            use ripmap::training::plots::plot_training_progress;
            match plot_training_progress(&scratchpad, plot_path.to_str().unwrap_or("training.png")) {
                Ok(()) => println!("Training chart saved to {}", plot_path.display()),
                Err(e) => eprintln!("Failed to generate chart: {}", e),
            }
        }
        #[cfg(not(feature = "plotters"))]
        {
            eprintln!("Chart generation requires: cargo build --features plotters");
            let _ = plot_path; // suppress unused warning
        }
    }

    Ok(())
}

/// Collect ranking failures for reasoning analysis.
fn collect_failures(
    params: &ParameterPoint,
    cases: &[WeightedCase],
    distractors: Option<&DistractorLookup>,
    threshold: f64,
) -> Vec<RankingFailure> {
    let mut failures = Vec::new();

    for case in cases.iter().take(50) { // Sample up to 50 cases
        let ranking = simulate_ranking(params, case, distractors);

        // Ground truth as (file, weight) pairs for metrics computation
        let ground_truth_weighted: Vec<_> = case.expected_related.clone();

        // Ground truth file names only for failure reporting
        let ground_truth_files: Vec<_> = case.expected_related
            .iter()
            .map(|(f, _)| f.clone())
            .collect();

        let metrics = CaseMetrics::compute(&ranking, &ground_truth_weighted, 0.1);

        if metrics.ndcg_at_10 < threshold {
            failures.push(RankingFailure {
                query: case.seed_file.clone(), // Use seed as query
                seed_file: case.seed_file.clone(),
                expected_top: ground_truth_files.iter().take(5).cloned().collect(),
                actual_top: ranking.iter().take(5).cloned().collect(),
                ndcg: metrics.ndcg_at_10,
                commit_context: format!("intent: {:?}", case.inferred_intent),
                repo_name: "curated".to_string(),
                repo_file_count: 100, // Placeholder
            });

            if failures.len() >= 10 {
                break; // Cap at 10 failures per episode
            }
        }
    }

    failures
}

/// Run distillation to crystallize scratchpad into wisdom.
fn run_distillation(args: &Args) -> anyhow::Result<()> {
    // Parse agent type
    let agent: Agent = args.agent.parse()
        .map_err(|e: String| anyhow::anyhow!(e))?;
    let model = args.model.as_deref();

    println!("\n=== DISTILLING SCRATCHPAD ===\n");

    if !args.scratchpad.exists() {
        eprintln!("Error: Scratchpad not found at {}", args.scratchpad.display());
        std::process::exit(1);
    }

    let scratchpad_file = File::open(&args.scratchpad)?;
    let scratchpad: Scratchpad = serde_json::from_reader(scratchpad_file)?;

    println!("Loaded {} episodes from scratchpad", scratchpad.episodes.len());
    print_scratchpad_summary(&scratchpad);

    println!("\nCalling {} for distillation...", agent);
    match distill_scratchpad(&scratchpad, agent, model) {
        Ok(wisdom) => {
            println!("\n=== DISTILLED WISDOM ===\n");
            println!("{}", wisdom);

            // Save wisdom
            let wisdom_path = args.output.with_extension("wisdom.json");
            let wisdom_file = File::create(&wisdom_path)?;
            wisdom_file.sync_all()?;
            std::fs::write(&wisdom_path, &wisdom)?;
            println!("\nWisdom saved to {}", wisdom_path.display());
        }
        Err(e) => {
            eprintln!("Error: Distillation failed: {}", e);
        }
    }

    Ok(())
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/cache/mod.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Persistent caching with redb.
//!
//! Caches parsed tags per file, keyed by (path, mtime).
//! Enables 100x+ speedup on warm runs.

mod store;

pub use store::{TagCache, CacheStats};

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/cache/store.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Persistent tag cache using redb.
//!
//! Strategy: Cache parsed tags per file, keyed by (path, mtime).
//! On cache hit with matching mtime, skip parsing entirely = 100x+ speedup.
//!
//! Cache structure:
//! - Database: .ripmap.cache/tags.redb (redb provides ACID guarantees)
//! - Key: file path (relative to project root)
//! - Value: bincode-serialized (mtime_secs, mtime_nanos, Vec<Tag>)
//!
//! Design decisions:
//! - Bincode for compact binary serialization (faster than JSON, smaller than msgpack)
//! - mtime stored in value for atomic validation (no separate metadata table)
//! - redb for zero-copy reads and write durability without WAL overhead
//! - Cache directory in .ripmap.cache/ to keep project root clean

use std::fs;
use std::path::{Path, PathBuf};
use std::time::SystemTime;

use anyhow::{Context, Result};
use redb::{Database, ReadableTable, ReadableTableMetadata, TableDefinition};
use serde::{Deserialize, Serialize};

use crate::types::Tag;

/// Table definition for tag cache.
/// Key = file path (relative), Value = serialized CacheEntry
const TAGS_TABLE: TableDefinition<&str, &[u8]> = TableDefinition::new("tags");

/// Cache entry containing mtime validation data + parsed tags.
/// Stored as bincode bytes in redb for compact representation.
#[derive(Debug, Clone, Serialize, Deserialize)]
struct CacheEntry {
    /// Modification time seconds since UNIX_EPOCH
    mtime_secs: u64,
    /// Modification time nanoseconds component
    mtime_nanos: u32,
    /// Parsed tags for this file
    tags: Vec<Tag>,
}

impl CacheEntry {
    /// Create entry from SystemTime and tags
    fn new(mtime: SystemTime, tags: Vec<Tag>) -> Result<Self> {
        let duration = mtime
            .duration_since(SystemTime::UNIX_EPOCH)
            .context("File mtime is before UNIX_EPOCH")?;

        Ok(Self {
            mtime_secs: duration.as_secs(),
            mtime_nanos: duration.subsec_nanos(),
            tags,
        })
    }

    /// Check if this entry's mtime matches the given mtime
    fn is_valid(&self, mtime: SystemTime) -> bool {
        let Ok(duration) = mtime.duration_since(SystemTime::UNIX_EPOCH) else {
            return false;
        };

        self.mtime_secs == duration.as_secs()
            && self.mtime_nanos == duration.subsec_nanos()
    }

    /// Serialize to bytes using bincode
    fn to_bytes(&self) -> Result<Vec<u8>> {
        bincode::serialize(self).context("Failed to serialize cache entry")
    }

    /// Deserialize from bytes
    fn from_bytes(bytes: &[u8]) -> Result<Self> {
        bincode::deserialize(bytes).context("Failed to deserialize cache entry")
    }
}

/// Persistent tag cache backed by redb.
///
/// Provides mtime-validated caching of parsed tags to avoid re-parsing
/// unchanged files. Cache hits can provide 100x+ speedup on warm runs.
pub struct TagCache {
    /// redb database handle (thread-safe, uses parking_lot internally)
    db: Database,
    /// Path to cache directory (.ripmap.cache/)
    /// Reserved for future features (e.g., cache cleanup, size management)
    #[allow(dead_code)]
    cache_dir: PathBuf,
}

impl TagCache {
    /// Open or create the tag cache database.
    ///
    /// Cache location: `<root>/.ripmap.cache/tags.redb`
    ///
    /// Creates the cache directory if it doesn't exist.
    /// Returns error if directory creation or database opening fails.
    pub fn open(root: &Path) -> Result<Self> {
        let cache_dir = root.join(".ripmap.cache");

        // Ensure cache directory exists
        fs::create_dir_all(&cache_dir)
            .with_context(|| format!("Failed to create cache directory: {}", cache_dir.display()))?;

        let db_path = cache_dir.join("tags.redb");

        // Open or create database
        // redb automatically handles schema migration and corruption recovery
        let db = Database::create(&db_path)
            .with_context(|| format!("Failed to open cache database: {}", db_path.display()))?;

        Ok(Self { db, cache_dir })
    }

    /// Get cached tags for a file if the cache entry is still valid.
    ///
    /// Returns `Some(tags)` if:
    /// - File exists in cache
    /// - Stored mtime matches current mtime
    ///
    /// Returns `None` if:
    /// - File not in cache
    /// - mtime mismatch (file was modified)
    /// - Deserialization error (cache corruption)
    pub fn get(&self, fname: &str, mtime: SystemTime) -> Option<Vec<Tag>> {
        // Read transaction - zero-copy access to cached data
        let read_txn = self.db.begin_read().ok()?;
        let table = read_txn.open_table(TAGS_TABLE).ok()?;

        // Lookup by file path
        let value_guard = table.get(fname).ok()??;
        let bytes = value_guard.value();

        // Deserialize and validate mtime
        let entry = CacheEntry::from_bytes(bytes).ok()?;

        if entry.is_valid(mtime) {
            Some(entry.tags)
        } else {
            // mtime mismatch - file was modified, cache invalid
            None
        }
    }

    /// Store tags for a file with its current mtime.
    ///
    /// Overwrites any existing cache entry for this file.
    /// Returns error if serialization or database write fails.
    pub fn set(&self, fname: &str, mtime: SystemTime, tags: &[Tag]) -> Result<()> {
        let entry = CacheEntry::new(mtime, tags.to_vec())?;
        let bytes = entry.to_bytes()?;

        // Write transaction - ACID guarantees from redb
        let write_txn = self.db.begin_write()
            .context("Failed to begin write transaction")?;

        {
            let mut table = write_txn.open_table(TAGS_TABLE)
                .context("Failed to open tags table")?;

            table.insert(fname, bytes.as_slice())
                .with_context(|| format!("Failed to insert cache entry for {}", fname))?;
        }

        // Commit transaction - durability guaranteed
        write_txn.commit()
            .context("Failed to commit cache write")?;

        Ok(())
    }

    /// Clear all cached data.
    ///
    /// Removes all entries from the cache database.
    /// Does not delete the database file itself.
    pub fn clear(&self) -> Result<()> {
        let write_txn = self.db.begin_write()
            .context("Failed to begin write transaction for clear")?;

        {
            let mut table = write_txn.open_table(TAGS_TABLE)
                .context("Failed to open tags table")?;

            // Drain iterator removes all entries
            let keys: Vec<String> = table.iter()
                .ok()
                .into_iter()
                .flatten()
                .filter_map(|r| r.ok())
                .map(|(k, _)| k.value().to_string())
                .collect();

            for key in keys {
                table.remove(key.as_str())
                    .context("Failed to remove cache entry during clear")?;
            }
        }

        write_txn.commit()
            .context("Failed to commit cache clear")?;

        Ok(())
    }

    /// Get cache statistics for monitoring and debugging.
    ///
    /// Returns number of cached files and approximate database size.
    pub fn stats(&self) -> CacheStats {
        let read_txn = match self.db.begin_read() {
            Ok(txn) => txn,
            Err(_) => return CacheStats::default(),
        };

        let table = match read_txn.open_table(TAGS_TABLE) {
            Ok(t) => t,
            Err(_) => return CacheStats::default(),
        };

        let entries = table.len().unwrap_or(0) as usize;

        // Approximate size by iterating entries and summing value lengths
        let size_bytes = table.iter()
            .ok()
            .into_iter()
            .flatten()
            .filter_map(|r| r.ok())
            .map(|(k, v)| k.value().len() + v.value().len())
            .sum::<usize>() as u64;

        CacheStats { entries, size_bytes }
    }
}

/// Cache statistics for monitoring.
#[derive(Debug, Clone, Default)]
pub struct CacheStats {
    /// Number of files in cache
    pub entries: usize,
    /// Approximate total size in bytes (keys + values)
    pub size_bytes: u64,
}

impl CacheStats {
    /// Format size in human-readable form (KB, MB, GB)
    pub fn size_human(&self) -> String {
        const KB: u64 = 1024;
        const MB: u64 = KB * 1024;
        const GB: u64 = MB * 1024;

        if self.size_bytes >= GB {
            format!("{:.2} GB", self.size_bytes as f64 / GB as f64)
        } else if self.size_bytes >= MB {
            format!("{:.2} MB", self.size_bytes as f64 / MB as f64)
        } else if self.size_bytes >= KB {
            format!("{:.2} KB", self.size_bytes as f64 / KB as f64)
        } else {
            format!("{} B", self.size_bytes)
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::types::{TagKind, Tag};

    fn make_test_tag(name: &str) -> Tag {
        Tag {
            rel_fname: "test.rs".into(),
            fname: "/tmp/test.rs".into(),
            line: 1,
            name: name.into(),
            kind: TagKind::Def,
            node_type: "function".into(),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
        metadata: None,
        }
    }

    #[test]
    fn test_cache_entry_mtime_validation() {
        let now = SystemTime::now();
        let tags = vec![make_test_tag("foo")];

        let entry = CacheEntry::new(now, tags.clone()).unwrap();

        // Same mtime should validate
        assert!(entry.is_valid(now));

        // Different mtime should not validate
        let later = now + std::time::Duration::from_secs(1);
        assert!(!entry.is_valid(later));
    }

    #[test]
    fn test_cache_entry_serialization() {
        let now = SystemTime::now();
        let tags = vec![
            make_test_tag("foo"),
            make_test_tag("bar"),
        ];

        let entry = CacheEntry::new(now, tags.clone()).unwrap();
        let bytes = entry.to_bytes().unwrap();
        let decoded = CacheEntry::from_bytes(&bytes).unwrap();

        assert_eq!(entry.mtime_secs, decoded.mtime_secs);
        assert_eq!(entry.mtime_nanos, decoded.mtime_nanos);
        assert_eq!(entry.tags.len(), decoded.tags.len());
    }

    #[test]
    fn test_cache_roundtrip() -> Result<()> {
        let temp_dir = std::env::temp_dir().join("ripmap_test_cache");
        let _ = fs::remove_dir_all(&temp_dir); // Clean up from previous runs
        fs::create_dir_all(&temp_dir)?;

        let cache = TagCache::open(&temp_dir)?;
        let now = SystemTime::now();
        let tags = vec![make_test_tag("test_fn")];

        // Store
        cache.set("test.rs", now, &tags)?;

        // Retrieve with matching mtime
        let retrieved = cache.get("test.rs", now);
        assert!(retrieved.is_some());
        assert_eq!(retrieved.unwrap().len(), 1);

        // Retrieve with different mtime
        let later = now + std::time::Duration::from_secs(1);
        let retrieved = cache.get("test.rs", later);
        assert!(retrieved.is_none());

        // Clean up
        fs::remove_dir_all(&temp_dir)?;
        Ok(())
    }

    #[test]
    fn test_cache_clear() -> Result<()> {
        let temp_dir = std::env::temp_dir().join("ripmap_test_cache_clear");
        let _ = fs::remove_dir_all(&temp_dir);
        fs::create_dir_all(&temp_dir)?;

        let cache = TagCache::open(&temp_dir)?;
        let now = SystemTime::now();

        // Add multiple entries
        cache.set("file1.rs", now, &[make_test_tag("fn1")])?;
        cache.set("file2.rs", now, &[make_test_tag("fn2")])?;

        let stats = cache.stats();
        assert_eq!(stats.entries, 2);

        // Clear
        cache.clear()?;

        let stats = cache.stats();
        assert_eq!(stats.entries, 0);

        // Clean up
        fs::remove_dir_all(&temp_dir)?;
        Ok(())
    }

    #[test]
    fn test_cache_stats() -> Result<()> {
        let temp_dir = std::env::temp_dir().join("ripmap_test_cache_stats");
        let _ = fs::remove_dir_all(&temp_dir);
        fs::create_dir_all(&temp_dir)?;

        let cache = TagCache::open(&temp_dir)?;
        let now = SystemTime::now();

        // Empty cache
        let stats = cache.stats();
        assert_eq!(stats.entries, 0);
        assert_eq!(stats.size_bytes, 0);

        // Add entry
        cache.set("test.rs", now, &[make_test_tag("foo")])?;

        let stats = cache.stats();
        assert_eq!(stats.entries, 1);
        assert!(stats.size_bytes > 0);

        // Human-readable size
        let size_str = stats.size_human();
        assert!(size_str.contains("B")); // Should contain "B" for bytes

        // Clean up
        fs::remove_dir_all(&temp_dir)?;
        Ok(())
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/callgraph/graph.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Core call graph data structures.
//!
//! The graph is strategy-agnostic - it just stores resolved edges.
//! Resolution strategies populate it; PageRank consumes it.

use std::collections::HashMap;
use std::sync::Arc;
use petgraph::graph::{DiGraph, NodeIndex};
use petgraph::visit::EdgeRef;
use serde::{Deserialize, Serialize};

/// Unique identifier for a function/method in the codebase.
///
/// Uses (file, name, line) tuple for disambiguation.
/// Line is needed because Python allows multiple functions with same name
/// in different scopes within one file.
#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct FunctionId {
    /// Relative file path
    pub file: Arc<str>,
    /// Function/method name
    pub name: Arc<str>,
    /// Line number (for disambiguation)
    pub line: u32,
    /// Parent class/module if applicable
    pub parent: Option<Arc<str>>,
}

impl FunctionId {
    pub fn new(file: impl Into<Arc<str>>, name: impl Into<Arc<str>>, line: u32) -> Self {
        Self {
            file: file.into(),
            name: name.into(),
            line,
            parent: None,
        }
    }

    pub fn with_parent(mut self, parent: impl Into<Arc<str>>) -> Self {
        self.parent = Some(parent.into());
        self
    }

    pub fn with_parent_opt(mut self, parent: Option<Arc<str>>) -> Self {
        self.parent = parent;
        self
    }

    /// Qualified name for display: "Class.method" or just "function"
    pub fn qualified_name(&self) -> String {
        match &self.parent {
            Some(p) => format!("{}.{}", p, self.name),
            None => self.name.to_string(),
        }
    }
}

/// An edge in the call graph representing a function call.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct CallEdge {
    /// How confident we are in this resolution (0.0 - 1.0)
    pub confidence: f64,
    /// Which strategy resolved this call
    pub strategy: String,
    /// Line number where the call occurs
    pub call_site: u32,
    /// Type information if available (e.g., "self: MyClass")
    pub type_hint: Option<String>,
}

impl CallEdge {
    pub fn new(confidence: f64, strategy: impl Into<String>, call_site: u32) -> Self {
        Self {
            confidence,
            strategy: strategy.into(),
            call_site,
            type_hint: None,
        }
    }

    pub fn with_type_hint(mut self, hint: impl Into<String>) -> Self {
        self.type_hint = Some(hint.into());
        self
    }
}

/// The call graph: functions as nodes, calls as edges.
///
/// Uses petgraph for efficient graph algorithms (PageRank, traversal).
/// Multiple edges between same pair allowed (same function called multiple times).
#[derive(Debug)]
pub struct CallGraph {
    /// The underlying directed graph
    graph: DiGraph<FunctionId, CallEdge>,
    /// Fast lookup: FunctionId -> NodeIndex
    index: HashMap<FunctionId, NodeIndex>,
}

impl CallGraph {
    pub fn new() -> Self {
        Self {
            graph: DiGraph::new(),
            index: HashMap::new(),
        }
    }

    /// Add a function to the graph, returns its node index.
    /// Idempotent - returns existing index if already present.
    pub fn add_function(&mut self, id: FunctionId) -> NodeIndex {
        if let Some(&idx) = self.index.get(&id) {
            return idx;
        }
        let idx = self.graph.add_node(id.clone());
        self.index.insert(id, idx);
        idx
    }

    /// Add a call edge between two functions.
    /// Both functions are auto-added if not present.
    pub fn add_call(&mut self, caller: FunctionId, callee: FunctionId, edge: CallEdge) {
        let caller_idx = self.add_function(caller);
        let callee_idx = self.add_function(callee);
        self.graph.add_edge(caller_idx, callee_idx, edge);
    }

    /// Get node index for a function (if exists)
    pub fn get_index(&self, id: &FunctionId) -> Option<NodeIndex> {
        self.index.get(id).copied()
    }

    /// Get function by index
    pub fn get_function(&self, idx: NodeIndex) -> Option<&FunctionId> {
        self.graph.node_weight(idx)
    }

    /// All functions (nodes) in the graph
    pub fn functions(&self) -> impl Iterator<Item = &FunctionId> {
        self.graph.node_weights()
    }

    /// Number of functions
    pub fn function_count(&self) -> usize {
        self.graph.node_count()
    }

    /// Number of call edges
    pub fn call_count(&self) -> usize {
        self.graph.edge_count()
    }

    /// Get all calls FROM a function (outgoing edges)
    pub fn calls_from(&self, id: &FunctionId) -> Vec<(&FunctionId, &CallEdge)> {
        let Some(idx) = self.index.get(id) else {
            return vec![];
        };
        self.graph
            .edges(*idx)
            .map(|e| {
                let target = self.graph.node_weight(e.target()).unwrap();
                (target, e.weight())
            })
            .collect()
    }

    /// Get all calls TO a function (incoming edges) - "called by"
    pub fn calls_to(&self, id: &FunctionId) -> Vec<(&FunctionId, &CallEdge)> {
        let Some(idx) = self.index.get(id) else {
            return vec![];
        };
        self.graph
            .edges_directed(*idx, petgraph::Direction::Incoming)
            .map(|e| {
                let source = self.graph.node_weight(e.source()).unwrap();
                (source, e.weight())
            })
            .collect()
    }

    /// Access underlying petgraph for algorithms
    pub fn inner(&self) -> &DiGraph<FunctionId, CallEdge> {
        &self.graph
    }

    /// Find functions by name (may match multiple across files)
    pub fn find_by_name(&self, name: &str) -> Vec<&FunctionId> {
        self.graph
            .node_weights()
            .filter(|f| f.name.as_ref() == name)
            .collect()
    }

    /// Find functions in a specific file
    pub fn functions_in_file(&self, file: &str) -> Vec<&FunctionId> {
        self.graph
            .node_weights()
            .filter(|f| f.file.as_ref() == file)
            .collect()
    }

    /// Merge another graph into this one.
    /// Used when building incrementally from multiple files.
    pub fn merge(&mut self, other: CallGraph) {
        for func in other.graph.node_weights() {
            self.add_function(func.clone());
        }
        for edge in other.graph.edge_references() {
            let source = other.graph.node_weight(edge.source()).unwrap();
            let target = other.graph.node_weight(edge.target()).unwrap();
            self.add_call(source.clone(), target.clone(), edge.weight().clone());
        }
    }

    /// Export edges in the format expected by FocusResolver.expand_via_graph
    /// Returns: Vec<(from_file, from_sym, to_file, to_sym)>
    ///
    /// This allows the call graph to be used for focus expansion:
    /// when user focuses on "foo", we can BFS through call relationships
    /// to find related functions (callers and callees).
    pub fn as_symbol_edges(&self) -> Vec<(Arc<str>, Arc<str>, Arc<str>, Arc<str>)> {
        self.graph
            .edge_indices()
            .filter_map(|e| {
                let (src, tgt) = self.graph.edge_endpoints(e)?;
                let from = self.graph.node_weight(src)?;
                let to = self.graph.node_weight(tgt)?;
                Some((
                    from.file.clone(),
                    from.name.clone(),
                    to.file.clone(),
                    to.name.clone(),
                ))
            })
            .collect()
    }

    /// Compute caller weights: how many unique callers each function has.
    /// Returns map of (file, symbol) -> caller_count.
    ///
    /// Used for boost calculation: heavily-called functions are more important.
    /// This is the "API surface" signal - functions called from many places
    /// are likely public interfaces or critical utilities.
    pub fn caller_weights(&self) -> HashMap<(Arc<str>, Arc<str>), usize> {
        let mut weights = HashMap::new();

        for id in self.graph.node_weights() {
            let key = (id.file.clone(), id.name.clone());
            let caller_count = self.calls_to(id).len();
            if caller_count > 0 {
                weights.insert(key, caller_count);
            }
        }

        weights
    }

    /// Get all function IDs in the graph
    pub fn all_functions(&self) -> impl Iterator<Item = &FunctionId> {
        self.graph.node_weights()
    }
}

impl Default for CallGraph {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_add_function_idempotent() {
        let mut graph = CallGraph::new();
        let f1 = FunctionId::new("test.py", "foo", 10);
        let idx1 = graph.add_function(f1.clone());
        let idx2 = graph.add_function(f1);
        assert_eq!(idx1, idx2);
        assert_eq!(graph.function_count(), 1);
    }

    #[test]
    fn test_add_call() {
        let mut graph = CallGraph::new();
        let caller = FunctionId::new("test.py", "main", 1);
        let callee = FunctionId::new("test.py", "helper", 10);

        graph.add_call(
            caller.clone(),
            callee.clone(),
            CallEdge::new(1.0, "test", 5),
        );

        assert_eq!(graph.function_count(), 2);
        assert_eq!(graph.call_count(), 1);

        let calls = graph.calls_from(&caller);
        assert_eq!(calls.len(), 1);
        assert_eq!(calls[0].0.name.as_ref(), "helper");

        let callers = graph.calls_to(&callee);
        assert_eq!(callers.len(), 1);
        assert_eq!(callers[0].0.name.as_ref(), "main");
    }

    #[test]
    fn test_qualified_name() {
        let method = FunctionId::new("test.py", "process", 10)
            .with_parent("MyClass");
        assert_eq!(method.qualified_name(), "MyClass.process");

        let func = FunctionId::new("test.py", "helper", 20);
        assert_eq!(func.qualified_name(), "helper");
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/callgraph/mod.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Call graph construction with pluggable resolution strategies.
//!
//! The call graph system is fully modular:
//! - Core graph structure is strategy-agnostic
//! - Resolution strategies are plug-and-play
//! - Each signal (types, imports, names) is independent
//! - Strategies can be combined with confidence weighting
//!
//! # Architecture
//!
//! ```text
//! Tags â†’ [Resolvers] â†’ CallGraph â†’ PageRank
//!          â†“
//!    â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
//!    â”‚ Strategies â”‚
//!    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
//!    â”‚ NameMatch â”‚  â† Always available
//!    â”‚ TypeHints â”‚  â† Python, TypeScript
//!    â”‚ Imports   â”‚  â† Cross-file resolution
//!    â”‚ SameFile  â”‚  â† Highest confidence
//!    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
//! ```
//!
//! # Usage
//!
//! ```ignore
//! let mut resolver = CallResolver::new();
//! resolver.add_strategy(Box::new(SameFileStrategy::new()));
//! resolver.add_strategy(Box::new(TypeHintStrategy::new()));
//! resolver.add_strategy(Box::new(NameMatchStrategy::new()));
//!
//! let graph = resolver.build_graph(&tags);
//! ```

mod graph;
mod resolver;
mod strategies;

pub use graph::{CallGraph, CallEdge, FunctionId};
pub use resolver::{CallResolver, ResolverBuilder, ResolverConfig, ResolutionStats};
pub use strategies::{
    ResolutionStrategy,
    ResolutionContext,
    Candidate,
    SameFileStrategy,
    NameMatchStrategy,
    TypeHintStrategy,
    ImportStrategy,
};

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/callgraph/resolver.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Call resolver: orchestrates multiple resolution strategies.
//!
//! The resolver is the brain of call graph construction:
//! 1. Builds context from tags (indexes, type maps)
//! 2. Runs each strategy on each call reference
//! 3. Picks highest-confidence resolution
//! 4. Builds the final CallGraph

use std::collections::HashMap;
use std::sync::Arc;
use crate::types::{Tag, TagKind};
use super::graph::{CallGraph, CallEdge, FunctionId};
use super::strategies::{ResolutionStrategy, ResolutionContext, Candidate};

/// Configuration for the resolver
#[derive(Debug, Clone)]
pub struct ResolverConfig {
    /// Minimum confidence threshold to accept a resolution
    pub min_confidence: f64,
    /// Whether to include unresolved calls as dangling nodes
    pub include_unresolved: bool,
    /// Language hint for strategy filtering (e.g., "python")
    pub language: Option<String>,
}

impl Default for ResolverConfig {
    fn default() -> Self {
        Self {
            min_confidence: 0.3,
            include_unresolved: false,
            language: None,
        }
    }
}

/// The main resolver that combines strategies to build call graphs.
pub struct CallResolver {
    strategies: Vec<Box<dyn ResolutionStrategy>>,
    config: ResolverConfig,
}

impl CallResolver {
    pub fn new() -> Self {
        Self {
            strategies: vec![],
            config: ResolverConfig::default(),
        }
    }

    pub fn with_config(mut self, config: ResolverConfig) -> Self {
        self.config = config;
        self
    }

    /// Add a resolution strategy (order matters for tie-breaking)
    pub fn add_strategy(&mut self, strategy: Box<dyn ResolutionStrategy>) {
        self.strategies.push(strategy);
    }

    /// Convenience builder pattern
    pub fn with_strategy(mut self, strategy: Box<dyn ResolutionStrategy>) -> Self {
        self.add_strategy(strategy);
        self
    }

    /// Build a complete call graph from extracted tags.
    ///
    /// Process:
    /// 1. Build resolution context (indexes, type maps)
    /// 2. Add all definitions as nodes
    /// 3. For each call reference, run strategies
    /// 4. Pick best resolution, add edge
    pub fn build_graph(&self, tags: &[Tag]) -> CallGraph {
        let context = ResolutionContext::new(tags);
        let mut graph = CallGraph::new();

        // Add all function definitions as nodes
        for tag in tags {
            if tag.kind.is_definition() {
                // Only add functions/methods, not classes or variables
                let node_type = tag.node_type.as_ref();
                if node_type.contains("function") || node_type.contains("method") {
                    let id = FunctionId::new(
                        tag.rel_fname.clone(),
                        tag.name.clone(),
                        tag.line,
                    ).with_parent_opt(tag.parent_name.clone());
                    graph.add_function(id);
                }
            }
        }

        // Process each call reference
        for tag in tags {
            if !tag.kind.is_reference() {
                continue;
            }

            // Find the enclosing function (caller)
            let caller = self.find_enclosing_function(tag, tags);
            let Some(caller) = caller else {
                continue; // Call not inside a function
            };

            // Run all strategies, collect candidates
            let mut all_candidates: Vec<(Candidate, &str)> = vec![];

            for strategy in &self.strategies {
                // Skip strategies that don't support this language
                if let Some(ref lang) = self.config.language {
                    if !strategy.supports_language(lang) {
                        continue;
                    }
                }

                let candidates = strategy.resolve(tag, &context);
                for c in candidates {
                    all_candidates.push((c, strategy.name()));
                }
            }

            // Pick the highest confidence candidate
            all_candidates.sort_by(|a, b| {
                b.0.confidence.partial_cmp(&a.0.confidence).unwrap()
            });

            if let Some((best, strategy_name)) = all_candidates.first() {
                if best.confidence >= self.config.min_confidence {
                    let edge = CallEdge::new(
                        best.confidence,
                        *strategy_name,
                        tag.line,
                    );
                    let edge = if let Some(ref hint) = best.type_hint {
                        edge.with_type_hint(hint.clone())
                    } else {
                        edge
                    };

                    graph.add_call(caller, best.target.clone(), edge);
                }
            } else if self.config.include_unresolved {
                // Add unresolved call as a dangling reference
                let unresolved = FunctionId::new(
                    Arc::<str>::from("?"), // Unknown file
                    tag.name.clone(),
                    0,
                );
                let edge = CallEdge::new(0.0, "unresolved", tag.line);
                graph.add_call(caller, unresolved, edge);
            }
        }

        graph
    }

    /// Find the function definition that encloses a given tag.
    /// Uses line number proximity within the same file.
    fn find_enclosing_function(&self, tag: &Tag, all_tags: &[Tag]) -> Option<FunctionId> {
        // Find all function definitions in the same file
        let mut functions: Vec<&Tag> = all_tags
            .iter()
            .filter(|t| {
                t.rel_fname == tag.rel_fname
                    && t.kind.is_definition()
                    && (t.node_type.contains("function") || t.node_type.contains("method"))
            })
            .collect();

        // Sort by line number descending (so we can find closest before)
        functions.sort_by_key(|t| std::cmp::Reverse(t.line));

        // Find the first function defined before this tag's line
        for func in functions {
            if func.line <= tag.line {
                return Some(FunctionId::new(
                    func.rel_fname.clone(),
                    func.name.clone(),
                    func.line,
                ).with_parent_opt(func.parent_name.clone()));
            }
        }

        None
    }

    /// Get statistics about resolution success
    pub fn stats(&self, tags: &[Tag]) -> ResolutionStats {
        let context = ResolutionContext::new(tags);
        let mut stats = ResolutionStats::default();

        for tag in tags {
            if !tag.kind.is_reference() {
                continue;
            }

            stats.total_calls += 1;

            let mut resolved = false;
            for strategy in &self.strategies {
                let candidates = strategy.resolve(tag, &context);
                if !candidates.is_empty() {
                    *stats.by_strategy.entry(strategy.name().to_string()).or_insert(0) += 1;
                    resolved = true;
                    break;
                }
            }

            if !resolved {
                stats.unresolved += 1;
            }
        }

        stats
    }
}

impl Default for CallResolver {
    fn default() -> Self {
        Self::new()
    }
}

/// Statistics about call resolution
#[derive(Debug, Default)]
pub struct ResolutionStats {
    pub total_calls: usize,
    pub unresolved: usize,
    pub by_strategy: HashMap<String, usize>,
}

impl ResolutionStats {
    pub fn resolution_rate(&self) -> f64 {
        if self.total_calls == 0 {
            return 1.0;
        }
        (self.total_calls - self.unresolved) as f64 / self.total_calls as f64
    }
}

/// Builder for creating a fully-configured resolver with default strategies.
pub struct ResolverBuilder {
    config: ResolverConfig,
    same_file: bool,
    type_hints: bool,
    imports: bool,
    name_match: bool,
}

impl ResolverBuilder {
    pub fn new() -> Self {
        Self {
            config: ResolverConfig::default(),
            same_file: true,
            type_hints: true,
            imports: true,
            name_match: true,
        }
    }

    pub fn config(mut self, config: ResolverConfig) -> Self {
        self.config = config;
        self
    }

    pub fn same_file(mut self, enabled: bool) -> Self {
        self.same_file = enabled;
        self
    }

    pub fn type_hints(mut self, enabled: bool) -> Self {
        self.type_hints = enabled;
        self
    }

    pub fn imports(mut self, enabled: bool) -> Self {
        self.imports = enabled;
        self
    }

    pub fn name_match(mut self, enabled: bool) -> Self {
        self.name_match = enabled;
        self
    }

    pub fn build(self) -> CallResolver {
        use super::strategies::*;

        let mut resolver = CallResolver::new().with_config(self.config);

        // Add strategies in order of confidence (highest first)
        if self.same_file {
            resolver.add_strategy(Box::new(SameFileStrategy::new()));
        }
        if self.type_hints {
            resolver.add_strategy(Box::new(TypeHintStrategy::new()));
        }
        if self.imports {
            resolver.add_strategy(Box::new(ImportStrategy::new()));
        }
        if self.name_match {
            resolver.add_strategy(Box::new(NameMatchStrategy::new()));
        }

        resolver
    }
}

impl Default for ResolverBuilder {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn make_def(file: &str, name: &str, line: u32) -> Tag {
        Tag {
            rel_fname: Arc::from(file),
            fname: Arc::from(file),
            line,
            name: Arc::from(name),
            kind: TagKind::Def,
            node_type: Arc::from("function"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        }
    }

    fn make_call(file: &str, name: &str, line: u32) -> Tag {
        Tag {
            rel_fname: Arc::from(file),
            fname: Arc::from(file),
            line,
            name: Arc::from(name),
            kind: TagKind::Ref,
            node_type: Arc::from("call"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        }
    }

    #[test]
    fn test_build_graph() {
        let tags = vec![
            make_def("test.py", "main", 1),
            make_def("test.py", "helper", 10),
            make_call("test.py", "helper", 5), // main calls helper at line 5
        ];

        let resolver = ResolverBuilder::new().build();
        let graph = resolver.build_graph(&tags);

        assert_eq!(graph.function_count(), 2);
        assert_eq!(graph.call_count(), 1);
    }

    #[test]
    fn test_resolution_stats() {
        let tags = vec![
            make_def("test.py", "main", 1),
            make_def("test.py", "helper", 10),
            make_call("test.py", "helper", 5),
            make_call("test.py", "unknown", 7), // Unresolved
        ];

        let resolver = ResolverBuilder::new()
            .name_match(false) // Disable name matching to test unresolved
            .build();
        let stats = resolver.stats(&tags);

        assert_eq!(stats.total_calls, 2);
        assert!(stats.resolution_rate() > 0.0);
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/callgraph/strategies.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Pluggable resolution strategies for call graph construction.
//!
//! Each strategy is an independent signal that can be enabled/disabled.
//! Strategies have confidence levels - higher confidence wins.
//!
//! # Strategy Hierarchy (by confidence)
//!
//! 1. SameFileStrategy (0.9) - caller and callee in same file
//! 2. TypeHintStrategy (0.85) - resolved via type annotations
//! 3. ImportStrategy (0.8) - resolved via import statements
//! 4. NameMatchStrategy (0.5) - fuzzy name matching (fallback)

use std::collections::HashMap;
use std::sync::Arc;
use crate::types::Tag;
use super::graph::FunctionId;

/// A resolution candidate with confidence score.
#[derive(Debug, Clone)]
pub struct Candidate {
    pub target: FunctionId,
    pub confidence: f64,
    pub type_hint: Option<String>,
}

/// Context passed to resolution strategies.
/// Contains all extracted tags and derived indexes.
pub struct ResolutionContext<'a> {
    /// All extracted tags
    pub tags: &'a [Tag],
    /// Function definitions indexed by name
    pub definitions: HashMap<&'a str, Vec<&'a Tag>>,
    /// Type annotations: variable -> type
    pub type_map: HashMap<String, String>,
    /// Import map: imported_name -> (module, original_name)
    pub imports: HashMap<String, (String, String)>,
}

impl<'a> ResolutionContext<'a> {
    /// Build context from tags, extracting indexes for fast lookup.
    pub fn new(tags: &'a [Tag]) -> Self {
        let mut definitions: HashMap<&str, Vec<&Tag>> = HashMap::new();
        let mut type_map = HashMap::new();
        let mut imports = HashMap::new();

        for tag in tags {
            // Index definitions by name
            if tag.kind.is_definition() {
                definitions.entry(tag.name.as_ref()).or_default().push(tag);
            }

            // Extract type annotations from tag metadata
            // These come from tree-sitter captures like @var.type
            if let Some(ref meta) = tag.metadata {
                if let Some(var_type) = meta.get("var_type") {
                    if let Some(var_name) = meta.get("var_name") {
                        type_map.insert(var_name.clone(), var_type.clone());
                    }
                }
                if let Some(receiver_type) = meta.get("receiver_type") {
                    if let Some(receiver) = meta.get("receiver") {
                        type_map.insert(receiver.clone(), receiver_type.clone());
                    }
                }
                // Import tracking
                if let Some(import_module) = meta.get("import_module") {
                    if let Some(import_name) = meta.get("import_name") {
                        imports.insert(
                            import_name.clone(),
                            (import_module.clone(), import_name.clone()),
                        );
                    }
                }
            }
        }

        Self {
            tags,
            definitions,
            type_map,
            imports,
        }
    }

    /// Find definitions matching a name
    pub fn find_definitions(&self, name: &str) -> Vec<&Tag> {
        self.definitions.get(name).cloned().unwrap_or_default()
    }

    /// Get type for a variable/receiver
    pub fn get_type(&self, name: &str) -> Option<&String> {
        self.type_map.get(name)
    }

    /// Check if name is imported, return source module
    pub fn get_import(&self, name: &str) -> Option<&(String, String)> {
        self.imports.get(name)
    }
}

/// Trait for pluggable resolution strategies.
///
/// Each strategy implements one heuristic for resolving calls.
/// The resolver combines all enabled strategies, picking highest confidence.
pub trait ResolutionStrategy: Send + Sync {
    /// Strategy name for debugging/display
    fn name(&self) -> &'static str;

    /// Attempt to resolve a call reference to its definition.
    /// Returns all possible candidates with confidence scores.
    fn resolve(
        &self,
        call: &Tag,
        context: &ResolutionContext,
    ) -> Vec<Candidate>;

    /// Whether this strategy applies to the given language
    fn supports_language(&self, lang: &str) -> bool {
        // Default: support all languages
        let _ = lang;
        true
    }
}

// =============================================================================
// Strategy Implementations
// =============================================================================

/// Highest confidence: definitions in the same file.
///
/// If foo() is called and foo is defined in the same file, it's almost
/// certainly that definition (unless shadowed, which we ignore for now).
pub struct SameFileStrategy {
    pub confidence: f64,
}

impl SameFileStrategy {
    pub fn new() -> Self {
        Self { confidence: 0.9 }
    }
}

impl Default for SameFileStrategy {
    fn default() -> Self {
        Self::new()
    }
}

impl ResolutionStrategy for SameFileStrategy {
    fn name(&self) -> &'static str {
        "same_file"
    }

    fn resolve(&self, call: &Tag, context: &ResolutionContext) -> Vec<Candidate> {
        let defs = context.find_definitions(&call.name);

        defs.into_iter()
            .filter(|def| def.rel_fname == call.rel_fname)
            .map(|def| Candidate {
                target: FunctionId::new(
                    def.rel_fname.clone(),
                    def.name.clone(),
                    def.line,
                ).with_parent_opt(def.parent_name.clone()),
                confidence: self.confidence,
                type_hint: None,
            })
            .collect()
    }
}

/// Type hint resolution: use Python/TypeScript type annotations.
///
/// Given `x: MyClass` and `x.method()`, resolve to `MyClass.method`.
pub struct TypeHintStrategy {
    pub confidence: f64,
}

impl TypeHintStrategy {
    pub fn new() -> Self {
        Self { confidence: 0.85 }
    }
}

impl Default for TypeHintStrategy {
    fn default() -> Self {
        Self::new()
    }
}

impl ResolutionStrategy for TypeHintStrategy {
    fn name(&self) -> &'static str {
        "type_hint"
    }

    fn supports_language(&self, lang: &str) -> bool {
        matches!(lang, "python" | "typescript" | "tsx")
    }

    fn resolve(&self, call: &Tag, context: &ResolutionContext) -> Vec<Candidate> {
        // Only applies to method calls with a receiver
        let receiver = call.metadata.as_ref()
            .and_then(|m| m.get("receiver"))
            .map(|s| s.as_str());

        let Some(receiver) = receiver else {
            return vec![];
        };

        // Look up the receiver's type
        let Some(receiver_type) = context.get_type(receiver) else {
            return vec![];
        };

        // Find method definitions in that class
        let defs = context.find_definitions(&call.name);

        defs.into_iter()
            .filter(|def| {
                // Check if this definition is a method of the receiver's type
                def.parent_name.as_ref().map_or(false, |p| p.as_ref() == receiver_type)
            })
            .map(|def| Candidate {
                target: FunctionId::new(
                    def.rel_fname.clone(),
                    def.name.clone(),
                    def.line,
                ).with_parent(receiver_type.as_str()),
                confidence: self.confidence,
                type_hint: Some(format!("{}: {}", receiver, receiver_type)),
            })
            .collect()
    }
}

/// Import-based resolution: track what's imported from where.
///
/// Given `from mymodule import helper` and call to `helper()`,
/// resolve to the definition in mymodule.
pub struct ImportStrategy {
    pub confidence: f64,
}

impl ImportStrategy {
    pub fn new() -> Self {
        Self { confidence: 0.8 }
    }
}

impl Default for ImportStrategy {
    fn default() -> Self {
        Self::new()
    }
}

impl ResolutionStrategy for ImportStrategy {
    fn name(&self) -> &'static str {
        "import"
    }

    fn resolve(&self, call: &Tag, context: &ResolutionContext) -> Vec<Candidate> {
        // Check if this call target is imported
        let Some((module, original_name)) = context.get_import(&call.name) else {
            return vec![];
        };

        // Find definitions in files that match the module path
        let defs = context.find_definitions(original_name);

        defs.into_iter()
            .filter(|def| {
                // Check if the file path contains the module name
                // e.g., "mypackage/utils.py" matches import from "mypackage.utils"
                let module_path = module.replace('.', "/");
                def.rel_fname.contains(&module_path)
            })
            .map(|def| Candidate {
                target: FunctionId::new(
                    def.rel_fname.clone(),
                    def.name.clone(),
                    def.line,
                ).with_parent_opt(def.parent_name.clone()),
                confidence: self.confidence,
                type_hint: Some(format!("from {} import {}", module, original_name)),
            })
            .collect()
    }
}

/// Fallback: name matching across the codebase.
///
/// If no other strategy resolves, match by name alone.
/// Lower confidence because name collisions are common.
pub struct NameMatchStrategy {
    pub confidence: f64,
    /// Prefer matches in "nearby" files (same directory)
    pub proximity_boost: f64,
}

impl NameMatchStrategy {
    pub fn new() -> Self {
        Self {
            confidence: 0.5,
            proximity_boost: 0.1,
        }
    }
}

impl Default for NameMatchStrategy {
    fn default() -> Self {
        Self::new()
    }
}

impl ResolutionStrategy for NameMatchStrategy {
    fn name(&self) -> &'static str {
        "name_match"
    }

    fn resolve(&self, call: &Tag, context: &ResolutionContext) -> Vec<Candidate> {
        let defs = context.find_definitions(&call.name);
        let call_dir = std::path::Path::new(call.rel_fname.as_ref())
            .parent()
            .map(|p| p.to_string_lossy().to_string());

        defs.into_iter()
            .map(|def| {
                // Boost confidence for same-directory matches
                let def_dir = std::path::Path::new(def.rel_fname.as_ref())
                    .parent()
                    .map(|p| p.to_string_lossy().to_string());

                let confidence = if call_dir == def_dir {
                    self.confidence + self.proximity_boost
                } else {
                    self.confidence
                };

                Candidate {
                    target: FunctionId::new(
                        def.rel_fname.clone(),
                        def.name.clone(),
                        def.line,
                    ).with_parent_opt(def.parent_name.clone()),
                    confidence,
                    type_hint: None,
                }
            })
            .collect()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::types::TagKind;

    fn make_def(file: &str, name: &str, line: u32, parent: Option<&str>) -> Tag {
        Tag {
            rel_fname: Arc::from(file),
            fname: Arc::from(file),
            line,
            name: Arc::from(name),
            kind: TagKind::Def,
            node_type: Arc::from("function"),
            parent_name: parent.map(|s| Arc::from(s)),
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        }
    }

    fn make_call(file: &str, name: &str, line: u32, receiver: Option<&str>) -> Tag {
        let metadata = receiver.map(|r| {
            let mut m = HashMap::new();
            m.insert("receiver".to_string(), r.to_string());
            m
        });

        Tag {
            rel_fname: Arc::from(file),
            fname: Arc::from(file),
            line,
            name: Arc::from(name),
            kind: TagKind::Ref,
            node_type: Arc::from("call"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata,
        }
    }

    #[test]
    fn test_same_file_strategy() {
        let tags = vec![
            make_def("test.py", "helper", 5, None),
            make_def("other.py", "helper", 10, None),
            make_call("test.py", "helper", 20, None),
        ];
        let ctx = ResolutionContext::new(&tags);
        let strategy = SameFileStrategy::new();

        let call = &tags[2];
        let candidates = strategy.resolve(call, &ctx);

        assert_eq!(candidates.len(), 1);
        assert_eq!(candidates[0].target.file.as_ref(), "test.py");
        assert_eq!(candidates[0].target.line, 5);
    }

    #[test]
    fn test_name_match_strategy() {
        let tags = vec![
            make_def("test.py", "helper", 5, None),
            make_def("other.py", "helper", 10, None),
            make_call("another.py", "helper", 20, None),
        ];
        let ctx = ResolutionContext::new(&tags);
        let strategy = NameMatchStrategy::new();

        let call = &tags[2];
        let candidates = strategy.resolve(call, &ctx);

        assert_eq!(candidates.len(), 2);
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/discovery/files.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Git-aware file discovery with parallel traversal.
//!
//! This module implements efficient file discovery that:
//! - Respects .gitignore automatically via the `ignore` crate
//! - Filters out binary files, images, archives, etc.
//! - Uses parallel walking for speed on large codebases
//! - Returns deterministic (sorted) results
//!
//! Design rationale:
//! - The `ignore` crate provides battle-tested .gitignore handling from ripgrep
//! - WalkBuilder with threads(0) auto-detects optimal parallelism
//! - Extension filtering prevents wasting cycles on non-source files
//! - Sorting ensures cache hits and reproducible output

use std::path::{Path, PathBuf};
use anyhow::Result;
use ignore::WalkBuilder;

/// File extensions excluded from discovery.
///
/// Rationale: These are binary/generated files that don't benefit from
/// semantic analysis. Including them would:
/// - Waste CPU parsing binary data
/// - Pollute the symbol graph with noise
/// - Slow down cartography without adding value
///
/// Note: Lock files (Cargo.lock, package-lock.json) are excluded because
/// they're generated and contain thousands of dependency entries that
/// would dominate the graph. The actual dependency structure is in
/// Cargo.toml/package.json which ARE included.
const EXCLUDED_EXTENSIONS: &[&str] = &[
    // Images
    "png", "jpg", "jpeg", "gif", "ico", "svg", "webp", "bmp", "tiff",
    // Fonts
    "woff", "woff2", "ttf", "eot", "otf",
    // Media
    "mp3", "mp4", "wav", "ogg", "webm", "avi", "mov", "flac",
    // Archives
    "zip", "tar", "gz", "rar", "7z", "bz2", "xz", "tgz",
    // Documents
    "pdf", "doc", "docx", "xls", "xlsx", "ppt", "pptx",
    // Compiled/Binary
    "pyc", "pyo", "so", "dylib", "dll", "exe", "o", "a", "lib",
    "class", "jar", "war", "ear",
    // Lock files (generated, high entropy, low signal)
    "lock", "sum",
    // Database files
    "db", "sqlite", "sqlite3",
    // Misc binary
    "wasm", "bin", "dat",
];

/// Find source files in a directory, respecting .gitignore.
///
/// This is the main entry point for file discovery. It uses the `ignore`
/// crate (from ripgrep) to provide git-aware traversal with:
/// - Automatic .gitignore respect
/// - Parallel walking for performance
/// - Standard ignore patterns (hidden files, .git/, etc.)
///
/// ## Arguments
/// - `directory`: Root path to scan (can be file or directory)
/// - `include_all`: If true, bypass extension filtering (for diagnostics)
///
/// ## Returns
/// Sorted vector of absolute paths to source files.
///
/// ## Performance
/// On a 10k file codebase, parallel walking takes ~5-10ms vs ~50ms sequential.
/// The sorting overhead (~1ms) is worth it for reproducibility.
pub fn find_source_files(directory: &Path, include_all: bool) -> Result<Vec<PathBuf>> {
    // Handle single file case early
    if directory.is_file() {
        return Ok(vec![directory.to_path_buf()]);
    }

    if !directory.is_dir() {
        anyhow::bail!("Path does not exist: {}", directory.display());
    }

    // Build parallel walker with sensible defaults
    // threads(0) = auto-detect based on CPU count
    let walker = WalkBuilder::new(directory)
        .hidden(false)          // Don't automatically skip hidden files (let .gitignore decide)
        .git_ignore(true)       // Respect .gitignore
        .git_global(true)       // Respect global gitignore
        .git_exclude(true)      // Respect .git/info/exclude
        .require_git(false)     // Work even in non-git directories
        .follow_links(false)    // Don't follow symlinks (avoid cycles)
        .threads(0)             // Auto-detect thread count for parallelism
        .build_parallel();

    // Collect files in parallel
    // Using a Vec and later sorting is faster than maintaining sorted order during traversal
    let files = std::sync::Mutex::new(Vec::new());

    walker.run(|| {
        Box::new(|entry_result| {
            // Process each directory entry
            match entry_result {
                Ok(entry) => {
                    let path = entry.path();

                    // Only process files (skip directories)
                    if !path.is_file() {
                        return ignore::WalkState::Continue;
                    }

                    // Apply extension filter unless include_all is set
                    if !include_all && is_excluded_by_extension(path) {
                        return ignore::WalkState::Continue;
                    }

                    // Add to results
                    if let Ok(mut files) = files.lock() {
                        files.push(path.to_path_buf());
                    }

                    ignore::WalkState::Continue
                }
                Err(_) => {
                    // Skip entries we can't read (permissions, broken symlinks, etc.)
                    // This matches the Python behavior of silently skipping inaccessible files
                    ignore::WalkState::Continue
                }
            }
        })
    });

    // Extract results and sort for determinism
    let mut files = files.into_inner()
        .map_err(|_| anyhow::anyhow!("Failed to unwrap mutex"))?;

    // Sort for reproducibility - critical for caching!
    // Without this, the same directory could yield different orderings
    // across runs, breaking cache invalidation logic.
    files.sort();

    Ok(files)
}

/// Check if a file should be excluded based on its extension.
///
/// This is an optimization to avoid processing binary files that won't
/// contribute to the semantic graph. We check the extension rather than
/// file contents because:
/// - Extension checking is O(1) vs O(n) for content sniffing
/// - False positives are rare in practice
/// - We can always override with include_all=true
fn is_excluded_by_extension(path: &Path) -> bool {
    if let Some(ext) = path.extension() {
        if let Some(ext_str) = ext.to_str() {
            let ext_lower = ext_str.to_ascii_lowercase();
            return EXCLUDED_EXTENSIONS.contains(&ext_lower.as_str());
        }
    }
    false
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::fs;

    #[test]
    fn test_extension_filtering() {
        assert!(is_excluded_by_extension(Path::new("image.png")));
        assert!(is_excluded_by_extension(Path::new("font.woff2")));
        assert!(is_excluded_by_extension(Path::new("video.mp4")));
        assert!(is_excluded_by_extension(Path::new("archive.zip")));
        assert!(is_excluded_by_extension(Path::new("Cargo.lock")));

        assert!(!is_excluded_by_extension(Path::new("main.rs")));
        assert!(!is_excluded_by_extension(Path::new("lib.py")));
        assert!(!is_excluded_by_extension(Path::new("README.md")));
        assert!(!is_excluded_by_extension(Path::new("Cargo.toml")));
    }

    #[test]
    fn test_case_insensitive_extension() {
        // Extension matching should be case-insensitive
        assert!(is_excluded_by_extension(Path::new("IMAGE.PNG")));
        assert!(is_excluded_by_extension(Path::new("Image.Png")));
    }

    #[test]
    fn test_single_file_input() -> Result<()> {
        // Create a temporary file
        let temp_dir = std::env::temp_dir();
        let test_file = temp_dir.join("test_ripmap_single.txt");
        fs::write(&test_file, "test")?;

        let result = find_source_files(&test_file, false)?;
        assert_eq!(result.len(), 1);
        assert_eq!(result[0], test_file);

        fs::remove_file(test_file)?;
        Ok(())
    }

    #[test]
    fn test_nonexistent_path() {
        let result = find_source_files(Path::new("/nonexistent/path/xyz"), false);
        assert!(result.is_err());
    }

    #[test]
    fn test_discovery_on_ripmap_codebase() -> Result<()> {
        // Test discovery on the ripmap codebase itself
        let repo_root = Path::new(".");
        let files = find_source_files(repo_root, false)?;

        // Should find at least some Rust files
        assert!(!files.is_empty(), "Should discover source files in ripmap repo");

        // Should include our discovery module
        let has_discovery = files.iter().any(|f| {
            f.to_string_lossy().contains("discovery/files.rs")
        });
        assert!(has_discovery, "Should find discovery/files.rs");

        // Should NOT include lock files
        let has_lock = files.iter().any(|f| {
            f.to_string_lossy().ends_with("Cargo.lock")
        });
        assert!(!has_lock, "Should exclude Cargo.lock");

        // Should be sorted for determinism
        let mut sorted_files = files.clone();
        sorted_files.sort();
        assert_eq!(files, sorted_files, "Results should be sorted");

        Ok(())
    }

    #[test]
    fn test_include_all_flag() -> Result<()> {
        // Create temporary directory with various file types
        let temp_dir = std::env::temp_dir().join("ripmap_test_include_all");
        fs::create_dir_all(&temp_dir)?;

        // Create test files
        fs::write(temp_dir.join("source.rs"), "fn main() {}")?;
        fs::write(temp_dir.join("image.png"), "fake png")?;
        fs::write(temp_dir.join("data.lock"), "lock data")?;

        // Test with include_all = false
        let files_filtered = find_source_files(&temp_dir, false)?;
        let has_png_filtered = files_filtered.iter()
            .any(|f| f.to_string_lossy().ends_with(".png"));
        let has_lock_filtered = files_filtered.iter()
            .any(|f| f.to_string_lossy().ends_with(".lock"));

        assert!(!has_png_filtered, "PNG should be filtered out");
        assert!(!has_lock_filtered, "Lock should be filtered out");

        // Test with include_all = true
        let files_all = find_source_files(&temp_dir, true)?;
        let has_png_all = files_all.iter()
            .any(|f| f.to_string_lossy().ends_with(".png"));
        let has_lock_all = files_all.iter()
            .any(|f| f.to_string_lossy().ends_with(".lock"));

        assert!(has_png_all, "PNG should be included with include_all");
        assert!(has_lock_all, "Lock should be included with include_all");

        // Cleanup
        fs::remove_dir_all(temp_dir)?;

        Ok(())
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/discovery/mod.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Git-aware file discovery.
//!
//! Uses the `ignore` crate to respect .gitignore and walk directories
//! efficiently. Parallel traversal with rayon.

mod files;

pub use files::find_source_files;

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/extraction/fields.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Class field extraction stub.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/extraction/mod.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Tag extraction from source code using tree-sitter.
//!
//! This module handles:
//! - Loading tree-sitter parsers and queries
//! - Parsing source files into ASTs
//! - Running .scm queries to extract symbol tags
//! - Walking AST for parent scopes
//! - Extracting function signatures and class fields
//!
//! # Parser Selection
//!
//! The module provides two parsing strategies:
//! - `TreeSitterParser`: Full AST parsing with .scm queries (preferred)
//! - `Parser`: Regex-based fallback for unsupported languages

mod parser;
mod tags;
mod signatures;
mod fields;
mod treesitter;

pub use parser::Parser;
pub use tags::extract_tags;
pub use treesitter::{TreeSitterParser, extension_to_language};

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/extraction/parser.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Tree-sitter parser with regex fallback.
//!
//! This module provides source code parsing for tag extraction.
//! Currently implements a **regex-based fallback** that handles common patterns
//! across Python, Rust, and JavaScript. Tree-sitter integration will be added
//! later for higher accuracy and AST-based extraction.
//!
//! Design rationale:
//! - Regex is "good enough" for 80% of cases and gets us shipping fast
//! - Patterns focus on DEFINITIONS (functions, classes, methods)
//! - Line number tracking via byte offset -> newline counting
//! - Each language gets its own parser function to keep patterns clean

use std::path::Path;
use std::sync::Arc;
use crate::types::{Tag, TagKind};
use anyhow::Result;
use regex::Regex;
use once_cell::sync::Lazy;

/// Main parser struct - currently stateless, but allows future tree-sitter state.
pub struct Parser;

impl Parser {
    pub fn new() -> Self {
        Self
    }

    /// Parse a file and extract tags using regex patterns.
    /// This is a fallback - tree-sitter will be added later for accuracy.
    ///
    /// Returns a vector of tags representing symbol definitions found in the file.
    pub fn parse_file(&self, path: &Path, rel_fname: &str) -> Result<Vec<Tag>> {
        let content = std::fs::read_to_string(path)?;
        let lang = detect_language(path);

        match lang {
            Language::Python => parse_python(&content, path, rel_fname),
            Language::Rust => parse_rust(&content, path, rel_fname),
            Language::JavaScript => parse_javascript(&content, path, rel_fname),
            Language::TypeScript => parse_typescript(&content, path, rel_fname),
            Language::Unknown => Ok(vec![]),
        }
    }
}

impl Default for Parser {
    fn default() -> Self {
        Self::new()
    }
}

/// Supported languages for regex extraction.
/// Each language has its own pattern set optimized for common definitions.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
enum Language {
    Python,
    Rust,
    JavaScript,
    TypeScript,
    Unknown,
}

/// Detect language from file extension.
/// Simple but effective - maps extensions to Language enum.
fn detect_language(path: &Path) -> Language {
    match path.extension().and_then(|e| e.to_str()) {
        Some("py") => Language::Python,
        Some("rs") => Language::Rust,
        Some("js" | "jsx") => Language::JavaScript,
        Some("ts" | "tsx") => Language::TypeScript,
        _ => Language::Unknown,
    }
}

/// Calculate 1-indexed line number from byte offset.
/// Counts newlines before the offset to determine the line.
fn line_number(content: &str, byte_offset: usize) -> u32 {
    content[..byte_offset].matches('\n').count() as u32 + 1
}

// ============================================================================
// PYTHON PARSING
// ============================================================================

/// Regex patterns for Python symbol extraction.
/// Cached as static to avoid recompilation on every parse.
mod python_patterns {
    use super::*;

    /// Match class definitions: `class Foo:` or `class Foo(Bar):`
    pub static CLASS: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^class\s+(\w+)").expect("Invalid Python class regex")
    });

    /// Match top-level function definitions: `def foo(`
    pub static FUNCTION: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^def\s+(\w+)\s*\(").expect("Invalid Python function regex")
    });

    /// Match method definitions (indented): `    def bar(`
    pub static METHOD: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^(?:    |\t)def\s+(\w+)\s*\(").expect("Invalid Python method regex")
    });

    /// Match top-level assignments: `FOO = ...` (constants/globals)
    pub static ASSIGNMENT: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^([A-Z_][A-Z0-9_]*)\s*=").expect("Invalid Python assignment regex")
    });
}

/// Parse Python source code for class/function/method definitions.
///
/// Extraction strategy:
/// - Classes: `class Name:` patterns at start of line
/// - Functions: `def name(` at start of line (top-level)
/// - Methods: `def name(` indented (inside classes)
/// - Constants: `UPPERCASE_NAME =` at start of line
///
/// Limitations (to be fixed with tree-sitter):
/// - Can't accurately determine parent scope (which class owns which method)
/// - Doesn't handle nested functions
/// - May miss async def, decorators
fn parse_python(content: &str, path: &Path, rel_fname: &str) -> Result<Vec<Tag>> {
    let mut tags = Vec::new();
    let fname: Arc<str> = Arc::from(path.to_string_lossy().into_owned());
    let rel: Arc<str> = Arc::from(rel_fname);

    // Extract classes
    for cap in python_patterns::CLASS.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("class"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None, // TODO: Extract class fields when tree-sitter lands
        metadata: None,
        });
    }

    // Extract top-level functions
    for cap in python_patterns::FUNCTION.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("function"),
            parent_name: None,
            parent_line: None,
            signature: None, // TODO: Extract signature params when tree-sitter lands
            fields: None,
            metadata: None,
        });
    }

    // Extract methods (indented functions - likely class methods)
    for cap in python_patterns::METHOD.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("method"),
            parent_name: None, // TODO: Link to parent class when tree-sitter lands
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    // Extract top-level constants/globals (UPPERCASE assignments)
    for cap in python_patterns::ASSIGNMENT.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("constant"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    Ok(tags)
}

// ============================================================================
// RUST PARSING
// ============================================================================

/// Regex patterns for Rust symbol extraction.
mod rust_patterns {
    use super::*;

    /// Match function definitions: `fn foo(` or `pub fn foo<T>(`
    pub static FUNCTION: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s*(?:pub\s+)?(?:async\s+)?fn\s+(\w+)").expect("Invalid Rust fn regex")
    });

    /// Match struct definitions: `struct Foo` or `pub struct Foo<T>`
    pub static STRUCT: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s*(?:pub\s+)?struct\s+(\w+)").expect("Invalid Rust struct regex")
    });

    /// Match enum definitions: `enum Bar` or `pub enum Bar`
    pub static ENUM: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s*(?:pub\s+)?enum\s+(\w+)").expect("Invalid Rust enum regex")
    });

    /// Match trait definitions: `trait Baz` or `pub trait Baz`
    pub static TRAIT: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s*(?:pub\s+)?trait\s+(\w+)").expect("Invalid Rust trait regex")
    });

    /// Match impl blocks: `impl Foo` or `impl Trait for Foo`
    pub static IMPL: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s*impl(?:\s+\w+\s+for)?\s+(\w+)").expect("Invalid Rust impl regex")
    });

    /// Match const definitions: `const FOO:` or `pub const FOO:`
    pub static CONST: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s*(?:pub\s+)?const\s+(\w+)").expect("Invalid Rust const regex")
    });

    /// Match static definitions: `static BAR:` or `pub static BAR:`
    pub static STATIC: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s*(?:pub\s+)?static\s+(\w+)").expect("Invalid Rust static regex")
    });
}

/// Parse Rust source code for function/struct/enum/trait definitions.
///
/// Extraction strategy:
/// - Functions: `fn name` patterns (handles pub/async modifiers)
/// - Structs: `struct Name` patterns
/// - Enums: `enum Name` patterns
/// - Traits: `trait Name` patterns
/// - Impls: `impl Name` patterns (for tracking methods)
/// - Constants: `const NAME:` patterns
/// - Statics: `static NAME:` patterns
///
/// Limitations (to be fixed with tree-sitter):
/// - Can't link impl block methods to their parent struct
/// - Doesn't extract generic parameters
/// - May miss some visibility modifiers
fn parse_rust(content: &str, path: &Path, rel_fname: &str) -> Result<Vec<Tag>> {
    let mut tags = Vec::new();
    let fname: Arc<str> = Arc::from(path.to_string_lossy().into_owned());
    let rel: Arc<str> = Arc::from(rel_fname);

    // Extract functions
    for cap in rust_patterns::FUNCTION.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("function"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    // Extract structs
    for cap in rust_patterns::STRUCT.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("struct"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None, // TODO: Extract struct fields when tree-sitter lands
        metadata: None,
        });
    }

    // Extract enums
    for cap in rust_patterns::ENUM.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("enum"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    // Extract traits
    for cap in rust_patterns::TRAIT.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("trait"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    // Extract impl blocks (for tracking methods later)
    for cap in rust_patterns::IMPL.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("impl"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    // Extract constants
    for cap in rust_patterns::CONST.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("const"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    // Extract statics
    for cap in rust_patterns::STATIC.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("static"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    Ok(tags)
}

// ============================================================================
// JAVASCRIPT PARSING
// ============================================================================

/// Regex patterns for JavaScript symbol extraction.
mod js_patterns {
    use super::*;

    /// Match function declarations: `function foo(` or `async function foo(`
    pub static FUNCTION: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s*(?:async\s+)?function\s+(\w+)\s*\(").expect("Invalid JS function regex")
    });

    /// Match class definitions: `class Foo` or `export class Foo`
    pub static CLASS: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s*(?:export\s+)?class\s+(\w+)").expect("Invalid JS class regex")
    });

    /// Match const arrow functions: `const foo = (`
    pub static CONST_ARROW: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s*(?:export\s+)?const\s+(\w+)\s*=\s*(?:async\s*)?\(").expect("Invalid JS const arrow regex")
    });

    /// Match const regular assignments: `const FOO = ...`
    pub static CONST_ASSIGN: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s*(?:export\s+)?const\s+([A-Z_][A-Z0-9_]*)\s*=").expect("Invalid JS const regex")
    });

    /// Match method definitions in classes: `  methodName(` or `  async methodName(`
    pub static METHOD: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s+(?:async\s+)?(\w+)\s*\(").expect("Invalid JS method regex")
    });
}

/// Parse JavaScript source code for class/function definitions.
///
/// Extraction strategy:
/// - Functions: `function name(` patterns
/// - Classes: `class Name` patterns
/// - Arrow functions: `const name = (` patterns
/// - Constants: `const UPPERCASE_NAME =` patterns
/// - Methods: indented `methodName(` inside classes
///
/// Limitations (to be fixed with tree-sitter):
/// - Can't distinguish between arrow functions and other const assignments
/// - Can't accurately link methods to their parent class
/// - Doesn't handle destructuring assignments
fn parse_javascript(content: &str, path: &Path, rel_fname: &str) -> Result<Vec<Tag>> {
    let mut tags = Vec::new();
    let fname: Arc<str> = Arc::from(path.to_string_lossy().into_owned());
    let rel: Arc<str> = Arc::from(rel_fname);

    // Extract classes
    for cap in js_patterns::CLASS.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("class"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    // Extract function declarations
    for cap in js_patterns::FUNCTION.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("function"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    // Extract const arrow functions
    for cap in js_patterns::CONST_ARROW.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("function"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    // Extract const assignments (constants)
    for cap in js_patterns::CONST_ASSIGN.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("constant"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    Ok(tags)
}

// ============================================================================
// TYPESCRIPT PARSING
// ============================================================================

/// Regex patterns for TypeScript symbol extraction.
/// TypeScript extends JavaScript patterns with type annotations.
mod ts_patterns {
    use super::*;

    /// Match interface definitions: `interface Foo` or `export interface Foo`
    pub static INTERFACE: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s*(?:export\s+)?interface\s+(\w+)").expect("Invalid TS interface regex")
    });

    /// Match type aliases: `type Foo =` or `export type Foo =`
    pub static TYPE_ALIAS: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s*(?:export\s+)?type\s+(\w+)\s*=").expect("Invalid TS type regex")
    });

    /// Match enum definitions: `enum Color` or `export enum Color`
    pub static ENUM: Lazy<Regex> = Lazy::new(|| {
        Regex::new(r"(?m)^\s*(?:export\s+)?enum\s+(\w+)").expect("Invalid TS enum regex")
    });
}

/// Parse TypeScript source code for type/interface/enum definitions.
///
/// TypeScript gets both JavaScript patterns (classes, functions) PLUS
/// type-specific patterns (interfaces, type aliases, enums).
fn parse_typescript(content: &str, path: &Path, rel_fname: &str) -> Result<Vec<Tag>> {
    // Start with JavaScript patterns
    let mut tags = parse_javascript(content, path, rel_fname)?;

    let fname: Arc<str> = Arc::from(path.to_string_lossy().into_owned());
    let rel: Arc<str> = Arc::from(rel_fname);

    // Add TypeScript-specific patterns

    // Extract interfaces
    for cap in ts_patterns::INTERFACE.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("interface"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    // Extract type aliases
    for cap in ts_patterns::TYPE_ALIAS.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("type"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    // Extract enums
    for cap in ts_patterns::ENUM.captures_iter(content) {
        let line = line_number(content, cap.get(0).unwrap().start());
        tags.push(Tag {
            rel_fname: rel.clone(),
            fname: fname.clone(),
            line,
            name: Arc::from(&cap[1]),
            kind: TagKind::Def,
            node_type: Arc::from("enum"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        });
    }

    Ok(tags)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_line_number() {
        let content = "line 1\nline 2\nline 3\n";
        assert_eq!(line_number(content, 0), 1);
        assert_eq!(line_number(content, 7), 2);
        assert_eq!(line_number(content, 14), 3);
    }

    #[test]
    fn test_detect_language() {
        assert_eq!(detect_language(Path::new("foo.py")), Language::Python);
        assert_eq!(detect_language(Path::new("bar.rs")), Language::Rust);
        assert_eq!(detect_language(Path::new("baz.js")), Language::JavaScript);
        assert_eq!(detect_language(Path::new("qux.ts")), Language::TypeScript);
        assert_eq!(detect_language(Path::new("unknown.txt")), Language::Unknown);
    }

    #[test]
    fn test_parse_python_class() {
        let content = "class Foo:\n    pass\n";
        let path = Path::new("test.py");
        let tags = parse_python(content, path, "test.py").unwrap();
        assert_eq!(tags.len(), 1);
        assert_eq!(tags[0].name.as_ref(), "Foo");
        assert_eq!(tags[0].node_type.as_ref(), "class");
        assert_eq!(tags[0].line, 1);
    }

    #[test]
    fn test_parse_python_function() {
        let content = "def bar():\n    pass\n";
        let path = Path::new("test.py");
        let tags = parse_python(content, path, "test.py").unwrap();
        assert_eq!(tags.len(), 1);
        assert_eq!(tags[0].name.as_ref(), "bar");
        assert_eq!(tags[0].node_type.as_ref(), "function");
    }

    #[test]
    fn test_parse_rust_function() {
        let content = "pub fn foo() {}\n";
        let path = Path::new("test.rs");
        let tags = parse_rust(content, path, "test.rs").unwrap();
        assert_eq!(tags.len(), 1);
        assert_eq!(tags[0].name.as_ref(), "foo");
        assert_eq!(tags[0].node_type.as_ref(), "function");
    }

    #[test]
    fn test_parse_rust_struct() {
        let content = "pub struct Bar {\n    field: i32\n}\n";
        let path = Path::new("test.rs");
        let tags = parse_rust(content, path, "test.rs").unwrap();
        assert_eq!(tags.len(), 1);
        assert_eq!(tags[0].name.as_ref(), "Bar");
        assert_eq!(tags[0].node_type.as_ref(), "struct");
    }

    #[test]
    fn test_parse_javascript_class() {
        let content = "class MyClass {\n  constructor() {}\n}\n";
        let path = Path::new("test.js");
        let tags = parse_javascript(content, path, "test.js").unwrap();
        assert_eq!(tags.len(), 1);
        assert_eq!(tags[0].name.as_ref(), "MyClass");
        assert_eq!(tags[0].node_type.as_ref(), "class");
    }

    #[test]
    fn test_parse_typescript_interface() {
        let content = "export interface IFoo {\n  bar: string;\n}\n";
        let path = Path::new("test.ts");
        let tags = parse_typescript(content, path, "test.ts").unwrap();
        assert!(tags.iter().any(|t| t.name.as_ref() == "IFoo" && t.node_type.as_ref() == "interface"));
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/extraction/signatures.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Function signature extraction stub.

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/extraction/tags.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Tag extraction orchestration.
//!
//! This module ties together tree-sitter and regex parsers to provide the main
//! entry point for extracting tags from source files. It uses tree-sitter when
//! available (for full AST accuracy) and falls back to regex for unsupported
//! languages.
//!
//! # Parser Selection Strategy
//!
//! 1. Check file extension to determine language
//! 2. If tree-sitter supports the language â†’ use AST-based extraction
//! 3. Otherwise â†’ fall back to regex patterns
//!
//! This ensures we always get results while maximizing accuracy for supported
//! languages.

use std::path::Path;
use std::cell::RefCell;

use anyhow::Result;

use crate::types::Tag;
use crate::extraction::Parser;
use crate::extraction::treesitter::{TreeSitterParser, extension_to_language};

thread_local! {
    /// Thread-local tree-sitter parser (tree-sitter parsers are not thread-safe)
    static TS_PARSER: RefCell<TreeSitterParser> = RefCell::new(TreeSitterParser::new());
}

/// Extract symbol tags from a source file.
///
/// This is the main entry point for tag extraction. Given a file path and
/// its relative name (for display), it uses tree-sitter for supported languages
/// and falls back to regex patterns otherwise.
///
/// # Arguments
/// * `path` - Absolute path to the source file to parse
/// * `rel_fname` - Relative path for display in output (e.g., "src/lib.rs")
/// * `parser` - Regex parser instance (fallback for unsupported languages)
///
/// # Returns
/// Vector of extracted tags. Empty vector if file can't be parsed or has no symbols.
///
/// # Parser Selection
/// - Python, Rust, JavaScript, TypeScript, Go, Java, C, C++, Ruby, PHP â†’ tree-sitter
/// - Other languages â†’ regex fallback
pub fn extract_tags(
    path: &Path,
    rel_fname: &str,
    parser: &Parser,
) -> Result<Vec<Tag>> {
    // Determine language from extension
    let ext = path.extension()
        .and_then(|e| e.to_str())
        .unwrap_or("");

    let language = extension_to_language(ext);

    // Try tree-sitter first for supported languages
    if let Some(lang) = language {
        if TreeSitterParser::supports_language(lang) {
            let content = std::fs::read_to_string(path)?;
            let fname = path.to_string_lossy().to_string();

            let tags = TS_PARSER.with(|p| {
                p.borrow_mut().extract_tags(&content, lang, &fname, rel_fname)
            });

            // If tree-sitter found tags, use them
            if !tags.is_empty() {
                return Ok(tags);
            }
            // Otherwise fall through to regex fallback
        }
    }

    // Fallback to regex-based parsing
    parser.parse_file(path, rel_fname)
}

/// Extract tags using only tree-sitter (no fallback).
///
/// Use this when you specifically need AST-based extraction and want to
/// know if tree-sitter supported the language.
pub fn extract_tags_treesitter(
    content: &str,
    language: &str,
    fname: &str,
    rel_fname: &str,
) -> Vec<Tag> {
    TS_PARSER.with(|p| {
        p.borrow_mut().extract_tags(content, language, fname, rel_fname)
    })
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::Path;

    #[test]
    fn test_extract_tags_python() {
        // Create a temporary Python file for testing
        let temp_dir = std::env::temp_dir();
        let test_file = temp_dir.join("test_extract.py");
        std::fs::write(&test_file, "class Foo:\n    def bar(self):\n        pass\n").unwrap();

        let parser = Parser::new();
        let tags = extract_tags(&test_file, "test_extract.py", &parser).unwrap();

        assert!(tags.len() >= 1); // At least the class
        assert!(tags.iter().any(|t| t.name.as_ref() == "Foo"));

        // Cleanup
        std::fs::remove_file(test_file).ok();
    }

    #[test]
    fn test_extract_tags_rust() {
        let temp_dir = std::env::temp_dir();
        let test_file = temp_dir.join("test_extract.rs");
        std::fs::write(&test_file, "pub fn foo() {}\nstruct Bar {}\n").unwrap();

        let parser = Parser::new();
        let tags = extract_tags(&test_file, "test_extract.rs", &parser).unwrap();

        assert!(tags.len() >= 2);
        assert!(tags.iter().any(|t| t.name.as_ref() == "foo"));
        assert!(tags.iter().any(|t| t.name.as_ref() == "Bar"));

        std::fs::remove_file(test_file).ok();
    }

    #[test]
    fn test_extract_tags_nonexistent_file() {
        let parser = Parser::new();
        let result = extract_tags(Path::new("/nonexistent/file.py"), "file.py", &parser);
        assert!(result.is_err());
    }

    #[test]
    fn test_treesitter_python_direct() {
        let code = r#"
class MyClass:
    def method(self):
        pass

def standalone():
    return 42
"#;
        let tags = extract_tags_treesitter(code, "python", "/test.py", "test.py");

        let names: Vec<&str> = tags.iter().map(|t| t.name.as_ref()).collect();
        assert!(names.contains(&"MyClass"), "Should find MyClass, got: {:?}", names);
        assert!(names.contains(&"method"), "Should find method, got: {:?}", names);
        assert!(names.contains(&"standalone"), "Should find standalone, got: {:?}", names);
    }

    #[test]
    fn test_treesitter_rust_direct() {
        let code = r#"
struct MyStruct {
    field: i32,
}

fn standalone() {
    println!("hello");
}
"#;
        let tags = extract_tags_treesitter(code, "rust", "/test.rs", "test.rs");

        let names: Vec<&str> = tags.iter().map(|t| t.name.as_ref()).collect();
        assert!(names.contains(&"MyStruct"), "Should find MyStruct, got: {:?}", names);
        assert!(names.contains(&"standalone"), "Should find standalone, got: {:?}", names);
    }

    #[test]
    fn test_treesitter_javascript_direct() {
        let code = r#"
class MyClass {
    method() { return 1; }
}

function standalone() {
    console.log("hello");
}
"#;
        let tags = extract_tags_treesitter(code, "javascript", "/test.js", "test.js");

        let names: Vec<&str> = tags.iter().map(|t| t.name.as_ref()).collect();
        assert!(names.contains(&"MyClass"), "Should find MyClass, got: {:?}", names);
        assert!(names.contains(&"method"), "Should find method, got: {:?}", names);
        assert!(names.contains(&"standalone"), "Should find standalone, got: {:?}", names);
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/extraction/treesitter.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Tree-sitter based code parsing with .scm query support.
//!
//! This module provides AST-aware tag extraction using tree-sitter grammars
//! and query files. It replaces regex-based parsing with full structural
//! understanding of the code.
//!
//! # Query Format
//!
//! The .scm query files use tree-sitter's query syntax with captures:
//! - `@name.definition.class` - class/struct name
//! - `@definition.class` - entire class node
//! - `@name.definition.function` - function name
//! - `@definition.function` - entire function node
//! - `@name.reference.call` - function call name
//! - `@reference.call` - entire call node

use std::collections::HashMap;
use std::sync::Arc;

use once_cell::sync::Lazy;
use streaming_iterator::StreamingIterator;
use tree_sitter::{Language, Parser as TsParser, Query, QueryCursor};

use crate::types::{Tag, TagKind};

/// Embedded query files - compiled into the binary
mod queries {
    pub const PYTHON: &str = include_str!("../../queries/python-tags.scm");
    pub const RUST: &str = include_str!("../../queries/rust-tags.scm");
    pub const JAVASCRIPT: &str = include_str!("../../queries/javascript-tags.scm");
    pub const TYPESCRIPT: &str = include_str!("../../queries/typescript-tags.scm");
    pub const GO: &str = include_str!("../../queries/go-tags.scm");
    pub const JAVA: &str = include_str!("../../queries/java-tags.scm");
    pub const C: &str = include_str!("../../queries/c-tags.scm");
    pub const CPP: &str = include_str!("../../queries/cpp-tags.scm");
    pub const RUBY: &str = include_str!("../../queries/ruby-tags.scm");
    pub const PHP: &str = include_str!("../../queries/php-tags.scm");
    pub const C_SHARP: &str = include_str!("../../queries/c_sharp-tags.scm");
    pub const KOTLIN: &str = include_str!("../../queries/kotlin-tags.scm");
    pub const SCALA: &str = include_str!("../../queries/scala-tags.scm");
}

/// Language configuration with grammar and query
struct LangConfig {
    language: Language,
    query: Query,
}

/// Get tree-sitter language by name
fn get_language(name: &str) -> Option<Language> {
    match name {
        "python" => Some(tree_sitter_python::LANGUAGE.into()),
        "rust" => Some(tree_sitter_rust::LANGUAGE.into()),
        "javascript" | "jsx" => Some(tree_sitter_javascript::LANGUAGE.into()),
        "typescript" | "tsx" => Some(tree_sitter_typescript::LANGUAGE_TYPESCRIPT.into()),
        "go" => Some(tree_sitter_go::LANGUAGE.into()),
        "java" => Some(tree_sitter_java::LANGUAGE.into()),
        "c" => Some(tree_sitter_c::LANGUAGE.into()),
        "cpp" | "c++" | "cc" | "cxx" => Some(tree_sitter_cpp::LANGUAGE.into()),
        "ruby" => Some(tree_sitter_ruby::LANGUAGE.into()),
        "php" => Some(tree_sitter_php::LANGUAGE_PHP.into()),
        _ => None,
    }
}

/// Get query source for a language
fn get_query_source(name: &str) -> Option<&'static str> {
    match name {
        "python" => Some(queries::PYTHON),
        "rust" => Some(queries::RUST),
        "javascript" | "jsx" => Some(queries::JAVASCRIPT),
        "typescript" | "tsx" => Some(queries::TYPESCRIPT),
        "go" => Some(queries::GO),
        "java" => Some(queries::JAVA),
        "c" => Some(queries::C),
        "cpp" | "c++" | "cc" | "cxx" => Some(queries::CPP),
        "ruby" => Some(queries::RUBY),
        "php" => Some(queries::PHP),
        _ => None,
    }
}

/// Map file extension to language name
pub fn extension_to_language(ext: &str) -> Option<&'static str> {
    match ext {
        "py" | "pyi" | "pyw" => Some("python"),
        "rs" => Some("rust"),
        "js" | "mjs" | "cjs" => Some("javascript"),
        "jsx" => Some("jsx"),
        "ts" | "mts" | "cts" => Some("typescript"),
        "tsx" => Some("tsx"),
        "go" => Some("go"),
        "java" => Some("java"),
        "c" | "h" => Some("c"),
        "cpp" | "cc" | "cxx" | "hpp" | "hxx" | "hh" => Some("cpp"),
        "rb" | "rake" | "gemspec" => Some("ruby"),
        "php" | "php3" | "php4" | "php5" | "phtml" => Some("php"),
        "cs" => Some("c_sharp"),
        "kt" | "kts" => Some("kotlin"),
        "scala" | "sc" => Some("scala"),
        _ => None,
    }
}

/// Cached language configurations
static LANG_CONFIGS: Lazy<HashMap<&'static str, LangConfig>> = Lazy::new(|| {
    let mut configs = HashMap::new();

    for lang_name in &["python", "rust", "javascript", "typescript", "go", "java", "c", "cpp", "ruby", "php"] {
        if let (Some(language), Some(query_src)) = (get_language(lang_name), get_query_source(lang_name)) {
            // Try to compile the query, skip if it fails (query syntax might not match grammar version)
            match Query::new(&language, query_src) {
                Ok(query) => {
                    configs.insert(*lang_name, LangConfig { language, query });
                }
                Err(e) => {
                    eprintln!("Warning: Failed to compile query for {}: {}", lang_name, e);
                }
            }
        }
    }

    configs
});

/// Tree-sitter based parser for extracting tags from source code.
pub struct TreeSitterParser {
    /// Thread-local parser instances (tree-sitter parsers are not thread-safe)
    parser: TsParser,
}

impl TreeSitterParser {
    /// Create a new tree-sitter parser.
    pub fn new() -> Self {
        Self {
            parser: TsParser::new(),
        }
    }

    /// Check if a language is supported.
    pub fn supports_language(lang: &str) -> bool {
        LANG_CONFIGS.contains_key(lang)
    }

    /// Extract tags from source code using tree-sitter queries.
    pub fn extract_tags(
        &mut self,
        content: &str,
        language: &str,
        fname: &str,
        rel_fname: &str,
    ) -> Vec<Tag> {
        let config = match LANG_CONFIGS.get(language) {
            Some(c) => c,
            None => return Vec::new(),
        };

        // Set language and parse
        if self.parser.set_language(&config.language).is_err() {
            return Vec::new();
        }

        let tree = match self.parser.parse(content, None) {
            Some(t) => t,
            None => return Vec::new(),
        };

        let mut tags = Vec::new();
        let mut cursor = QueryCursor::new();

        // Track capture names for processing
        let capture_names: Vec<String> = config.query.capture_names().iter().map(|s| s.to_string()).collect();

        // Use streaming iterator pattern for tree-sitter 0.24+
        let mut matches = cursor.matches(&config.query, tree.root_node(), content.as_bytes());
        while let Some(m) = matches.next() {
            let mut name: Option<String> = None;
            let mut node_type: Option<&str> = None;
            let mut kind: Option<TagKind> = None;
            let mut line: Option<u32> = None;

            for capture in m.captures {
                let capture_name = capture_names.get(capture.index as usize).map(|s| s.as_str()).unwrap_or("");
                let node = capture.node;
                let text = node.utf8_text(content.as_bytes()).unwrap_or("").to_string();

                // Parse capture name: @name.definition.class, @definition.function, etc.
                if capture_name.starts_with("name.") {
                    line = Some(node.start_position().row as u32 + 1);
                    name = Some(text);

                    // Extract kind from capture name
                    if capture_name.contains(".definition.") {
                        kind = Some(TagKind::Def);
                    } else if capture_name.contains(".reference.") {
                        kind = Some(TagKind::Ref);
                    }

                    // Extract node type from capture name
                    if capture_name.ends_with(".class") {
                        node_type = Some("class");
                    } else if capture_name.ends_with(".function") {
                        node_type = Some("function");
                    } else if capture_name.ends_with(".method") {
                        node_type = Some("method");
                    } else if capture_name.ends_with(".call") {
                        node_type = Some("call");
                    } else if capture_name.ends_with(".interface") {
                        node_type = Some("interface");
                    } else if capture_name.ends_with(".module") {
                        node_type = Some("module");
                    } else if capture_name.ends_with(".macro") {
                        node_type = Some("macro");
                    } else if capture_name.ends_with(".implementation") {
                        node_type = Some("impl");
                    }
                }
            }

            // Create tag if we have the required fields
            if let (Some(name), Some(node_type), Some(kind), Some(line)) = (name, node_type, kind, line) {
                // Skip empty names or very short names (likely noise)
                if name.is_empty() || (name.len() == 1 && !name.chars().next().unwrap().is_alphabetic()) {
                    continue;
                }

                tags.push(Tag {
                    rel_fname: Arc::from(rel_fname),
                    fname: Arc::from(fname),
                    line,
                    name: Arc::from(name.as_str()),
                    kind,
                    node_type: Arc::from(node_type),
                    parent_name: None,  // TODO: extract from AST parent traversal
                    parent_line: None,
                    signature: None,    // TODO: extract from AST
                    fields: None,
                metadata: None,
                });
            }
        }

        tags
    }
}

impl Default for TreeSitterParser {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_extension_mapping() {
        assert_eq!(extension_to_language("py"), Some("python"));
        assert_eq!(extension_to_language("rs"), Some("rust"));
        assert_eq!(extension_to_language("js"), Some("javascript"));
        assert_eq!(extension_to_language("ts"), Some("typescript"));
        assert_eq!(extension_to_language("unknown"), None);
    }

    #[test]
    fn test_python_parsing() {
        let mut parser = TreeSitterParser::new();
        let code = r#"
class MyClass:
    def method(self):
        pass

def standalone_function():
    return 42

standalone_function()
"#;
        let tags = parser.extract_tags(code, "python", "/test.py", "test.py");

        // Should find class, method, function, and call
        let names: Vec<&str> = tags.iter().map(|t| t.name.as_ref()).collect();
        assert!(names.contains(&"MyClass"), "Should find MyClass");
        assert!(names.contains(&"method"), "Should find method");
        assert!(names.contains(&"standalone_function"), "Should find standalone_function");
    }

    #[test]
    fn test_rust_parsing() {
        let mut parser = TreeSitterParser::new();
        let code = r#"
struct MyStruct {
    field: i32,
}

impl MyStruct {
    fn new() -> Self {
        Self { field: 0 }
    }
}

fn standalone() {
    println!("hello");
}
"#;
        let tags = parser.extract_tags(code, "rust", "/test.rs", "test.rs");

        let names: Vec<&str> = tags.iter().map(|t| t.name.as_ref()).collect();
        assert!(names.contains(&"MyStruct"), "Should find MyStruct");
        assert!(names.contains(&"new"), "Should find new method");
        assert!(names.contains(&"standalone"), "Should find standalone function");
    }

    #[test]
    fn test_javascript_parsing() {
        let mut parser = TreeSitterParser::new();
        let code = r#"
class MyClass {
    constructor() {}
    method() { return 1; }
}

function standalone() {
    console.log("hello");
}

const arrow = () => 42;
"#;
        let tags = parser.extract_tags(code, "javascript", "/test.js", "test.js");

        let names: Vec<&str> = tags.iter().map(|t| t.name.as_ref()).collect();
        assert!(names.contains(&"MyClass"), "Should find MyClass");
        assert!(names.contains(&"method"), "Should find method");
        assert!(names.contains(&"standalone"), "Should find standalone function");
    }

    #[test]
    fn test_unsupported_language() {
        let mut parser = TreeSitterParser::new();
        let tags = parser.extract_tags("content", "unsupported", "/test.xyz", "test.xyz");
        assert!(tags.is_empty());
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/lib.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! ripmap - Ultra-fast codebase cartography
//!
//! A Rust rewrite of grepmap, targeting 1000x faster performance.
//! Uses tree-sitter for parsing, PageRank for importance ranking,
//! and rich terminal rendering for output.
//!
//! # Architecture
//!
//! ```text
//! File Discovery â†’ Tag Extraction â†’ Graph Building â†’ PageRank â†’ Boosts â†’ Rendering
//!       â†“              â†“                â†“              â†“          â†“          â†“
//!    ignore        tree-sitter      petgraph      iterative   contextual   ANSI
//!    crate          + .scm          DiGraph        power       signals     colors
//! ```
//!
//! # Performance Strategies
//!
//! - Parallel file parsing via rayon
//! - Memory-mapped I/O for large files
//! - Arena allocators for tag batches
//! - Lock-free graph building with dashmap
//! - String interning for symbol names
//! - Persistent cache with redb

pub mod types;
pub mod extraction;
pub mod discovery;
pub mod ranking;
pub mod rendering;
pub mod cache;
pub mod mcp;
pub mod training;
pub mod training_outer;
pub mod callgraph;

// Re-export core types
pub use types::{
    Tag, TagKind, RankedTag, SignatureInfo, FieldInfo,
    DetailLevel, RankingConfig, FilePhase, Intent, SymbolId,
};

// Re-export call graph types
pub use callgraph::{
    CallGraph, CallEdge, FunctionId,
    CallResolver, ResolverBuilder, ResolverConfig, ResolutionStats,
    ResolutionStrategy, ResolutionContext, Candidate,
    SameFileStrategy, NameMatchStrategy, TypeHintStrategy, ImportStrategy,
};

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/main.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! ripmap CLI - Ultra-fast codebase cartography
//!
//! This is the command-line entry point for ripmap, the 1000x faster Rust
//! rewrite of grepmap. It orchestrates the full pipeline:
//!
//! 1. File Discovery: Find source files respecting .gitignore
//! 2. Tag Extraction: Parse files with tree-sitter, extract symbols (cached)
//! 3. Graph Building: Build symbol reference graph
//! 4. PageRank: Compute importance scores via iterative power method
//! 5. Contextual Boosts: Apply intent-aware, focus-aware, git-aware multipliers
//! 6. Rendering: Output rich terminal visualization within token budget
//!
//! Design philosophy:
//! - Start simple, expand incrementally
//! - Fail fast with clear error messages
//! - Respect token budgets (LLM context is precious)
//! - Make defaults sane (--color=true, --directory=true)
//! - Verbose mode for debugging the debugger

use clap::Parser;
use std::path::PathBuf;
use anyhow::Result;

/// Ultra-fast codebase cartography - 1000x faster grepmap
///
/// ripmap discovers the load-bearing structure of your codebase using
/// tree-sitter parsing, PageRank importance ranking, and git-aware boosts.
/// It reveals what matters, not by size but by structural significance.
///
/// Examples:
///   ripmap .                           # Map entire codebase
///   ripmap src/lib.rs                  # Focus on specific file
///   ripmap --focus "auth parser"       # Semantic focus query
///   ripmap --tokens 4096 --git-weight  # Customize output & ranking
#[derive(Parser, Debug)]
#[command(name = "ripmap")]
#[command(version)]
#[command(about, long_about = None)]
pub struct Cli {
    /// Files or directories to focus on
    ///
    /// Can be specific file paths (src/lib.rs) or symbol queries.
    /// If empty, scans the entire project root.
    #[arg(value_name = "FILES")]
    pub files: Vec<String>,

    /// Focus query for symbol matching
    ///
    /// Semantic search across symbol names. Examples:
    ///   --focus "auth"           Match authentication-related symbols
    ///   --focus "parse render"   Match parsing OR rendering symbols
    ///
    /// Focus symbols receive boost multipliers in ranking.
    #[arg(short, long)]
    pub focus: Option<String>,

    /// Additional files to include in analysis
    ///
    /// Use this to add extra context files beyond the main focus.
    /// Useful for including related modules or config files.
    #[arg(long, value_name = "FILES")]
    pub other: Vec<PathBuf>,

    /// Maximum output tokens (LLM context budget)
    ///
    /// Controls how much content to render. Higher = more context but
    /// uses more of your LLM context window. Typical values:
    ///   2048  - Quick overview
    ///   8192  - Standard exploration (default)
    ///   32768 - Deep dive with full signatures
    #[arg(short = 't', long, default_value = "8192")]
    pub tokens: usize,

    /// Force cache refresh
    ///
    /// Ignores cached tag extractions and re-parses all files.
    /// Use this if you suspect cache corruption or after major
    /// refactoring that changes many files.
    #[arg(long)]
    pub refresh: bool,

    /// Enable colored output
    ///
    /// Uses ANSI colors for badges, ranks, and syntax highlighting.
    /// Disable with --no-color for piping to files or LLMs that
    /// don't handle ANSI well.
    #[arg(long, default_value = "true")]
    pub color: bool,

    /// Use directory mode (vs tree mode)
    ///
    /// Directory mode groups symbols by file in a flat list.
    /// Tree mode shows hierarchical file structure.
    /// Directory mode is usually clearer for LLM consumption.
    #[arg(long, default_value = "true")]
    pub directory: bool,

    /// Show statistics
    ///
    /// Prints performance stats at the end:
    ///   - Files scanned
    ///   - Tags extracted
    ///   - Graph size (nodes/edges)
    ///   - PageRank iterations
    ///   - Time breakdown
    #[arg(long)]
    pub stats: bool,

    /// Enable git-based weighting
    ///
    /// Boosts recently changed and high-churn files.
    /// Requires git repository. Adds ~10-50ms overhead.
    /// Recommended for debugging and refactoring tasks.
    #[arg(long)]
    pub git_weight: bool,

    /// Enable diagnostic output
    ///
    /// Shows internal state for debugging ripmap itself:
    ///   - File discovery details
    ///   - Cache hit/miss rates
    ///   - Graph construction logs
    ///   - Ranking computation traces
    ///
    /// Very verbose - use for ripmap development.
    #[arg(long)]
    pub diagnose: bool,

    /// Show call graph relationships
    ///
    /// Displays what each function calls and what calls it.
    /// Uses type hints and imports for resolution when available.
    /// Adds visual indicators like:
    ///   â†’ calls: foo(), bar()
    ///   â† called by: main(), handler()
    #[arg(long)]
    pub calls: bool,

    /// Project root directory
    ///
    /// Base path for file discovery and git operations.
    /// Defaults to current directory.
    #[arg(short, long, default_value = ".")]
    pub root: PathBuf,

    /// Verbose output
    ///
    /// Shows progress messages during execution:
    ///   "Scanning: /path/to/project"
    ///   "Found 1234 files"
    ///   "Extracting tags..."
    ///   "Computing PageRank..."
    ///
    /// Helpful for understanding performance on large codebases.
    #[arg(short, long)]
    pub verbose: bool,

    /// Output full file contents joined with clear separators
    ///
    /// Instead of generating a ranked symbol map, concatenates the full
    /// content of all specified files with clear separators between them.
    /// Useful for creating a codebase artifact when the project is small
    /// enough that full file content fits in context.
    ///
    /// Works with extension filters (-e/--ext) to select file types.
    /// Warns if output exceeds 200KB.
    #[arg(long)]
    pub join: bool,

    /// Filter files by extension (can be repeated)
    ///
    /// Only include files with these extensions. Examples:
    ///   -e rs          # Rust files only
    ///   -e py -e pyi   # Python files and stubs
    ///   --ext ts --ext tsx  # TypeScript
    #[arg(short = 'e', long = "ext", value_name = "EXT")]
    pub extensions: Vec<String>,

    /// Disable colored output
    ///
    /// Equivalent to --color=false. Useful for piping to files or
    /// LLMs that don't handle ANSI escape codes well.
    #[arg(long)]
    pub no_color: bool,
}

/// Size threshold (200KB) above which we warn the user about large output in join mode
const JOIN_SIZE_WARNING_THRESHOLD: usize = 200 * 1024;

fn main() -> Result<()> {
    let cli = Cli::parse();

    // Join mode: concatenate full file contents instead of symbol map
    if cli.join {
        run_join_mode(&cli)?;
        return Ok(());
    }

    // Run the cartography pipeline
    let output = run(&cli)?;

    // Print to stdout (can be piped or redirected)
    println!("{}", output);

    Ok(())
}

/// Execute join mode - output full file contents with clear separators
///
/// This is a fundamentally different output mode from the symbol map. Instead of
/// extracting signatures and ranking, we just concatenate full files. Useful for
/// small-to-medium projects where the full codebase artifact fits in context.
///
/// Features:
/// - Extension filtering via -e/--ext flags
/// - Size warning when output exceeds 200KB
/// - Optional colored output with ANSI codes
/// - Sorted file order for deterministic output
fn run_join_mode(cli: &Cli) -> Result<()> {
    use ripmap::discovery::find_source_files;

    // Determine effective color setting (--no-color overrides --color)
    let use_color = cli.color && !cli.no_color;

    // Canonicalize root path
    let root = cli.root.canonicalize()
        .map_err(|e| anyhow::anyhow!("Failed to resolve root path '{}': {}", cli.root.display(), e))?;

    if cli.verbose {
        eprintln!("ğŸ”— ripmap join mode");
        eprintln!("ğŸ“‚ Scanning: {}", root.display());
    }

    // Discover files
    let all_files = find_source_files(&root, false)?;

    // Filter by extension if specified
    let files: Vec<_> = if cli.extensions.is_empty() {
        all_files
    } else {
        let exts: std::collections::HashSet<_> = cli.extensions.iter()
            .map(|e| e.strip_prefix('.').unwrap_or(e).to_lowercase())
            .collect();
        all_files.into_iter()
            .filter(|f| {
                f.extension()
                    .map(|e| exts.contains(&e.to_string_lossy().to_lowercase()))
                    .unwrap_or(false)
            })
            .collect()
    };

    if files.is_empty() {
        return Err(anyhow::anyhow!("No files to join. Provide paths or directories, or check --ext filters."));
    }

    // Collect all content and compute total size
    let mut segments: Vec<(String, String)> = Vec::new();
    let mut total_size: usize = 0;

    for fpath in files.iter() {
        let content = match std::fs::read_to_string(fpath) {
            Ok(c) => c,
            Err(_) => continue, // Skip unreadable files (binary, etc.)
        };

        // Compute relative path for display
        let rel_path = fpath.strip_prefix(&root)
            .unwrap_or(fpath)
            .to_string_lossy()
            .to_string();

        total_size += content.len();
        segments.push((rel_path, content));
    }

    // Sort by path for deterministic output
    segments.sort_by(|a, b| a.0.cmp(&b.0));

    // Warn if output is large
    if total_size > JOIN_SIZE_WARNING_THRESHOLD {
        let size_kb = total_size / 1024;
        eprintln!("âš ï¸  Warning: Output is {}KB ({} files). Consider using the default map mode for large codebases.",
            size_kb, segments.len());
    }

    if cli.verbose {
        eprintln!("âœ“ Joining {} files ({:.1}KB)", segments.len(), total_size as f64 / 1024.0);
    }

    // Output with separators
    let separator = "â”€".repeat(80);

    for (rel_path, content) in &segments {
        // Header with file path
        if use_color {
            // Blue separator and inverted white-on-blue path
            println!("\n\x1b[1;34m{}\x1b[0m", separator);
            println!("\x1b[1;37;44m {} \x1b[0m", rel_path);
            println!("\x1b[1;34m{}\x1b[0m\n", separator);
        } else {
            println!("\n{}", separator);
            println!(" {} ", rel_path);
            println!("{}\n", separator);
        }

        // File content (no syntax highlighting for now - keeps deps minimal)
        print!("{}", content);

        // Ensure content ends with newline
        if !content.ends_with('\n') {
            println!();
        }
    }

    // Final separator
    if use_color {
        println!("\n\x1b[1;34m{}\x1b[0m", separator);
    } else {
        println!("\n{}", separator);
    }

    Ok(())
}

/// Execute the full ripmap pipeline
///
/// This orchestrates all stages of the codebase cartography:
/// 1. File Discovery - find source files respecting .gitignore
/// 2. Tag Extraction - parse with regex (tree-sitter later), cached
/// 3. Graph Building - symbol references as edges
/// 4. PageRank - importance scores via power iteration
/// 5. Contextual Boosts - focus, git, intent multipliers
/// 6. Rendering - rich terminal output within token budget
fn run(cli: &Cli) -> Result<String> {
    use std::collections::{HashMap, HashSet};
    use std::sync::Arc;
    use std::time::Instant;
    use ripmap::discovery::find_source_files;
    use ripmap::extraction::{Parser, extract_tags};
    use ripmap::ranking::{PageRanker, BoostCalculator};
    use ripmap::rendering::DirectoryRenderer;
    use ripmap::types::{RankingConfig, DetailLevel};
    use ripmap::callgraph::ResolverBuilder;

    let start = Instant::now();

    // Canonicalize root path for consistent cache keys
    let root = cli.root.canonicalize()
        .map_err(|e| anyhow::anyhow!("Failed to resolve root path '{}': {}", cli.root.display(), e))?;

    if cli.verbose {
        eprintln!("ğŸ—ºï¸  ripmap v{}", env!("CARGO_PKG_VERSION"));
        eprintln!("ğŸ“‚ Scanning: {}", root.display());
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Stage 1: File Discovery
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    let files = find_source_files(&root, false)?;

    if files.is_empty() {
        return Ok("No source files found. Check your path and .gitignore settings.".into());
    }

    if cli.verbose {
        eprintln!("âœ“ Found {} files ({:.2?})", files.len(), start.elapsed());
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Stage 2: Tag Extraction (regex-based, tree-sitter later)
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    let extract_start = Instant::now();
    let parser = Parser::new();
    let mut tags_by_file: HashMap<String, Vec<ripmap::Tag>> = HashMap::new();
    let mut total_tags = 0;

    for file in &files {
        let rel_fname = file.strip_prefix(&root)
            .unwrap_or(file)
            .to_string_lossy()
            .to_string();

        match extract_tags(file, &rel_fname, &parser) {
            Ok(tags) => {
                total_tags += tags.len();
                if !tags.is_empty() {
                    tags_by_file.insert(rel_fname, tags);
                }
            }
            Err(_) => continue, // Skip files that fail to parse
        }
    }

    if cli.verbose {
        eprintln!("âœ“ Extracted {} tags from {} files ({:.2?})",
            total_tags, tags_by_file.len(), extract_start.elapsed());
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Stage 3 & 4: PageRank on Symbol Graph
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    let rank_start = Instant::now();

    // RankingConfig defaults are the baseline. Future: meta-lever inference
    // will derive these from context signals (query, git state, repo shape).
    let config = RankingConfig::default();

    let page_ranker = PageRanker::new(config.clone());

    // Determine chat files (focus files get boost)
    let chat_fnames: Vec<String> = cli.files.iter()
        .filter_map(|f| {
            let path = std::path::Path::new(f);
            if path.exists() {
                path.strip_prefix(&root).ok()
                    .or(Some(path))
                    .map(|p| p.to_string_lossy().to_string())
            } else {
                None
            }
        })
        .collect();

    let file_ranks = page_ranker.compute_ranks(&tags_by_file, &chat_fnames);

    if cli.verbose {
        eprintln!("âœ“ Computed PageRank for {} files ({:.2?})",
            file_ranks.len(), rank_start.elapsed());
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Stage 4.5: Build Call Graph
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Call graph is built unconditionally - it powers focus expansion and
    // caller weight boosts, not just rendering. The --calls flag only controls
    // whether call relationships are displayed in output.
    let cg_start = Instant::now();

    // Flatten all tags for call graph construction
    let all_tags: Vec<ripmap::Tag> = tags_by_file.values()
        .flat_map(|tags| tags.iter().cloned())
        .collect();

    // Build resolver with all strategies enabled
    let resolver = ResolverBuilder::new()
        .same_file(true)
        .type_hints(true)
        .imports(true)
        .name_match(true)
        .build();

    let call_graph = resolver.build_graph(&all_tags);

    if cli.verbose {
        let stats = resolver.stats(&all_tags);
        let resolved = stats.total_calls - stats.unresolved;
        eprintln!("âœ“ Built call graph: {} functions, {} calls ({:.2?})",
            call_graph.function_count(), call_graph.call_count(), cg_start.elapsed());
        eprintln!("  Resolution: {:.1}% success ({} resolved, {} unresolved)",
            stats.resolution_rate() * 100.0, resolved, stats.unresolved);
    }

    // Compute function-level PageRank on the call graph
    // This gives more precise importance scores than file-level PageRank
    let function_ranks = page_ranker.compute_function_ranks(&call_graph);

    // Convert FunctionId -> (file, name) tuple for symbol_ranks
    let symbol_ranks: HashMap<(Arc<str>, Arc<str>), f64> = function_ranks
        .into_iter()
        .map(|(func_id, rank)| ((func_id.file, func_id.name), rank))
        .collect();

    if cli.verbose && !symbol_ranks.is_empty() {
        eprintln!("âœ“ Computed function-level PageRank for {} symbols", symbol_ranks.len());
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Stage 4.6: Testâ†”Source Coupling Detection
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Codex optimization identified "path-aware testâ†”crate coupling edges" as
    // a missing architectural feature. Detect test files and link them to their
    // corresponding source files, adding synthetic edges for graph expansion.
    use ripmap::ranking::{FocusResolver, TestCouplingDetector};

    let coupling_detector = TestCouplingDetector::new()
        .with_min_confidence(config.test_coupling_min_confidence);

    // Detect testâ†”source couplings from file list
    let file_paths: Vec<_> = files.iter().map(|f| {
        f.strip_prefix(&root).unwrap_or(f).to_path_buf()
    }).collect();

    let test_couplings = coupling_detector.detect_couplings(&file_paths);
    let test_coupling_edges = coupling_detector.as_symbol_edges(&test_couplings);

    if cli.verbose && !test_couplings.is_empty() {
        eprintln!("âœ“ Detected {} testâ†”source couplings", test_couplings.len());
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Stage 4.7: Focus Expansion via Call Graph
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // When user provides --focus, we BFS through call relationships to find
    // related functions. This surfaces the call chain around focused symbols.

    let focus_expansion_weights = if cli.focus.is_some() {
        let focus_start = Instant::now();
        let focus_resolver = FocusResolver::new(&root);

        // Parse focus targets from query
        let focus_targets: Vec<String> = cli.focus.as_ref()
            .map(|f| f.split_whitespace().map(String::from).collect())
            .unwrap_or_default();

        // Resolve focus targets to matched symbols
        let (_matched_files, matched_idents) = focus_resolver.resolve(&focus_targets, &tags_by_file);

        // Convert matched idents to the format expand_via_graph expects
        let matched_set: HashSet<String> = matched_idents;

        // Get call graph edges for BFS expansion
        // Combine call graph edges with testâ†”source coupling edges
        let mut symbol_edges = call_graph.as_symbol_edges();
        symbol_edges.extend(test_coupling_edges.clone());

        // Expand through call relationships: callers and callees of focused functions
        // Uses configurable max_hops and decay from RankingConfig
        let expanded = focus_resolver.expand_via_graph(
            &matched_set,
            &symbol_edges,
            config.focus_expansion_max_hops,
            config.focus_expansion_decay,
        );

        if cli.verbose && !expanded.is_empty() {
            eprintln!("âœ“ Focus expansion: {} symbols via call graph ({:.2?})",
                expanded.len(), focus_start.elapsed());
        }

        Some(expanded)
    } else {
        None
    };

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Stage 5: Apply Contextual Boosts
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    let boost_start = Instant::now();
    let boost_calculator = BoostCalculator::new(config);

    let chat_fnames_set: HashSet<String> = chat_fnames.into_iter().collect();
    let mentioned_fnames: HashSet<String> = cli.files.iter().cloned().collect();

    // Extract identifiers from focus query
    let mentioned_idents: HashSet<String> = cli.focus
        .as_ref()
        .map(|f| f.split_whitespace().map(String::from).collect())
        .unwrap_or_default();

    // Get caller weights from call graph - functions called by many others
    // are likely important API surfaces.
    // Aggregate symbol-level caller counts to file-level weights.
    let symbol_caller_weights = call_graph.caller_weights();
    let mut caller_weights: HashMap<String, f64> = HashMap::new();
    for ((file, _symbol), count) in &symbol_caller_weights {
        *caller_weights.entry(file.to_string()).or_insert(0.0) += *count as f64;
    }
    // Normalize by applying log scale (many callers = high importance, but diminishing returns)
    for weight in caller_weights.values_mut() {
        *weight = 1.0 + (*weight).ln().max(0.0);
    }

    let ranked_tags = boost_calculator.apply_boosts(
        &tags_by_file,
        &file_ranks,
        if symbol_ranks.is_empty() { None } else { Some(&symbol_ranks) },
        &chat_fnames_set,
        &mentioned_fnames,
        &mentioned_idents,
        &HashSet::new(), // temporal_boost_files - TODO with git
        None, // git_weights - TODO
        if caller_weights.is_empty() { None } else { Some(&caller_weights) },
        focus_expansion_weights.as_ref(),
    );

    if cli.verbose {
        eprintln!("âœ“ Applied boosts to {} tags ({:.2?})",
            ranked_tags.len(), boost_start.elapsed());
    }

    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    // Stage 6: Rendering
    // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    let render_start = Instant::now();

    // Simple token counter (4 chars â‰ˆ 1 token)
    let token_counter = |s: &str| s.len() / 4;
    let renderer = DirectoryRenderer::new(Box::new(token_counter));

    // Choose detail level based on token budget
    let detail = if cli.tokens >= 16384 {
        DetailLevel::High
    } else if cli.tokens >= 4096 {
        DetailLevel::Medium
    } else {
        DetailLevel::Low
    };

    // Render output (with call graph visualization if --calls enabled)
    let output = renderer.render_with_calls(
        &ranked_tags,
        detail,
        &HashMap::new(), // badges - TODO with git
        &HashMap::new(), // temporal_mates - TODO
        if cli.calls { Some(&call_graph) } else { None },
    );

    if cli.verbose {
        eprintln!("âœ“ Rendered output ({:.2?})", render_start.elapsed());
        eprintln!("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”");
        eprintln!("Total time: {:.2?}", start.elapsed());
    }

    // Prepend header
    let header = format!(
        "# Ranking: {} | {} symbols | ~{} tokens\n",
        if ranked_tags.len() > 100 { "high (dense)" } else { "low (sparse)" },
        ranked_tags.len(),
        token_counter(&output)
    );

    if cli.stats {
        let stats = format!(
            "\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n\
             ## Statistics\n\
             Files discovered: {}\n\
             Files with tags: {}\n\
             Tags extracted: {}\n\
             Ranked symbols: {}\n\
             Total time: {:.2?}\n",
            files.len(),
            tags_by_file.len(),
            total_tags,
            ranked_tags.len(),
            start.elapsed()
        );
        Ok(format!("{}{}{}", header, output, stats))
    } else {
        Ok(format!("{}{}", header, output))
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_cli_parse_minimal() {
        // Test minimal invocation
        let cli = Cli::parse_from(&["ripmap"]);
        assert_eq!(cli.files.len(), 0);
        assert_eq!(cli.tokens, 8192);
        assert!(cli.directory);
        assert!(cli.color);
    }

    #[test]
    fn test_cli_parse_with_files() {
        let cli = Cli::parse_from(&["ripmap", "src/lib.rs", "src/main.rs"]);
        assert_eq!(cli.files, vec!["src/lib.rs", "src/main.rs"]);
    }

    #[test]
    fn test_cli_parse_focus() {
        let cli = Cli::parse_from(&["ripmap", "--focus", "auth parser"]);
        assert_eq!(cli.focus, Some("auth parser".into()));
    }

    #[test]
    fn test_cli_parse_tokens() {
        let cli = Cli::parse_from(&["ripmap", "--tokens", "4096"]);
        assert_eq!(cli.tokens, 4096);
    }

    #[test]
    fn test_cli_parse_flags() {
        let cli = Cli::parse_from(&[
            "ripmap",
            "--refresh",
            "--stats",
            "--git-weight",
            "--verbose",
            "--diagnose",
        ]);
        assert!(cli.refresh);
        assert!(cli.stats);
        assert!(cli.git_weight);
        assert!(cli.verbose);
        assert!(cli.diagnose);
    }

    #[test]
    fn test_cli_parse_root() {
        let cli = Cli::parse_from(&["ripmap", "--root", "/tmp/test"]);
        assert_eq!(cli.root, PathBuf::from("/tmp/test"));
    }

    #[test]
    fn test_run_on_ripmap_itself() -> Result<()> {
        // Test running on the ripmap codebase itself
        let cli = Cli {
            files: vec![],
            focus: None,
            other: vec![],
            tokens: 8192,
            refresh: false,
            color: true,
            directory: true,
            stats: false,
            git_weight: false,
            diagnose: false,
            calls: false,
            root: PathBuf::from("."),
            verbose: false,
            join: false,
            extensions: vec![],
            no_color: false,
        };

        let output = run(&cli)?;

        // Output should contain ranking header and symbols
        assert!(output.contains("# Ranking:"), "Missing ranking header");
        assert!(output.contains("symbols"), "Missing symbols count");
        assert!(output.contains("tokens"), "Missing tokens estimate");

        Ok(())
    }

    #[test]
    fn test_cli_parse_join_mode() {
        let cli = Cli::parse_from(&["ripmap", "--join", "-e", "rs"]);
        assert!(cli.join);
        assert_eq!(cli.extensions, vec!["rs"]);
    }

    #[test]
    fn test_cli_parse_multiple_extensions() {
        let cli = Cli::parse_from(&["ripmap", "-e", "py", "-e", "pyi", "--ext", "rs"]);
        assert_eq!(cli.extensions, vec!["py", "pyi", "rs"]);
    }

    #[test]
    fn test_cli_parse_no_color() {
        let cli = Cli::parse_from(&["ripmap", "--no-color"]);
        assert!(cli.no_color);
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/mcp/mod.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! MCP (Model Context Protocol) server for ripmap.
//!
//! Exposes ripmap's codebase cartography as an MCP tool that can be
//! invoked by AI assistants like Claude. The server runs over stdio
//! and provides:
//!
//! - `grep_map`: Generate topology-aware structural maps using PageRank
//!
//! # Architecture
//!
//! The MCP server wraps the core ripmap pipeline:
//! ```text
//! MCP Request â†’ ripmap pipeline â†’ MCP Response
//!     â†“              â†“                â†“
//! JSON-RPC      discover/extract   JSON-RPC
//! over stdio    rank/render        over stdio
//! ```
//!
//! # Usage
//!
//! Run the MCP server:
//! ```bash
//! ripmap-mcp
//! ```
//!
//! Or as a subcommand:
//! ```bash
//! ripmap mcp
//! ```

mod server;

pub use server::RipmapServer;

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/mcp/server.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! MCP server implementation for ripmap.
//!
//! Provides the `grep_map` tool via MCP protocol over stdio.
//! This enables AI assistants to invoke ripmap for codebase cartography.

use std::borrow::Cow;
use std::collections::{HashMap, HashSet};
use std::path::PathBuf;

use rmcp::{
    handler::server::{router::tool::ToolRouter, tool::Parameters},
    model::{ErrorData as McpError, ErrorCode, *},
    tool, tool_handler, tool_router, ServerHandler,
};
use serde::{Deserialize, Serialize};

use crate::discovery::find_source_files;
use crate::extraction::{Parser, extract_tags};
use crate::ranking::{PageRanker, BoostCalculator};
use crate::rendering::DirectoryRenderer;
use crate::types::{RankingConfig, DetailLevel};

/// Ripmap MCP server - exposes codebase cartography as an MCP tool.
///
/// This server wraps the full ripmap pipeline and exposes it via the
/// Model Context Protocol for use by AI assistants.
#[derive(Debug, Clone)]
pub struct RipmapServer {
    tool_router: ToolRouter<RipmapServer>,
}

/// Request parameters for the grep_map tool.
#[derive(Debug, Deserialize, schemars::JsonSchema)]
#[allow(dead_code)]  // Future: implement other_files, exclude_unranked, force_refresh
pub struct GrepMapRequest {
    /// Absolute path to the project root directory.
    #[schemars(description = "Absolute path to the project root directory")]
    pub project_root: String,

    /// Files you're actively working on (relative paths). Get highest ranking boost.
    #[schemars(description = "Files you're actively working on (relative paths). Get highest ranking boost.")]
    pub chat_files: Option<Vec<String>>,

    /// Additional files to consider. If omitted, scans entire project.
    #[schemars(description = "Additional files to consider. If omitted, scans entire project.")]
    pub other_files: Option<Vec<String>>,

    /// Token budget for the map output (default: 8192). Increase for more detail.
    #[schemars(description = "Token budget for the map output (default: 8192)")]
    pub token_limit: Option<usize>,

    /// Hide files with PageRank of 0 (peripheral files).
    #[schemars(description = "Hide files with PageRank of 0 (peripheral files)")]
    pub exclude_unranked: Option<bool>,

    /// Bypass cache and reparse everything.
    #[schemars(description = "Bypass cache and reparse everything")]
    pub force_refresh: Option<bool>,

    /// Files mentioned in conversation (mid-level ranking boost).
    #[schemars(description = "Files mentioned in conversation (mid-level ranking boost)")]
    pub mentioned_files: Option<Vec<String>>,

    /// Identifiers to boost (function/class names you're looking for).
    #[schemars(description = "Identifiers to boost (function/class names)")]
    pub mentioned_idents: Option<Vec<String>>,
}

/// Response from the grep_map tool.
#[derive(Debug, Serialize, schemars::JsonSchema)]
pub struct GrepMapResponse {
    /// The generated codebase map.
    pub map: String,
    /// Report with statistics about the mapping.
    pub report: GrepMapReport,
}

/// Statistics report from grep_map.
#[derive(Debug, Serialize, schemars::JsonSchema)]
pub struct GrepMapReport {
    /// Number of files excluded from output.
    pub excluded: usize,
    /// Number of definition matches found.
    pub definition_matches: usize,
    /// Number of reference matches found.
    pub reference_matches: usize,
    /// Total files considered in analysis.
    pub total_files_considered: usize,
}

#[tool_router]
impl RipmapServer {
    /// Create a new ripmap MCP server.
    pub fn new() -> Self {
        Self {
            tool_router: Self::tool_router(),
        }
    }

    /// Generate a topology-aware structural map using PageRank over the dependency graph.
    ///
    /// **What this provides:**
    /// NOT alphabetical file lists. YES graph-theoretic importance analysis.
    /// - Parses all code with tree-sitter (functions, classes, imports, references)
    /// - Builds dependency graph: files as nodes, symbol references as edges
    /// - Runs PageRank with depth-aware personalization (root=1.0x, vendor=0.01x)
    /// - Binary-searches token budget to maximize information density
    ///
    /// **Topology preservation:**
    /// Output maintains directory hierarchy and class structure:
    /// - Directory nesting shows architectural layers
    /// - Classes display fields/properties/methods grouped hierarchically
    /// - Multi-line signatures collapsed to one line with full type info
    ///
    /// **Causality model:**
    /// High-ranked files are *dependencies* of many others (causal anchors).
    /// If session.py has high PageRank, it's because many files import from it.
    /// This is transitive importance, not file size or alphabetical proximity.
    #[tool(
        name = "grep_map",
        description = "Generate a topology-aware structural map using PageRank over the dependency graph. Surfaces the load-bearing structure of a codebase by parsing all code, building a dependency graph, and ranking files by structural importance."
    )]
    async fn grep_map(
        &self,
        Parameters(request): Parameters<GrepMapRequest>,
    ) -> Result<CallToolResult, McpError> {
        // Validate project root
        let root = PathBuf::from(&request.project_root);
        if !root.is_dir() {
            return Err(McpError {
                code: ErrorCode(-32602),
                message: Cow::from(format!(
                    "Project root directory not found: {}",
                    request.project_root
                )),
                data: None,
            });
        }

        // Canonicalize root for consistent paths
        let root = root.canonicalize().map_err(|e| McpError {
            code: ErrorCode(-32603),
            message: Cow::from(format!("Failed to resolve root path: {}", e)),
            data: None,
        })?;

        let token_limit = request.token_limit.unwrap_or(8192);
        let chat_files = request.chat_files.unwrap_or_default();
        let mentioned_files: HashSet<String> = request
            .mentioned_files
            .unwrap_or_default()
            .into_iter()
            .collect();
        let mentioned_idents: HashSet<String> = request
            .mentioned_idents
            .unwrap_or_default()
            .into_iter()
            .collect();

        // Stage 1: File Discovery
        let files = find_source_files(&root, false).map_err(|e| McpError {
            code: ErrorCode(-32603),
            message: Cow::from(format!("File discovery failed: {}", e)),
            data: None,
        })?;

        if files.is_empty() {
            return Ok(CallToolResult::success(vec![Content::text(
                "No source files found. Check your path and .gitignore settings.",
            )]));
        }

        // Stage 2: Tag Extraction
        let parser = Parser::new();
        let mut tags_by_file: HashMap<String, Vec<crate::Tag>> = HashMap::new();
        let mut _total_tags = 0;  // Future: include in detailed stats
        let mut definition_matches = 0;
        let mut reference_matches = 0;

        for file in &files {
            let rel_fname = file
                .strip_prefix(&root)
                .unwrap_or(file)
                .to_string_lossy()
                .to_string();

            match extract_tags(file, &rel_fname, &parser) {
                Ok(tags) => {
                    for tag in &tags {
                        if tag.is_def() {
                            definition_matches += 1;
                        } else {
                            reference_matches += 1;
                        }
                    }
                    _total_tags += tags.len();
                    if !tags.is_empty() {
                        tags_by_file.insert(rel_fname, tags);
                    }
                }
                Err(_) => continue,
            }
        }

        // Stage 3 & 4: PageRank
        let config = RankingConfig::default();
        let page_ranker = PageRanker::new(config.clone());

        // Resolve chat files to relative paths
        let chat_fnames: Vec<String> = chat_files
            .iter()
            .filter_map(|f| {
                let path = root.join(f);
                if path.exists() {
                    Some(f.clone())
                } else {
                    None
                }
            })
            .collect();

        let file_ranks = page_ranker.compute_ranks(&tags_by_file, &chat_fnames);

        // Stage 5: Apply Boosts
        let boost_calculator = BoostCalculator::new(config);
        let chat_fnames_set: HashSet<String> = chat_fnames.into_iter().collect();

        let ranked_tags = boost_calculator.apply_boosts(
            &tags_by_file,
            &file_ranks,
            None,
            &chat_fnames_set,
            &mentioned_files,
            &mentioned_idents,
            &HashSet::new(),
            None,
            None,
            None,
        );

        // Stage 6: Rendering
        let token_counter = |s: &str| s.len() / 4;
        let renderer = DirectoryRenderer::new(Box::new(token_counter));

        let detail = if token_limit >= 16384 {
            DetailLevel::High
        } else if token_limit >= 4096 {
            DetailLevel::Medium
        } else {
            DetailLevel::Low
        };

        let output = renderer.render(
            &ranked_tags,
            detail,
            &HashMap::new(),
            &HashMap::new(),
        );

        // Build response
        let header = format!(
            "# Codebase Map: {} | {} symbols | ~{} tokens\n\n",
            if ranked_tags.len() > 100 {
                "dense"
            } else {
                "sparse"
            },
            ranked_tags.len(),
            token_counter(&output)
        );

        let map_content = format!("{}{}", header, output);

        let report = GrepMapReport {
            excluded: files.len() - tags_by_file.len(),
            definition_matches,
            reference_matches,
            total_files_considered: files.len(),
        };

        // Return as JSON for structured response
        let response = GrepMapResponse {
            map: map_content,
            report,
        };

        let json = serde_json::to_string_pretty(&response).map_err(|e| McpError {
            code: ErrorCode(-32603),
            message: Cow::from(format!("JSON serialization failed: {}", e)),
            data: None,
        })?;

        Ok(CallToolResult::success(vec![Content::text(json)]))
    }
}

impl Default for RipmapServer {
    fn default() -> Self {
        Self::new()
    }
}

#[tool_handler]
impl ServerHandler for RipmapServer {
    fn get_info(&self) -> ServerInfo {
        ServerInfo {
            protocol_version: ProtocolVersion::V_2024_11_05,
            capabilities: ServerCapabilities::builder().enable_tools().build(),
            server_info: Implementation {
                name: "ripmap".into(),
                version: env!("CARGO_PKG_VERSION").into(),
            },
            instructions: Some(
                "Ultra-fast codebase cartography using PageRank. \
                 Use grep_map to generate topology-aware structural maps \
                 that surface load-bearing code structure."
                    .to_string(),
            ),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_server_creation() {
        let server = RipmapServer::new();
        let info = server.get_info();
        assert_eq!(info.server_info.name, "ripmap");
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/ranking/boosts.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Contextual boost calculation for ripmap.
//!
//! This module applies multiplicative boosts to base PageRank scores based on
//! various contextual signals:
//!
//! - **Mentioned identifiers**: Symbols explicitly referenced in queries/chat (10x)
//! - **Mentioned files**: Files explicitly named in context (5x)
//! - **Chat files**: Files being actively edited (20x)
//! - **Temporal coupling**: Files that co-change with chat files (3x)
//! - **Focus expansion**: Graph neighbors of focus symbols (5x)
//!
//! The final rank combines all signals multiplicatively:
//! ```text
//! final_rank = base_rank Ã— boost Ã— git_weight Ã— caller_weight
//! ```
//!
//! This creates a powerful ranking signal that elevates contextually relevant
//! symbols to the top while still respecting structural importance (PageRank).

use std::collections::{HashMap, HashSet};
use std::sync::Arc;

use crate::types::{RankedTag, RankingConfig, Tag, TagKind};

/// Calculator for applying contextual boosts to symbol ranks.
///
/// Takes base PageRank scores and amplifies them based on contextual signals
/// like chat files, mentions, temporal coupling, and graph expansion.
///
/// The boost system is multiplicative: multiple signals stack to create
/// very high boosts for highly relevant symbols.
pub struct BoostCalculator {
    config: RankingConfig,
}

impl BoostCalculator {
    /// Create a new boost calculator with the given configuration.
    pub fn new(config: RankingConfig) -> Self {
        Self { config }
    }

    /// Apply boosts to tags based on contextual signals.
    ///
    /// Produces RankedTag instances with final scores computed as:
    /// ```text
    /// final_rank = base_rank Ã— boost Ã— git_weight Ã— caller_weight
    /// ```
    ///
    /// # Arguments
    ///
    /// * `tags_by_file` - Map from absolute file path to its tags
    /// * `file_ranks` - PageRank scores for files (keyed by relative path)
    /// * `symbol_ranks` - Optional PageRank scores for symbols (keyed by (file, name))
    /// * `chat_fnames` - Files being actively edited (absolute paths)
    /// * `mentioned_fnames` - Files explicitly mentioned in query (relative paths)
    /// * `mentioned_idents` - Symbols explicitly mentioned in query
    /// * `temporal_boost_files` - Files that co-change with chat files (relative paths)
    /// * `git_weights` - Recency/churn-based git weights (keyed by relative path)
    /// * `caller_weights` - Reverse edge bias weights for debugging intent
    /// * `focus_expansion_weights` - Graph neighbor expansion weights for (file, symbol)
    ///
    /// # Returns
    ///
    /// Vector of RankedTag sorted by final rank (descending - highest first).
    /// Only includes definition tags (TagKind::Def).
    pub fn apply_boosts(
        &self,
        tags_by_file: &HashMap<String, Vec<Tag>>,
        file_ranks: &HashMap<String, f64>,
        symbol_ranks: Option<&HashMap<(Arc<str>, Arc<str>), f64>>,
        chat_fnames: &HashSet<String>,
        mentioned_fnames: &HashSet<String>,
        mentioned_idents: &HashSet<String>,
        temporal_boost_files: &HashSet<String>,
        git_weights: Option<&HashMap<String, f64>>,
        caller_weights: Option<&HashMap<String, f64>>,
        focus_expansion_weights: Option<&HashMap<(Arc<str>, Arc<str>), f64>>,
    ) -> Vec<RankedTag> {
        let mut result = Vec::new();

        // Convert chat_fnames (absolute paths) to relative for comparison
        let chat_rel_fnames: HashSet<String> = chat_fnames
            .iter()
            .map(|f| extract_rel_fname(f))
            .collect();

        for (fname, tags) in tags_by_file {
            let rel_fname = extract_rel_fname(fname);

            // Get file-level rank and weights
            let file_rank = file_ranks.get(&rel_fname).copied().unwrap_or(0.0);
            let git_weight = git_weights
                .and_then(|w| w.get(&rel_fname))
                .copied()
                .unwrap_or(1.0);
            // Caller weight with hub damping: balance between "called = important"
            // and "utility function = noise".
            //
            // The interplay between boost_caller_weight and hub_damping:
            //   - boost_caller_weight amplifies the caller signal
            //   - hub_damping counteracts it to penalize "hub" nodes
            //
            // Effective formula:
            //   effective_boost = boost_caller_weight * (1.0 - hub_damping)
            //
            // With hub_damping = 0.0: full boost (called functions are important)
            // With hub_damping = 1.0: boost neutralized (caller count ignored)
            // With hub_damping > 1.0: penalty (hubs are downranked)
            let raw_caller_weight = caller_weights
                .and_then(|w| w.get(&rel_fname))
                .copied()
                .unwrap_or(1.0);

            // Apply hub damping: reduce or invert the caller weight effect
            let effective_boost = self.config.boost_caller_weight * (1.0 - self.config.hub_damping);
            let caller_weight = (1.0 + (raw_caller_weight - 1.0) * effective_boost).max(0.01);

            // Process only definition tags
            for tag in tags.iter().filter(|t| t.kind == TagKind::Def) {
                // Determine base rank: use symbol rank if available, else file rank
                let base_rank = symbol_ranks
                    .and_then(|sr| {
                        let key = (Arc::clone(&tag.rel_fname), Arc::clone(&tag.name));
                        sr.get(&key).copied()
                    })
                    .unwrap_or(file_rank);

                // Calculate contextual boost by multiplying all applicable signals
                let mut boost = 1.0;

                // Boost 1: Mentioned identifier (10x) - symbol explicitly in query
                if mentioned_idents.contains(tag.name.as_ref()) {
                    boost *= self.config.boost_mentioned_ident;
                }

                // Boost 2: Mentioned file (5x) - file explicitly in query
                if mentioned_fnames.contains(&rel_fname) {
                    boost *= self.config.boost_mentioned_file;
                }

                // Boost 3: Chat file (20x) - file being actively edited
                if chat_rel_fnames.contains(&rel_fname) {
                    boost *= self.config.boost_chat_file;
                }

                // Boost 4: Temporal coupling (3x) - co-changes with chat files
                if temporal_boost_files.contains(&rel_fname) {
                    boost *= self.config.boost_temporal_coupling;
                }

                // Boost 5: Focus expansion (5x Ã— expansion_weight) - graph neighbor
                if let Some(expansion_weights) = focus_expansion_weights {
                    let key = (Arc::clone(&tag.rel_fname), Arc::clone(&tag.name));
                    if let Some(&expansion_weight) = expansion_weights.get(&key) {
                        boost *= self.config.boost_focus_expansion * expansion_weight;
                    }
                }

                // Compute final rank: base Ã— boost Ã— git Ã— caller
                let final_rank = base_rank * boost * git_weight * caller_weight;

                result.push(RankedTag::new(final_rank, tag.clone()));
            }
        }

        // Sort by rank descending (highest first)
        result.sort();

        result
    }
}

/// Extract relative filename from absolute path.
///
/// Simplified heuristic: strips leading slash. In production, this would use
/// proper path resolution relative to repo root.
fn extract_rel_fname(abs_fname: &str) -> String {
    abs_fname.strip_prefix('/').unwrap_or(abs_fname).to_string()
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::types::TagKind;

    /// Helper to create a test tag
    fn make_tag(rel_fname: &str, name: &str, kind: TagKind) -> Tag {
        Tag {
            rel_fname: Arc::from(rel_fname),
            fname: Arc::from(format!("/{}", rel_fname)),
            line: 1,
            name: Arc::from(name),
            kind,
            node_type: Arc::from("function"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
        metadata: None,
        }
    }

    #[test]
    fn test_no_boosts() {
        // Without any contextual signals, should just use base ranks
        let config = RankingConfig::default();
        let calculator = BoostCalculator::new(config);

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![make_tag("a.rs", "foo", TagKind::Def)],
        );

        let mut file_ranks = HashMap::new();
        file_ranks.insert("a.rs".to_string(), 0.5);

        let result = calculator.apply_boosts(
            &tags_by_file,
            &file_ranks,
            None,
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            None,
            None,
            None,
        );

        assert_eq!(result.len(), 1);
        assert_eq!(result[0].rank, 0.5); // base_rank Ã— 1.0 (no boosts)
        assert_eq!(result[0].tag.name.as_ref(), "foo");
    }

    #[test]
    fn test_mentioned_ident_boost() {
        // Mentioned identifier should get 10x boost
        let config = RankingConfig::default();
        let calculator = BoostCalculator::new(config.clone());

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![make_tag("a.rs", "foo", TagKind::Def)],
        );

        let mut file_ranks = HashMap::new();
        file_ranks.insert("a.rs".to_string(), 0.1);

        let mut mentioned_idents = HashSet::new();
        mentioned_idents.insert("foo".to_string());

        let result = calculator.apply_boosts(
            &tags_by_file,
            &file_ranks,
            None,
            &HashSet::new(),
            &HashSet::new(),
            &mentioned_idents,
            &HashSet::new(),
            None,
            None,
            None,
        );

        assert_eq!(result.len(), 1);
        assert_eq!(result[0].rank, 0.1 * config.boost_mentioned_ident); // 0.1 Ã— 10 = 1.0
    }

    #[test]
    fn test_mentioned_file_boost() {
        // Mentioned file should get 5x boost
        let config = RankingConfig::default();
        let calculator = BoostCalculator::new(config.clone());

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![make_tag("a.rs", "foo", TagKind::Def)],
        );

        let mut file_ranks = HashMap::new();
        file_ranks.insert("a.rs".to_string(), 0.2);

        let mut mentioned_fnames = HashSet::new();
        mentioned_fnames.insert("a.rs".to_string());

        let result = calculator.apply_boosts(
            &tags_by_file,
            &file_ranks,
            None,
            &HashSet::new(),
            &mentioned_fnames,
            &HashSet::new(),
            &HashSet::new(),
            None,
            None,
            None,
        );

        assert_eq!(result.len(), 1);
        assert_eq!(result[0].rank, 0.2 * config.boost_mentioned_file); // 0.2 Ã— 5 = 1.0
    }

    #[test]
    fn test_chat_file_boost() {
        // Chat file should get 20x boost
        let config = RankingConfig::default();
        let calculator = BoostCalculator::new(config.clone());

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![make_tag("a.rs", "foo", TagKind::Def)],
        );

        let mut file_ranks = HashMap::new();
        file_ranks.insert("a.rs".to_string(), 0.05);

        let mut chat_fnames = HashSet::new();
        chat_fnames.insert("/a.rs".to_string()); // Absolute path

        let result = calculator.apply_boosts(
            &tags_by_file,
            &file_ranks,
            None,
            &chat_fnames,
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            None,
            None,
            None,
        );

        assert_eq!(result.len(), 1);
        assert_eq!(result[0].rank, 0.05 * config.boost_chat_file); // 0.05 Ã— 20 = 1.0
    }

    #[test]
    fn test_temporal_coupling_boost() {
        // Temporal coupling should get 3x boost
        let config = RankingConfig::default();
        let calculator = BoostCalculator::new(config.clone());

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![make_tag("a.rs", "foo", TagKind::Def)],
        );

        let mut file_ranks = HashMap::new();
        file_ranks.insert("a.rs".to_string(), 0.5);

        let mut temporal_boost_files = HashSet::new();
        temporal_boost_files.insert("a.rs".to_string());

        let result = calculator.apply_boosts(
            &tags_by_file,
            &file_ranks,
            None,
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            &temporal_boost_files,
            None,
            None,
            None,
        );

        assert_eq!(result.len(), 1);
        assert_eq!(result[0].rank, 0.5 * config.boost_temporal_coupling); // 0.5 Ã— 3 = 1.5
    }

    #[test]
    fn test_multiple_boosts_multiply() {
        // Multiple boosts should stack multiplicatively
        let config = RankingConfig::default();
        let calculator = BoostCalculator::new(config.clone());

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![make_tag("a.rs", "foo", TagKind::Def)],
        );

        let mut file_ranks = HashMap::new();
        file_ranks.insert("a.rs".to_string(), 0.01);

        let mut mentioned_idents = HashSet::new();
        mentioned_idents.insert("foo".to_string());

        let mut chat_fnames = HashSet::new();
        chat_fnames.insert("/a.rs".to_string());

        let result = calculator.apply_boosts(
            &tags_by_file,
            &file_ranks,
            None,
            &chat_fnames,
            &HashSet::new(),
            &mentioned_idents,
            &HashSet::new(),
            None,
            None,
            None,
        );

        assert_eq!(result.len(), 1);
        // 0.01 Ã— 10 (mentioned_ident) Ã— 20 (chat_file) = 2.0
        assert_eq!(
            result[0].rank,
            0.01 * config.boost_mentioned_ident * config.boost_chat_file
        );
    }

    #[test]
    fn test_git_weight_multiplier() {
        // Git weight should multiply into final rank
        let config = RankingConfig::default();
        let calculator = BoostCalculator::new(config);

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![make_tag("a.rs", "foo", TagKind::Def)],
        );

        let mut file_ranks = HashMap::new();
        file_ranks.insert("a.rs".to_string(), 1.0);

        let mut git_weights = HashMap::new();
        git_weights.insert("a.rs".to_string(), 2.0);

        let result = calculator.apply_boosts(
            &tags_by_file,
            &file_ranks,
            None,
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            Some(&git_weights),
            None,
            None,
        );

        assert_eq!(result.len(), 1);
        assert_eq!(result[0].rank, 1.0 * 2.0); // base Ã— git_weight
    }

    #[test]
    fn test_caller_weight_multiplier() {
        // Caller weight should multiply into final rank
        let config = RankingConfig::default();
        let calculator = BoostCalculator::new(config);

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![make_tag("a.rs", "foo", TagKind::Def)],
        );

        let mut file_ranks = HashMap::new();
        file_ranks.insert("a.rs".to_string(), 1.0);

        let mut caller_weights = HashMap::new();
        caller_weights.insert("a.rs".to_string(), 1.5);

        let result = calculator.apply_boosts(
            &tags_by_file,
            &file_ranks,
            None,
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            None,
            Some(&caller_weights),
            None,
        );

        assert_eq!(result.len(), 1);
        // With boost_caller_weight=2.0 (default) and raw=1.5:
        // caller_weight = 1.0 + (1.5 - 1.0) * 2.0 = 2.0
        assert_eq!(result[0].rank, 1.0 * 2.0); // base Ã— scaled_caller_weight
    }

    #[test]
    fn test_focus_expansion_weight() {
        // Focus expansion weight should boost graph neighbors
        let config = RankingConfig::default();
        let calculator = BoostCalculator::new(config.clone());

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![make_tag("a.rs", "foo", TagKind::Def)],
        );

        let mut file_ranks = HashMap::new();
        file_ranks.insert("a.rs".to_string(), 0.1);

        let mut focus_expansion_weights = HashMap::new();
        focus_expansion_weights.insert((Arc::from("a.rs"), Arc::from("foo")), 0.8);

        let result = calculator.apply_boosts(
            &tags_by_file,
            &file_ranks,
            None,
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            None,
            None,
            Some(&focus_expansion_weights),
        );

        assert_eq!(result.len(), 1);
        // 0.1 Ã— (5.0 Ã— 0.8) = 0.4
        assert_eq!(
            result[0].rank,
            0.1 * config.boost_focus_expansion * 0.8
        );
    }

    #[test]
    fn test_symbol_rank_overrides_file_rank() {
        // When symbol rank is available, use it instead of file rank
        let config = RankingConfig::default();
        let calculator = BoostCalculator::new(config);

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![make_tag("a.rs", "foo", TagKind::Def)],
        );

        let mut file_ranks = HashMap::new();
        file_ranks.insert("a.rs".to_string(), 0.1);

        let mut symbol_ranks = HashMap::new();
        symbol_ranks.insert((Arc::from("a.rs"), Arc::from("foo")), 0.9);

        let result = calculator.apply_boosts(
            &tags_by_file,
            &file_ranks,
            Some(&symbol_ranks),
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            None,
            None,
            None,
        );

        assert_eq!(result.len(), 1);
        assert_eq!(result[0].rank, 0.9); // Uses symbol rank, not file rank
    }

    #[test]
    fn test_only_definitions_included() {
        // Only TagKind::Def should be included, not TagKind::Ref
        let config = RankingConfig::default();
        let calculator = BoostCalculator::new(config);

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![
                make_tag("a.rs", "foo", TagKind::Def),
                make_tag("a.rs", "bar", TagKind::Ref),
            ],
        );

        let mut file_ranks = HashMap::new();
        file_ranks.insert("a.rs".to_string(), 1.0);

        let result = calculator.apply_boosts(
            &tags_by_file,
            &file_ranks,
            None,
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            None,
            None,
            None,
        );

        assert_eq!(result.len(), 1); // Only "foo" (Def), not "bar" (Ref)
        assert_eq!(result[0].tag.name.as_ref(), "foo");
    }

    #[test]
    fn test_sorting_descending() {
        // Results should be sorted by rank descending (highest first)
        let config = RankingConfig::default();
        let calculator = BoostCalculator::new(config);

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![
                make_tag("a.rs", "low", TagKind::Def),
                make_tag("a.rs", "medium", TagKind::Def),
                make_tag("a.rs", "high", TagKind::Def),
            ],
        );

        let mut file_ranks = HashMap::new();
        file_ranks.insert("a.rs".to_string(), 1.0);

        let mut symbol_ranks = HashMap::new();
        symbol_ranks.insert((Arc::from("a.rs"), Arc::from("low")), 0.1);
        symbol_ranks.insert((Arc::from("a.rs"), Arc::from("medium")), 0.5);
        symbol_ranks.insert((Arc::from("a.rs"), Arc::from("high")), 0.9);

        let result = calculator.apply_boosts(
            &tags_by_file,
            &file_ranks,
            Some(&symbol_ranks),
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            &HashSet::new(),
            None,
            None,
            None,
        );

        assert_eq!(result.len(), 3);
        assert_eq!(result[0].tag.name.as_ref(), "high");
        assert_eq!(result[1].tag.name.as_ref(), "medium");
        assert_eq!(result[2].tag.name.as_ref(), "low");
    }

    #[test]
    fn test_combined_all_weights() {
        // Test all weight/boost combinations together
        let config = RankingConfig::default();
        let calculator = BoostCalculator::new(config.clone());

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![make_tag("a.rs", "foo", TagKind::Def)],
        );

        let mut file_ranks = HashMap::new();
        file_ranks.insert("a.rs".to_string(), 0.1);

        let mut mentioned_idents = HashSet::new();
        mentioned_idents.insert("foo".to_string());

        let mut mentioned_fnames = HashSet::new();
        mentioned_fnames.insert("a.rs".to_string());

        let mut chat_fnames = HashSet::new();
        chat_fnames.insert("/a.rs".to_string());

        let mut temporal_boost_files = HashSet::new();
        temporal_boost_files.insert("a.rs".to_string());

        let mut git_weights = HashMap::new();
        git_weights.insert("a.rs".to_string(), 2.0);

        let mut caller_weights = HashMap::new();
        caller_weights.insert("a.rs".to_string(), 1.5);

        let mut focus_expansion_weights = HashMap::new();
        focus_expansion_weights.insert((Arc::from("a.rs"), Arc::from("foo")), 0.5);

        let result = calculator.apply_boosts(
            &tags_by_file,
            &file_ranks,
            None,
            &chat_fnames,
            &mentioned_fnames,
            &mentioned_idents,
            &temporal_boost_files,
            Some(&git_weights),
            Some(&caller_weights),
            Some(&focus_expansion_weights),
        );

        assert_eq!(result.len(), 1);

        // Expected: 0.1 Ã— 10 (ident) Ã— 5 (file) Ã— 20 (chat) Ã— 3 (temporal)
        //           Ã— (5 Ã— 0.5) (focus) Ã— 2.0 (git) Ã— scaled_caller
        // With raw_caller=1.5, boost_caller_weight=2.0:
        //   scaled_caller = 1.0 + (1.5 - 1.0) * 2.0 = 2.0
        let raw_caller = 1.5;
        let scaled_caller = 1.0 + (raw_caller - 1.0) * config.boost_caller_weight;
        let expected = 0.1
            * config.boost_mentioned_ident
            * config.boost_mentioned_file
            * config.boost_chat_file
            * config.boost_temporal_coupling
            * (config.boost_focus_expansion * 0.5)
            * 2.0
            * scaled_caller;

        assert!((result[0].rank - expected).abs() < 1e-6);
    }

    #[test]
    fn test_extract_rel_fname() {
        assert_eq!(extract_rel_fname("/a.rs"), "a.rs");
        assert_eq!(extract_rel_fname("/src/lib.rs"), "src/lib.rs");
        assert_eq!(extract_rel_fname("no_slash.rs"), "no_slash.rs");
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/ranking/bridges.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Bridge detection stub.

pub struct BridgeDetector;

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/ranking/coupling.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Testâ†”Source file coupling detection.
//!
//! Codex optimization identified that path-aware testâ†”crate coupling edges
//! are a missing architectural feature. This module detects test files and
//! links them to the source files they test.
//!
//! ## Rationale
//!
//! When debugging or exploring code, test files are highly relevant to their
//! implementation files. But there's no explicit code reference between them -
//! the coupling is implicit in file naming conventions:
//!
//! ```text
//! tests/test_parser.rs  â†’  src/parser.rs
//! foo_test.py           â†’  foo.py
//! Button.spec.tsx       â†’  Button.tsx
//! ```
//!
//! By detecting these patterns and creating synthetic edges, we can:
//! 1. Boost test files when focusing on implementation
//! 2. Boost implementation when focusing on tests
//! 3. Surface testâ†”impl pairs in focus expansion
//!
//! ## Usage
//!
//! ```ignore
//! let detector = TestCouplingDetector::new();
//! let edges = detector.detect_couplings(&file_list);
//! // edges: Vec<(test_file, source_file, confidence)>
//! ```

use std::collections::HashMap;
use std::path::Path;
use std::sync::Arc;

/// Detects testâ†”source file couplings based on naming conventions.
///
/// Supports multiple language conventions:
/// - Rust: `tests/test_*.rs`, `*_test.rs`
/// - Python: `test_*.py`, `*_test.py`, `tests/test_*.py`
/// - JavaScript/TypeScript: `*.spec.ts`, `*.test.js`, `__tests__/*.js`
/// - Go: `*_test.go`
pub struct TestCouplingDetector {
    /// Minimum confidence to include a coupling
    min_confidence: f64,
}

impl TestCouplingDetector {
    pub fn new() -> Self {
        Self {
            min_confidence: 0.5,
        }
    }

    /// Set minimum confidence threshold for couplings.
    pub fn with_min_confidence(mut self, min: f64) -> Self {
        self.min_confidence = min;
        self
    }

    /// Detect testâ†”source couplings from a list of file paths.
    ///
    /// Returns edges as (test_file, source_file, confidence).
    /// Confidence is based on how strong the naming match is:
    /// - 0.9: Exact match (test_foo.py â†’ foo.py exists)
    /// - 0.7: Directory match (tests/test_foo.py â†’ src/foo.py exists)
    /// - 0.5: Pattern match but source not found
    pub fn detect_couplings(
        &self,
        files: &[impl AsRef<Path>],
    ) -> Vec<(Arc<str>, Arc<str>, f64)> {
        // Build a set of all files for existence checking
        let file_set: HashMap<String, &Path> = files
            .iter()
            .map(|f| {
                let p = f.as_ref();
                let name = p.file_name()
                    .map(|n| n.to_string_lossy().to_string())
                    .unwrap_or_default();
                (name, p)
            })
            .collect();

        // Also index by stem (without extension) for flexible matching
        let stem_map: HashMap<String, Vec<&Path>> = files
            .iter()
            .map(|f| f.as_ref())
            .filter_map(|p| {
                let stem = p.file_stem()?.to_string_lossy().to_string();
                Some((stem, p))
            })
            .fold(HashMap::new(), |mut map, (stem, path)| {
                map.entry(stem).or_default().push(path);
                map
            });

        let mut couplings = Vec::new();

        for file in files {
            let path = file.as_ref();
            let path_str = path.to_string_lossy();

            // Check if this is a test file
            if let Some((source_stem, confidence_base)) = self.is_test_file(path) {
                // Try to find the corresponding source file
                if let Some(source_candidates) = stem_map.get(&source_stem) {
                    for source_path in source_candidates {
                        // Skip if source is also a test file
                        if self.is_test_file(source_path).is_some() {
                            continue;
                        }

                        // Calculate final confidence based on path proximity
                        let confidence = self.calculate_confidence(
                            path,
                            source_path,
                            confidence_base,
                        );

                        if confidence >= self.min_confidence {
                            couplings.push((
                                Arc::from(path_str.as_ref()),
                                Arc::from(source_path.to_string_lossy().as_ref()),
                                confidence,
                            ));
                        }
                    }
                }
            }
        }

        couplings
    }

    /// Check if a file is a test file and extract the source stem it tests.
    ///
    /// Returns (source_stem, base_confidence) if it's a test file.
    fn is_test_file(&self, path: &Path) -> Option<(String, f64)> {
        let file_name = path.file_name()?.to_string_lossy();
        let stem = path.file_stem()?.to_string_lossy();
        let path_str = path.to_string_lossy();

        // Rust: tests/test_foo.rs, foo_test.rs
        if file_name.ends_with(".rs") {
            if stem.starts_with("test_") {
                return Some((stem[5..].to_string(), 0.9));
            }
            if stem.ends_with("_test") {
                return Some((stem[..stem.len()-5].to_string(), 0.9));
            }
            // In tests/ directory
            if path_str.contains("/tests/") || path_str.contains("\\tests\\") {
                return Some((stem.to_string(), 0.7));
            }
        }

        // Python: test_foo.py, foo_test.py
        if file_name.ends_with(".py") {
            if stem.starts_with("test_") {
                return Some((stem[5..].to_string(), 0.9));
            }
            if stem.ends_with("_test") {
                return Some((stem[..stem.len()-5].to_string(), 0.9));
            }
        }

        // JavaScript/TypeScript: foo.spec.ts, foo.test.js
        if file_name.ends_with(".ts") || file_name.ends_with(".tsx")
            || file_name.ends_with(".js") || file_name.ends_with(".jsx")
        {
            if stem.ends_with(".spec") {
                return Some((stem[..stem.len()-5].to_string(), 0.9));
            }
            if stem.ends_with(".test") {
                return Some((stem[..stem.len()-5].to_string(), 0.9));
            }
            // __tests__ directory convention
            if path_str.contains("/__tests__/") || path_str.contains("\\__tests__\\") {
                return Some((stem.to_string(), 0.8));
            }
        }

        // Go: foo_test.go
        if file_name.ends_with(".go") && stem.ends_with("_test") {
            return Some((stem[..stem.len()-5].to_string(), 0.9));
        }

        None
    }

    /// Calculate confidence based on path proximity.
    ///
    /// Higher confidence when test and source are in related directories:
    /// - Same directory: +0.1
    /// - tests/ â†’ src/: +0.05
    /// - Completely unrelated: no bonus
    fn calculate_confidence(&self, test_path: &Path, source_path: &Path, base: f64) -> f64 {
        let test_parent = test_path.parent().map(|p| p.to_string_lossy().to_string());
        let source_parent = source_path.parent().map(|p| p.to_string_lossy().to_string());

        match (test_parent, source_parent) {
            (Some(tp), Some(sp)) => {
                // Same directory
                if tp == sp {
                    return (base + 0.1).min(1.0);
                }

                // tests/ â†’ src/ pattern
                if (tp.contains("/tests") || tp.contains("\\tests"))
                    && (sp.contains("/src") || sp.contains("\\src"))
                {
                    return (base + 0.05).min(1.0);
                }

                // Adjacent directories (e.g., foo/tests and foo/src)
                let tp_parts: Vec<_> = tp.split(['/', '\\']).collect();
                let sp_parts: Vec<_> = sp.split(['/', '\\']).collect();
                if tp_parts.len() > 1 && sp_parts.len() > 1 {
                    if tp_parts[..tp_parts.len()-1] == sp_parts[..sp_parts.len()-1] {
                        return (base + 0.05).min(1.0);
                    }
                }

                base
            }
            _ => base,
        }
    }

    /// Convert couplings to symbol graph edges format.
    ///
    /// Returns edges in the format expected by FocusResolver.expand_via_graph:
    /// (from_file, from_symbol, to_file, to_symbol)
    ///
    /// For file-level coupling, we use a synthetic symbol name "__file__".
    pub fn as_symbol_edges(
        &self,
        couplings: &[(Arc<str>, Arc<str>, f64)],
    ) -> Vec<(Arc<str>, Arc<str>, Arc<str>, Arc<str>)> {
        let file_symbol: Arc<str> = Arc::from("__file__");

        couplings
            .iter()
            .flat_map(|(test, source, _conf)| {
                // Bidirectional: testâ†”source
                vec![
                    (test.clone(), file_symbol.clone(), source.clone(), file_symbol.clone()),
                    (source.clone(), file_symbol.clone(), test.clone(), file_symbol.clone()),
                ]
            })
            .collect()
    }
}

impl Default for TestCouplingDetector {
    fn default() -> Self {
        Self::new()
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::path::PathBuf;

    #[test]
    fn test_detect_rust_test_files() {
        let detector = TestCouplingDetector::new();

        let files: Vec<PathBuf> = vec![
            "src/parser.rs".into(),
            "tests/test_parser.rs".into(),
            "src/lexer.rs".into(),
            "src/lexer_test.rs".into(),
        ];

        let couplings = detector.detect_couplings(&files);

        // Should find test_parser â†’ parser and lexer_test â†’ lexer
        assert_eq!(couplings.len(), 2);

        let test_files: Vec<_> = couplings.iter().map(|(t, _, _)| t.as_ref()).collect();
        assert!(test_files.contains(&"tests/test_parser.rs"));
        assert!(test_files.contains(&"src/lexer_test.rs"));
    }

    #[test]
    fn test_detect_python_test_files() {
        let detector = TestCouplingDetector::new();

        let files: Vec<PathBuf> = vec![
            "mymodule.py".into(),
            "test_mymodule.py".into(),
            "utils.py".into(),
            "utils_test.py".into(),
        ];

        let couplings = detector.detect_couplings(&files);
        assert_eq!(couplings.len(), 2);
    }

    #[test]
    fn test_detect_js_spec_files() {
        let detector = TestCouplingDetector::new();

        let files: Vec<PathBuf> = vec![
            "Button.tsx".into(),
            "Button.spec.tsx".into(),
            "utils.ts".into(),
            "utils.test.ts".into(),
        ];

        let couplings = detector.detect_couplings(&files);
        assert_eq!(couplings.len(), 2);
    }

    #[test]
    fn test_no_self_coupling() {
        let detector = TestCouplingDetector::new();

        // Test files shouldn't couple to other test files
        let files: Vec<PathBuf> = vec![
            "test_foo.py".into(),
            "test_bar.py".into(),
        ];

        let couplings = detector.detect_couplings(&files);
        assert!(couplings.is_empty());
    }

    #[test]
    fn test_confidence_same_directory() {
        let detector = TestCouplingDetector::new();

        let files: Vec<PathBuf> = vec![
            "src/foo.rs".into(),
            "src/foo_test.rs".into(),
        ];

        let couplings = detector.detect_couplings(&files);
        assert_eq!(couplings.len(), 1);
        // Same directory should boost confidence
        assert!(couplings[0].2 > 0.9);
    }

    #[test]
    fn test_as_symbol_edges() {
        let detector = TestCouplingDetector::new();

        let couplings = vec![
            (Arc::from("test_foo.py"), Arc::from("foo.py"), 0.9),
        ];

        let edges = detector.as_symbol_edges(&couplings);

        // Should be bidirectional
        assert_eq!(edges.len(), 2);
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/ranking/focus.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Focus query resolution and graph expansion.
//!
//! The focus system enables targeted exploration of large codebases:
//! - Fuzzy matching: resolve user queries to specific symbols and files
//! - Graph expansion: BFS with decay to find related code
//! - Multi-target: comma-separated queries like "auth,parser,main.rs"
//!
//! ## Fuzzy Matching Strategy
//!
//! Matches are attempted in priority order:
//! 1. Exact match (case-insensitive)
//! 2. Substring containment
//! 3. Word-part subset matching (handles snake_case, camelCase)
//! 4. Any query part found in name (3+ chars)
//! 5. Stem matching (auth ~ authenticate ~ authorization)
//! 6. Edit distance <= 1 for typos (4+ chars)
//!
//! This enables natural queries:
//! - "auth" â†’ matches "authenticate", "authorization", "auth_handler"
//! - "getuser" â†’ matches "getUserName", "get_user_profile"
//! - "parsr" â†’ matches "parser" (typo tolerance)
//!
//! ## Graph Expansion
//!
//! From seed symbols, expand via BFS with exponential decay:
//! - Seed nodes: weight = 1.0
//! - 1-hop neighbors: weight = decay^1 (default 0.5)
//! - 2-hop neighbors: weight = decay^2 (default 0.25)
//!
//! This surfaces related code without flooding with transitive dependencies.

use std::collections::{HashMap, HashSet, VecDeque};
use std::path::Path;
use std::sync::Arc;

use crate::types::Tag;

/// Focus resolver - converts user queries into matched files/symbols,
/// then expands via graph traversal to find related code.
pub struct FocusResolver {
    /// Repository root for resolving relative paths
    root: std::path::PathBuf,
}

impl FocusResolver {
    /// Create a new focus resolver rooted at the given path.
    pub fn new(root: impl AsRef<Path>) -> Self {
        Self {
            root: root.as_ref().to_path_buf(),
        }
    }

    /// Resolve focus targets to matched files and symbol identifiers.
    ///
    /// Supports:
    /// - File paths: "src/lib.rs", "./parser.rs", absolute paths
    /// - Symbol queries: "authenticate", "parse_config", fuzzy matched
    /// - Comma-separated: "auth,parser,main.rs"
    ///
    /// Returns:
    /// - matched_files: absolute file paths that match
    /// - matched_idents: symbol names (Arc<str>) that match
    pub fn resolve(
        &self,
        focus_targets: &[String],
        tags_by_file: &HashMap<String, Vec<Tag>>,
    ) -> (HashSet<String>, HashSet<String>) {
        let mut matched_files = HashSet::new();
        let mut matched_idents = HashSet::new();

        // Parse comma-separated targets
        let targets = focus_targets
            .iter()
            .flat_map(|t| t.split(',').map(|s| s.trim()))
            .filter(|t| !t.is_empty())
            .collect::<Vec<_>>();

        for target in targets {
            // Try as file path first (absolute or relative)
            if let Some(matched_file) = self.try_match_file(target, tags_by_file) {
                matched_files.insert(matched_file);
                continue;
            }

            // Try as symbol query - fuzzy match against all symbols
            let query_idents = self.fuzzy_match_symbols(target, tags_by_file);
            if !query_idents.is_empty() {
                matched_idents.extend(query_idents);
                continue;
            }

            // No matches - silently skip (user may be experimenting)
            eprintln!("focus: no matches for '{}'", target);
        }

        (matched_files, matched_idents)
    }

    /// Try to match a target as a file path.
    fn try_match_file(
        &self,
        target: &str,
        tags_by_file: &HashMap<String, Vec<Tag>>,
    ) -> Option<String> {
        // Only try file matching if target looks like a file path
        // (contains path separator or file extension)
        if !target.contains('/') && !target.contains('.') {
            return None;
        }

        // Try absolute path
        let abs_path = std::path::Path::new(target);
        if abs_path.is_absolute() && abs_path.exists() {
            return Some(abs_path.to_string_lossy().to_string());
        }

        // Try relative to root
        let rel_path = self.root.join(target);
        if rel_path.exists() {
            return Some(rel_path.to_string_lossy().to_string());
        }

        // Try matching against known files (suffix matching)
        // E.g., "lib.rs" matches "src/lib.rs"
        for file_path in tags_by_file.keys() {
            if file_path.ends_with(target) || file_path.contains(&format!("/{}", target)) {
                return Some(file_path.clone());
            }
        }

        None
    }

    /// Fuzzy match a query against all symbols in tags_by_file.
    /// Returns the set of matched symbol names.
    fn fuzzy_match_symbols(
        &self,
        query: &str,
        tags_by_file: &HashMap<String, Vec<Tag>>,
    ) -> HashSet<String> {
        let mut matched = HashSet::new();

        for tags in tags_by_file.values() {
            for tag in tags {
                if matches_query(&tag.name, query) {
                    matched.insert(tag.name.to_string());
                }
            }
        }

        matched
    }

    /// Expand focus via BFS graph traversal with exponential decay.
    ///
    /// Starting from matched identifiers (seed nodes), traverse the symbol
    /// graph to find related code. Each hop reduces weight by decay factor.
    ///
    /// # Arguments
    /// * `matched_idents` - Seed symbol names
    /// * `symbol_graph` - Edges as (from_file, from_sym, to_file, to_sym)
    /// * `max_hops` - Maximum BFS depth (typically 1-2)
    /// * `decay` - Weight decay per hop (typically 0.5)
    ///
    /// # Returns
    /// Map of (file, symbol) -> weight, where weight = decay^hop_distance
    pub fn expand_via_graph(
        &self,
        matched_idents: &HashSet<String>,
        symbol_graph: &[(Arc<str>, Arc<str>, Arc<str>, Arc<str>)],
        max_hops: usize,
        decay: f64,
    ) -> HashMap<(Arc<str>, Arc<str>), f64> {
        let mut expanded = HashMap::new();

        // Find seed nodes: any symbol in graph matching our identifiers
        let seeds: HashSet<_> = symbol_graph
            .iter()
            .filter_map(|(from_file, from_sym, to_file, to_sym)| {
                // Check both endpoints of each edge
                let mut matches = Vec::new();
                if matched_idents.contains(&**from_sym) {
                    matches.push((from_file.clone(), from_sym.clone()));
                }
                if matched_idents.contains(&**to_sym) {
                    matches.push((to_file.clone(), to_sym.clone()));
                }
                if matches.is_empty() {
                    None
                } else {
                    Some(matches)
                }
            })
            .flatten()
            .collect();

        if seeds.is_empty() {
            return expanded;
        }

        // Initialize seeds with weight 1.0
        for seed in &seeds {
            expanded.insert(seed.clone(), 1.0);
        }

        // BFS expansion with decay
        let mut frontier: VecDeque<_> = seeds.into_iter().collect();
        let mut visited = HashSet::new();

        for hop in 1..=max_hops {
            let weight = decay.powi(hop as i32);
            let frontier_size = frontier.len();

            for _ in 0..frontier_size {
                let node = match frontier.pop_front() {
                    Some(n) => n,
                    None => break,
                };

                if !visited.insert(node.clone()) {
                    continue;
                }

                // Find neighbors (graph is undirected for expansion purposes)
                for (from_file, from_sym, to_file, to_sym) in symbol_graph {
                    let neighbor = if from_file == &node.0 && from_sym == &node.1 {
                        // Forward edge
                        Some((to_file.clone(), to_sym.clone()))
                    } else if to_file == &node.0 && to_sym == &node.1 {
                        // Backward edge (reverse reference graph)
                        Some((from_file.clone(), from_sym.clone()))
                    } else {
                        None
                    };

                    if let Some(neighbor_node) = neighbor {
                        // Only expand to new nodes
                        if !expanded.contains_key(&neighbor_node) {
                            expanded.insert(neighbor_node.clone(), weight);
                            frontier.push_back(neighbor_node);
                        }
                    }
                }
            }

            if frontier.is_empty() {
                break;
            }
        }

        expanded
    }
}

/// Fuzzy match a symbol name against a query.
///
/// Implements multi-strategy matching for natural queries:
/// 1. Exact match (case-insensitive)
/// 2. Substring containment
/// 3. Word-part subset (handles identifiers)
/// 4. Any query part in name (3+ chars)
/// 5. Stem matching (morphological variants)
/// 6. Edit distance <= 1 (typo tolerance)
fn matches_query(name: &str, query: &str) -> bool {
    let name_lower = name.to_lowercase();
    let query_lower = query.to_lowercase();

    // 1. Exact match
    if name_lower == query_lower {
        return true;
    }

    // 2. Substring match
    if name_lower.contains(&query_lower) {
        return true;
    }

    // 3. Word part matching - query parts must be subset of name parts
    let query_parts: HashSet<_> = split_identifier(&query_lower);
    let name_parts: HashSet<_> = split_identifier(&name_lower);

    if !query_parts.is_empty() && query_parts.is_subset(&name_parts) {
        return true;
    }

    // 4. Any query part in name (3+ chars to avoid false positives)
    if query_parts
        .iter()
        .any(|p| p.len() >= 3 && name_lower.contains(p))
    {
        return true;
    }

    // 5. Stem matching - handles morphological variants
    let query_stems: HashSet<_> = query_parts.iter().filter_map(|p| get_stem(p)).collect();
    let name_stems: HashSet<_> = name_parts.iter().filter_map(|p| get_stem(p)).collect();

    if !query_stems.is_empty() && !query_stems.is_disjoint(&name_stems) {
        return true;
    }

    // 6. Edit distance for typos (4+ chars, distance <= 1)
    if query_lower.len() >= 4 {
        for part in &name_parts {
            if part.len() >= 4 && levenshtein(&query_lower, part) <= 1 {
                return true;
            }
        }
    }

    false
}

/// Split identifier into constituent words.
///
/// Handles both snake_case and camelCase:
/// - "getUserName" -> ["get", "user", "name"]
/// - "get_user_name" -> ["get", "user", "name"]
/// - "HTTPServer" -> ["http", "server"]
fn split_identifier(s: &str) -> HashSet<String> {
    let mut parts = HashSet::new();
    let mut current = String::new();

    let chars: Vec<char> = s.chars().collect();
    let mut i = 0;

    while i < chars.len() {
        let ch = chars[i];

        if ch == '_' || ch == '-' || ch == '.' {
            // Word boundary - flush current
            if !current.is_empty() {
                parts.insert(current.to_lowercase());
                current.clear();
            }
            i += 1;
            continue;
        }

        if ch.is_uppercase() && !current.is_empty() {
            // camelCase boundary - check if this is an acronym
            // "HTTPServer" should split as HTTP, Server not H, T, T, P, Server
            let next_is_lower = i + 1 < chars.len() && chars[i + 1].is_lowercase();
            let prev_is_upper = current.chars().last().map_or(false, |c| c.is_uppercase());

            if next_is_lower || !prev_is_upper {
                // Boundary found
                parts.insert(current.to_lowercase());
                current.clear();
            }
        }

        current.push(ch);
        i += 1;
    }

    if !current.is_empty() {
        parts.insert(current.to_lowercase());
    }

    parts
}

/// Get canonical stem for a word.
///
/// Stem groups capture common morphological variants:
/// - auth, authenticate, authentication, authorized, authorization
/// - parse, parser, parsing, parsed
/// - valid, validate, validation, validator, invalid
///
/// Returns the canonical form (first element of the group).
fn get_stem(word: &str) -> Option<&'static str> {
    STEM_GROUPS
        .iter()
        .find(|group| group.contains(&word))
        .map(|group| group[0])
}

/// Stem groups for common programming vocabulary.
///
/// Each group lists morphological variants that should fuzzy-match.
/// First element is the canonical stem.
const STEM_GROUPS: &[&[&str]] = &[
    // Authentication/Authorization
    &["auth", "authenticate", "authentication", "authenticated", "authorize", "authorization", "authorized"],
    // Parsing
    &["parse", "parser", "parsing", "parsed"],
    // Validation
    &["valid", "validate", "validation", "validator", "validated", "invalid", "invalidate", "invalidated"],
    // Configuration
    &["config", "configure", "configuration", "configured", "configurator"],
    // Initialization
    &["init", "initialize", "initialization", "initialized", "initializer"],
    // Rendering
    &["render", "renderer", "rendering", "rendered"],
    // Caching
    &["cache", "caching", "cached"],
    // Handling
    &["handle", "handler", "handling", "handled"],
    // Execution
    &["exec", "execute", "execution", "executed", "executor"],
    // Processing
    &["process", "processor", "processing", "processed"],
    // Serialization
    &["serial", "serialize", "serialization", "serialized", "serializer", "deserialize", "deserialized"],
    // Connection
    &["connect", "connection", "connected", "connector", "disconnect", "disconnected"],
    // Transformation
    &["transform", "transformer", "transformation", "transformed"],
    // Compilation
    &["compile", "compiler", "compilation", "compiled"],
    // Evaluation
    &["eval", "evaluate", "evaluation", "evaluated", "evaluator"],
    // Generation
    &["gen", "generate", "generation", "generated", "generator"],
    // Registration
    &["register", "registration", "registered", "registry"],
    // Query/Request
    &["query", "request", "req"],
    // Response
    &["response", "resp", "reply"],
];

/// Compute Levenshtein edit distance between two strings.
///
/// Used for typo tolerance in fuzzy matching.
/// Optimized for early exit when distance exceeds threshold.
fn levenshtein(a: &str, b: &str) -> usize {
    let a_len = a.len();
    let b_len = b.len();

    if a_len == 0 {
        return b_len;
    }
    if b_len == 0 {
        return a_len;
    }

    // Use single-row optimization (space O(min(a,b)))
    let mut prev_row: Vec<usize> = (0..=b_len).collect();
    let mut curr_row = vec![0; b_len + 1];

    for (i, a_char) in a.chars().enumerate() {
        curr_row[0] = i + 1;

        for (j, b_char) in b.chars().enumerate() {
            let cost = if a_char == b_char { 0 } else { 1 };
            curr_row[j + 1] = (curr_row[j] + 1) // insertion
                .min(prev_row[j + 1] + 1) // deletion
                .min(prev_row[j] + cost); // substitution
        }

        std::mem::swap(&mut prev_row, &mut curr_row);
    }

    prev_row[b_len]
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_split_identifier() {
        // snake_case
        assert_eq!(
            split_identifier("get_user_name"),
            ["get", "user", "name"].iter().map(|s| s.to_string()).collect()
        );

        // camelCase
        assert_eq!(
            split_identifier("getUserName"),
            ["get", "user", "name"].iter().map(|s| s.to_string()).collect()
        );

        // PascalCase
        assert_eq!(
            split_identifier("GetUserName"),
            ["get", "user", "name"].iter().map(|s| s.to_string()).collect()
        );

        // Acronyms
        let parts = split_identifier("HTTPServer");
        assert!(parts.contains("http") || parts.contains("h")); // Either is acceptable
        assert!(parts.contains("server"));

        // Mixed
        assert_eq!(
            split_identifier("parse_JSONObject"),
            ["parse", "json", "object"].iter().map(|s| s.to_string()).collect()
        );
    }

    #[test]
    fn test_get_stem() {
        assert_eq!(get_stem("authenticate"), Some("auth"));
        assert_eq!(get_stem("authentication"), Some("auth"));
        assert_eq!(get_stem("authorized"), Some("auth"));
        assert_eq!(get_stem("auth"), Some("auth"));

        assert_eq!(get_stem("parser"), Some("parse"));
        assert_eq!(get_stem("parsing"), Some("parse"));
        assert_eq!(get_stem("parsed"), Some("parse"));

        assert_eq!(get_stem("validator"), Some("valid"));
        assert_eq!(get_stem("invalid"), Some("valid"));

        assert_eq!(get_stem("unknown_word"), None);
    }

    #[test]
    fn test_levenshtein() {
        assert_eq!(levenshtein("", ""), 0);
        assert_eq!(levenshtein("a", ""), 1);
        assert_eq!(levenshtein("", "b"), 1);
        assert_eq!(levenshtein("abc", "abc"), 0);
        assert_eq!(levenshtein("abc", "abd"), 1);
        assert_eq!(levenshtein("abc", "axc"), 1);
        assert_eq!(levenshtein("abc", "abcd"), 1);
        assert_eq!(levenshtein("parse", "parser"), 1);
        assert_eq!(levenshtein("parsr", "parser"), 1); // typo
    }

    #[test]
    fn test_matches_query_exact() {
        assert!(matches_query("authenticate", "authenticate"));
        assert!(matches_query("Authenticate", "authenticate")); // case insensitive
        assert!(matches_query("AUTHENTICATE", "authenticate"));
    }

    #[test]
    fn test_matches_query_substring() {
        assert!(matches_query("authenticate", "auth"));
        assert!(matches_query("user_authentication", "auth"));
        assert!(matches_query("HTTPServer", "http"));
    }

    #[test]
    fn test_matches_query_word_parts() {
        // Query parts are subset of name parts
        assert!(matches_query("getUserName", "getuser"));
        assert!(matches_query("get_user_name", "user"));
        assert!(matches_query("parseHTTPRequest", "parse"));
    }

    #[test]
    fn test_matches_query_stem() {
        // Stem matching across morphological variants
        assert!(matches_query("authenticate", "auth"));
        assert!(matches_query("authentication", "auth"));
        assert!(matches_query("authorized", "auth"));

        assert!(matches_query("parser", "parse"));
        assert!(matches_query("parsing", "parse"));

        assert!(matches_query("validator", "valid"));
        assert!(matches_query("invalid", "valid"));
    }

    #[test]
    fn test_matches_query_typo() {
        // Edit distance <= 1 for 4+ char words
        assert!(matches_query("parser", "parsr")); // missing 'e'
        assert!(matches_query("authenticate", "authentcate")); // missing 'i'
        // Note: "parsxr" would match "parser" via parts matching ("pars" substring)
        // For a true negative, need a completely different word
        assert!(!matches_query("parser", "xyzabc"));
    }

    #[test]
    fn test_matches_query_negative() {
        assert!(!matches_query("authenticate", "xyz"));
        assert!(!matches_query("getUserName", "setpassword"));
        assert!(!matches_query("parser", "compiler"));
    }

    #[test]
    fn test_resolve_empty() {
        let resolver = FocusResolver::new("/tmp");
        let tags_by_file = HashMap::new();
        let (files, idents) = resolver.resolve(&[], &tags_by_file);
        assert!(files.is_empty());
        assert!(idents.is_empty());
    }

    #[test]
    fn test_resolve_symbols() {
        let resolver = FocusResolver::new("/tmp");

        // Create some mock tags
        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/src/auth.rs".to_string(),
            vec![
                Tag {
                    rel_fname: "src/auth.rs".into(),
                    fname: "/src/auth.rs".into(),
                    line: 10,
                    name: "authenticate".into(),
                    kind: crate::types::TagKind::Def,
                    node_type: "function".into(),
                    parent_name: None,
                    parent_line: None,
                    signature: None,
                    fields: None,
                metadata: None,
                },
                Tag {
                    rel_fname: "src/auth.rs".into(),
                    fname: "/src/auth.rs".into(),
                    line: 20,
                    name: "authorize".into(),
                    kind: crate::types::TagKind::Def,
                    node_type: "function".into(),
                    parent_name: None,
                    parent_line: None,
                    signature: None,
                    fields: None,
                metadata: None,
                },
            ],
        );

        let (files, idents) = resolver.resolve(&["auth".to_string()], &tags_by_file);

        assert!(files.is_empty()); // "auth" is not a file
        assert_eq!(idents.len(), 2); // matches both authenticate and authorize
        assert!(idents.contains("authenticate"));
        assert!(idents.contains("authorize"));
    }

    #[test]
    fn test_resolve_comma_separated() {
        let resolver = FocusResolver::new("/tmp");

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/src/parser.rs".to_string(),
            vec![Tag {
                rel_fname: "src/parser.rs".into(),
                fname: "/src/parser.rs".into(),
                line: 10,
                name: "parse".into(),
                kind: crate::types::TagKind::Def,
                node_type: "function".into(),
                parent_name: None,
                parent_line: None,
                signature: None,
                fields: None,
            metadata: None,
            }],
        );
        tags_by_file.insert(
            "/src/auth.rs".to_string(),
            vec![Tag {
                rel_fname: "src/auth.rs".into(),
                fname: "/src/auth.rs".into(),
                line: 20,
                name: "authenticate".into(),
                kind: crate::types::TagKind::Def,
                node_type: "function".into(),
                parent_name: None,
                parent_line: None,
                signature: None,
                fields: None,
            metadata: None,
            }],
        );

        let (files, idents) = resolver.resolve(&["parse,auth".to_string()], &tags_by_file);

        assert!(files.is_empty());
        assert_eq!(idents.len(), 2);
        assert!(idents.contains("parse"));
        assert!(idents.contains("authenticate"));
    }

    #[test]
    fn test_expand_via_graph_empty() {
        let resolver = FocusResolver::new("/tmp");
        let matched_idents = HashSet::new();
        let symbol_graph = vec![];

        let expanded = resolver.expand_via_graph(&matched_idents, &symbol_graph, 2, 0.5);
        assert!(expanded.is_empty());
    }

    #[test]
    fn test_expand_via_graph_seeds_only() {
        let resolver = FocusResolver::new("/tmp");

        let mut matched_idents = HashSet::new();
        matched_idents.insert("foo".to_string());

        // Graph with foo but no connections
        let symbol_graph = vec![];

        let expanded = resolver.expand_via_graph(&matched_idents, &symbol_graph, 2, 0.5);
        // No seeds found in empty graph
        assert!(expanded.is_empty());
    }

    #[test]
    fn test_expand_via_graph_one_hop() {
        let resolver = FocusResolver::new("/tmp");

        let mut matched_idents = HashSet::new();
        matched_idents.insert("foo".to_string());

        // foo calls bar
        let symbol_graph = vec![(
            Arc::from("a.rs"),
            Arc::from("foo"),
            Arc::from("b.rs"),
            Arc::from("bar"),
        )];

        let expanded = resolver.expand_via_graph(&matched_idents, &symbol_graph, 1, 0.5);

        // Should have foo (weight 1.0) and bar (weight 0.5)
        assert_eq!(expanded.len(), 2);
        assert_eq!(expanded.get(&(Arc::from("a.rs"), Arc::from("foo"))), Some(&1.0));
        assert_eq!(expanded.get(&(Arc::from("b.rs"), Arc::from("bar"))), Some(&0.5));
    }

    #[test]
    fn test_expand_via_graph_two_hops() {
        let resolver = FocusResolver::new("/tmp");

        let mut matched_idents = HashSet::new();
        matched_idents.insert("foo".to_string());

        // foo -> bar -> baz
        let symbol_graph = vec![
            (
                Arc::from("a.rs"),
                Arc::from("foo"),
                Arc::from("b.rs"),
                Arc::from("bar"),
            ),
            (
                Arc::from("b.rs"),
                Arc::from("bar"),
                Arc::from("c.rs"),
                Arc::from("baz"),
            ),
        ];

        let expanded = resolver.expand_via_graph(&matched_idents, &symbol_graph, 2, 0.5);

        // Should have foo (1.0), bar (0.5), baz (0.25)
        assert_eq!(expanded.len(), 3);
        assert_eq!(expanded.get(&(Arc::from("a.rs"), Arc::from("foo"))), Some(&1.0));
        assert_eq!(expanded.get(&(Arc::from("b.rs"), Arc::from("bar"))), Some(&0.5));
        assert_eq!(expanded.get(&(Arc::from("c.rs"), Arc::from("baz"))), Some(&0.25));
    }

    #[test]
    fn test_expand_via_graph_max_hops() {
        let resolver = FocusResolver::new("/tmp");

        let mut matched_idents = HashSet::new();
        matched_idents.insert("foo".to_string());

        // Long chain: foo -> bar -> baz -> qux
        let symbol_graph = vec![
            (
                Arc::from("a.rs"),
                Arc::from("foo"),
                Arc::from("b.rs"),
                Arc::from("bar"),
            ),
            (
                Arc::from("b.rs"),
                Arc::from("bar"),
                Arc::from("c.rs"),
                Arc::from("baz"),
            ),
            (
                Arc::from("c.rs"),
                Arc::from("baz"),
                Arc::from("d.rs"),
                Arc::from("qux"),
            ),
        ];

        // Limit to 1 hop
        let expanded = resolver.expand_via_graph(&matched_idents, &symbol_graph, 1, 0.5);
        assert_eq!(expanded.len(), 2); // foo, bar only
        assert!(!expanded.contains_key(&(Arc::from("c.rs"), Arc::from("baz"))));

        // Limit to 2 hops
        let expanded = resolver.expand_via_graph(&matched_idents, &symbol_graph, 2, 0.5);
        assert_eq!(expanded.len(), 3); // foo, bar, baz
        assert!(!expanded.contains_key(&(Arc::from("d.rs"), Arc::from("qux"))));
    }

    #[test]
    fn test_expand_via_graph_bidirectional() {
        let resolver = FocusResolver::new("/tmp");

        let mut matched_idents = HashSet::new();
        matched_idents.insert("bar".to_string());

        // foo -> bar <- baz (bar is called by both foo and baz)
        let symbol_graph = vec![
            (
                Arc::from("a.rs"),
                Arc::from("foo"),
                Arc::from("b.rs"),
                Arc::from("bar"),
            ),
            (
                Arc::from("c.rs"),
                Arc::from("baz"),
                Arc::from("b.rs"),
                Arc::from("bar"),
            ),
        ];

        let expanded = resolver.expand_via_graph(&matched_idents, &symbol_graph, 1, 0.5);

        // Should expand to both callers
        assert_eq!(expanded.len(), 3); // bar, foo, baz
        assert_eq!(expanded.get(&(Arc::from("b.rs"), Arc::from("bar"))), Some(&1.0));
        assert_eq!(expanded.get(&(Arc::from("a.rs"), Arc::from("foo"))), Some(&0.5));
        assert_eq!(expanded.get(&(Arc::from("c.rs"), Arc::from("baz"))), Some(&0.5));
    }

    #[test]
    fn test_expand_via_graph_custom_decay() {
        let resolver = FocusResolver::new("/tmp");

        let mut matched_idents = HashSet::new();
        matched_idents.insert("foo".to_string());

        let symbol_graph = vec![(
            Arc::from("a.rs"),
            Arc::from("foo"),
            Arc::from("b.rs"),
            Arc::from("bar"),
        )];

        // Test with different decay values
        let expanded = resolver.expand_via_graph(&matched_idents, &symbol_graph, 1, 0.75);
        assert_eq!(expanded.get(&(Arc::from("b.rs"), Arc::from("bar"))), Some(&0.75));

        let expanded = resolver.expand_via_graph(&matched_idents, &symbol_graph, 1, 0.25);
        assert_eq!(expanded.get(&(Arc::from("b.rs"), Arc::from("bar"))), Some(&0.25));
    }

    #[test]
    fn test_resolve_file_with_extension() {
        let resolver = FocusResolver::new("/tmp");

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/src/auth.rs".to_string(),
            vec![Tag {
                rel_fname: "src/auth.rs".into(),
                fname: "/src/auth.rs".into(),
                line: 10,
                name: "authenticate".into(),
                kind: crate::types::TagKind::Def,
                node_type: "function".into(),
                parent_name: None,
                parent_line: None,
                signature: None,
                fields: None,
            metadata: None,
            }],
        );

        // Should match as a file (has .rs extension)
        let (files, idents) = resolver.resolve(&["auth.rs".to_string()], &tags_by_file);
        assert_eq!(files.len(), 1);
        assert!(files.contains("/src/auth.rs"));
        assert!(idents.is_empty());
    }

    #[test]
    fn test_expand_via_graph_cycle() {
        let resolver = FocusResolver::new("/tmp");

        let mut matched_idents = HashSet::new();
        matched_idents.insert("foo".to_string());

        // Cycle: foo -> bar -> foo
        let symbol_graph = vec![
            (
                Arc::from("a.rs"),
                Arc::from("foo"),
                Arc::from("b.rs"),
                Arc::from("bar"),
            ),
            (
                Arc::from("b.rs"),
                Arc::from("bar"),
                Arc::from("a.rs"),
                Arc::from("foo"),
            ),
        ];

        let expanded = resolver.expand_via_graph(&matched_idents, &symbol_graph, 2, 0.5);

        // Should handle cycle gracefully - foo not re-added at lower weight
        assert_eq!(expanded.len(), 2); // foo, bar
        assert_eq!(expanded.get(&(Arc::from("a.rs"), Arc::from("foo"))), Some(&1.0));
        assert_eq!(expanded.get(&(Arc::from("b.rs"), Arc::from("bar"))), Some(&0.5));
    }

    #[test]
    fn test_split_identifier_edge_cases() {
        // Single word
        assert_eq!(split_identifier("foo"), ["foo"].iter().map(|s| s.to_string()).collect());

        // Empty
        assert_eq!(split_identifier(""), HashSet::new());

        // Numbers
        let parts = split_identifier("foo123bar");
        assert!(parts.contains("foo123bar") || parts.len() >= 1);

        // Multiple delimiters
        assert_eq!(
            split_identifier("foo__bar--baz"),
            ["foo", "bar", "baz"].iter().map(|s| s.to_string()).collect()
        );
    }

    #[test]
    fn test_matches_query_case_insensitive() {
        assert!(matches_query("AuthHandler", "authhandler"));
        assert!(matches_query("AUTH_HANDLER", "auth"));
        assert!(matches_query("parseJSON", "parsejson"));
    }

    #[test]
    fn test_expand_via_graph_multiple_seeds() {
        let resolver = FocusResolver::new("/tmp");

        let mut matched_idents = HashSet::new();
        matched_idents.insert("foo".to_string());
        matched_idents.insert("baz".to_string());

        // Two separate chains: foo -> bar, baz -> qux
        let symbol_graph = vec![
            (
                Arc::from("a.rs"),
                Arc::from("foo"),
                Arc::from("b.rs"),
                Arc::from("bar"),
            ),
            (
                Arc::from("c.rs"),
                Arc::from("baz"),
                Arc::from("d.rs"),
                Arc::from("qux"),
            ),
        ];

        let expanded = resolver.expand_via_graph(&matched_idents, &symbol_graph, 1, 0.5);

        // Should expand from both seeds
        assert_eq!(expanded.len(), 4); // foo, bar, baz, qux
        assert_eq!(expanded.get(&(Arc::from("a.rs"), Arc::from("foo"))), Some(&1.0));
        assert_eq!(expanded.get(&(Arc::from("b.rs"), Arc::from("bar"))), Some(&0.5));
        assert_eq!(expanded.get(&(Arc::from("c.rs"), Arc::from("baz"))), Some(&1.0));
        assert_eq!(expanded.get(&(Arc::from("d.rs"), Arc::from("qux"))), Some(&0.5));
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/ranking/git.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Git-based weighting and temporal analysis.
//!
//! This module extracts temporal signals from git history to inform ranking:
//! - **Recency**: Files touched recently are likely more relevant (exponential decay)
//! - **Churn**: High commit frequency indicates volatility or active development (logarithmic)
//! - **Lifecycle phases**: Crystal (stable), Rotting (debt), Emergent (new), Evolving (normal)
//! - **Temporal coupling**: Files that change together often have hidden dependencies
//!
//! ## Design Philosophy
//!
//! Git history reveals the "temperature" of code - where attention flows, what's stable,
//! what's churning. This complements PageRank (structural importance) with temporal
//! patterns that only emerge over time.
//!
//! We use git log parsing instead of libgit2 for:
//! - **Speed**: Spawning git is faster than FFI overhead for large repos
//! - **Simplicity**: No dependency hell, works with any git version
//! - **Flexibility**: Easy to extend with custom git commands
//!
//! ## Weight Formulas
//!
//! **Recency boost** (exponential decay):
//! ```text
//! recency_boost = 1.0 + (MAX_BOOST - 1.0) * exp(-days / DECAY_DAYS)
//! ```
//! - Today: 10x boost
//! - 30 days ago: ~4.3x
//! - 60 days ago: ~1.6x
//! - Asymptotes to 1.0 (no boost) for old files
//!
//! **Churn boost** (logarithmic to dampen outliers):
//! ```text
//! excess = max(0, commit_count - THRESHOLD)
//! churn_boost = 1.0 + ln(1.0 + excess) * (MAX_BOOST - 1.0) / 5.0
//! ```
//! - 5 commits: 1.0x (baseline, at threshold)
//! - 10 commits: ~2.79x (ln(6) â‰ˆ 1.79)
//! - 20 commits: ~3.97x (ln(16) â‰ˆ 2.77)
//! - 50 commits: ~5.29x (near MAX_BOOST of 6.0)
//!
//! **Combined weight**:
//! ```text
//! weight = recency_boost.powf(recency_scale) * churn_boost.powf(churn_scale)
//! ```
//! Intent recipes adjust `recency_scale` and `churn_scale` for different use cases.

use anyhow::{Context, Result};
use std::collections::{HashMap, HashSet};
use std::path::Path;
use std::process::Command;

use crate::types::{FilePhase, RankingConfig};

/// Git-based weight calculation and lifecycle analysis.
///
/// This calculator extracts temporal signals from git history:
/// - Recency: When was this file last touched?
/// - Churn: How frequently does it change?
/// - Age: How long has it been in the repo?
/// - Authors: Who has worked on it?
/// - Coupling: What files change together?
///
/// All operations are batched for performance - we parse git log once
/// for all files rather than invoking git per-file.
pub struct GitWeightCalculator {
    config: RankingConfig,
}

/// Per-file statistics extracted from git history.
///
/// These stats capture both temporal patterns (when) and collaboration
/// patterns (who) that inform file lifecycle classification.
#[derive(Debug, Clone)]
pub struct FileStats {
    /// Days since last modification (0 = modified today)
    pub last_modified_days: u32,
    /// Days since first appearance in repo
    pub first_seen_days: u32,
    /// Total number of commits touching this file
    pub commit_count: usize,
    /// Unique authors who have committed to this file
    pub authors: HashSet<String>,
}

impl GitWeightCalculator {
    /// Create a new calculator with the given configuration.
    pub fn new(config: RankingConfig) -> Self {
        Self { config }
    }

    /// Compute git-based weights for all files in one pass.
    ///
    /// Returns a map of `rel_fname -> weight` where weight combines:
    /// - Recency boost (exponential decay)
    /// - Churn boost (logarithmic)
    ///
    /// Files not found in git history get weight 1.0 (neutral).
    ///
    /// # Performance
    ///
    /// This batches all files into a single `git log` invocation to minimize
    /// process overhead. For large repos with thousands of files, this is
    /// 100-1000x faster than per-file git queries.
    pub fn compute_weights(
        &self,
        root: &Path,
        rel_fnames: &[String],
    ) -> Result<HashMap<String, f64>> {
        let stats = self
            .get_file_stats(root, rel_fnames)
            .context("Failed to get git file stats")?;

        let mut weights = HashMap::with_capacity(rel_fnames.len());

        for fname in rel_fnames {
            let weight = if let Some(file_stats) = stats.get(fname) {
                self.calculate_weight(file_stats)
            } else {
                // File not in git history - neutral weight
                1.0
            };
            weights.insert(fname.clone(), weight);
        }

        Ok(weights)
    }

    /// Extract file statistics from git history.
    ///
    /// Runs a single batched `git log` command that extracts:
    /// - Commit timestamps (for recency and age)
    /// - Author emails (for collaboration patterns)
    /// - File paths (for associating commits with files)
    ///
    /// # Git Command
    ///
    /// ```bash
    /// git log --format=%aI|%ae --name-only --all -n 500 -- [files...]
    /// ```
    ///
    /// Output format:
    /// ```text
    /// 2024-01-15T10:30:00+00:00|author@example.com
    /// src/lib.rs
    /// src/main.rs
    ///
    /// 2024-01-14T09:00:00+00:00|other@example.com
    /// src/lib.rs
    /// ```
    ///
    /// We parse this incrementally to build per-file stats.
    ///
    /// # Limits
    ///
    /// Capped at 500 commits to keep latency low. For most use cases,
    /// recent history (last 500 commits) captures the relevant temporal
    /// signal. For repos with >500 commits, this samples the most recent
    /// activity which is what we care about for ranking.
    pub fn get_file_stats(
        &self,
        root: &Path,
        rel_fnames: &[String],
    ) -> Result<HashMap<String, FileStats>> {
        if rel_fnames.is_empty() {
            return Ok(HashMap::new());
        }

        // Spawn git log with batched file arguments
        let output = Command::new("git")
            .arg("log")
            .arg("--format=%aI|%ae") // ISO timestamp | author email
            .arg("--name-only") // Show modified files after each commit
            .arg("--all") // All branches (not just HEAD)
            .arg("-n")
            .arg("500") // Limit to recent 500 commits for performance
            .arg("--") // Separator before file paths
            .args(rel_fnames)
            .current_dir(root)
            .output()
            .context("Failed to execute git log")?;

        if !output.status.success() {
            // Not a git repo or git not installed - return empty stats
            // This allows ripmap to work gracefully in non-git directories
            return Ok(HashMap::new());
        }

        let log_text = String::from_utf8_lossy(&output.stdout);
        self.parse_git_log(&log_text, rel_fnames)
    }

    /// Parse git log output into per-file statistics.
    ///
    /// The log format alternates between:
    /// 1. Commit metadata: `timestamp|author`
    /// 2. Modified files: one per line
    /// 3. Empty line (commit separator)
    ///
    /// We track:
    /// - First/last commit timestamps per file
    /// - Commit count per file
    /// - Unique authors per file
    fn parse_git_log(
        &self,
        log_text: &str,
        rel_fnames: &[String],
    ) -> Result<HashMap<String, FileStats>> {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .context("System time is before UNIX epoch")?
            .as_secs() as i64;

        // Build a set for fast file lookup
        let fname_set: HashSet<&str> = rel_fnames.iter().map(|s| s.as_str()).collect();

        // Accumulators for per-file data
        let mut first_seen: HashMap<String, i64> = HashMap::new();
        let mut last_modified: HashMap<String, i64> = HashMap::new();
        let mut commit_counts: HashMap<String, usize> = HashMap::new();
        let mut authors_map: HashMap<String, HashSet<String>> = HashMap::new();

        let mut current_timestamp: Option<i64> = None;
        let mut current_author: Option<String> = None;

        for line in log_text.lines() {
            let line = line.trim();

            if line.is_empty() {
                // Commit separator - reset state
                current_timestamp = None;
                current_author = None;
                continue;
            }

            if let Some(pipe_idx) = line.find('|') {
                // Commit header: timestamp|author
                let (timestamp_str, author_str) = line.split_at(pipe_idx);
                let author_str = &author_str[1..]; // Skip the '|'

                // Parse ISO 8601 timestamp to Unix epoch
                if let Ok(timestamp) = parse_iso8601(timestamp_str) {
                    current_timestamp = Some(timestamp);
                    current_author = Some(author_str.to_string());
                }
            } else if let Some(ts) = current_timestamp {
                // This is a file path - check if it's in our target set
                if fname_set.contains(line) {
                    let fname = line.to_string();

                    // Update first seen (minimum timestamp)
                    first_seen
                        .entry(fname.clone())
                        .and_modify(|t| *t = (*t).min(ts))
                        .or_insert(ts);

                    // Update last modified (maximum timestamp)
                    last_modified
                        .entry(fname.clone())
                        .and_modify(|t| *t = (*t).max(ts))
                        .or_insert(ts);

                    // Increment commit count
                    *commit_counts.entry(fname.clone()).or_insert(0) += 1;

                    // Track unique authors
                    if let Some(ref author) = current_author {
                        authors_map
                            .entry(fname.clone())
                            .or_insert_with(HashSet::new)
                            .insert(author.clone());
                    }
                }
            }
        }

        // Convert raw data into FileStats
        let mut stats = HashMap::new();
        for fname in fname_set {
            if let Some(&last_ts) = last_modified.get(fname) {
                let last_modified_days = days_since(now, last_ts);
                let first_ts = first_seen.get(fname).copied().unwrap_or(last_ts);
                let first_seen_days = days_since(now, first_ts);
                let commit_count = commit_counts.get(fname).copied().unwrap_or(0);
                let authors = authors_map.get(fname).cloned().unwrap_or_default();

                stats.insert(
                    fname.to_string(),
                    FileStats {
                        last_modified_days,
                        first_seen_days,
                        commit_count,
                        authors,
                    },
                );
            }
        }

        Ok(stats)
    }

    /// Calculate combined weight from recency and churn.
    ///
    /// This is the core weighting formula that combines two signals:
    /// 1. **Recency**: Exponential decay favoring recently-touched files
    /// 2. **Churn**: Logarithmic boost for frequently-changed files
    ///
    /// The intent recipe controls how much each signal contributes via
    /// exponentiation (allowing non-linear scaling).
    fn calculate_weight(&self, stats: &FileStats) -> f64 {
        let recency_boost = self.recency_boost(stats.last_modified_days);
        let churn_boost = self.churn_boost(stats.commit_count);

        // Combine boosts multiplicatively
        // Intent recipes can scale these via exponentiation in the caller
        recency_boost * churn_boost
    }

    /// Recency boost: exponential decay from last modification.
    ///
    /// Formula: `1.0 + (MAX - 1.0) * exp(-days / DECAY)`
    ///
    /// This gives:
    /// - MAX boost for files touched today
    /// - ~50% of MAX at DECAY_DAYS
    /// - Asymptotic approach to 1.0 (no boost) for old files
    fn recency_boost(&self, days: u32) -> f64 {
        let max_boost = self.config.git_recency_max_boost;
        let decay_days = self.config.git_recency_decay_days;

        1.0 + (max_boost - 1.0) * (-f64::from(days) / decay_days).exp()
    }

    /// Churn boost: logarithmic increase with commit count.
    ///
    /// Formula: `1.0 + ln(1 + excess) * (MAX - 1.0) / 5.0`
    /// where `excess = max(0, commits - THRESHOLD)`
    ///
    /// Logarithmic scaling prevents extreme outliers from dominating.
    /// A file with 1000 commits won't be 200x more important than one
    /// with 5 commits - the boost saturates gracefully.
    fn churn_boost(&self, commit_count: usize) -> f64 {
        let threshold = self.config.git_churn_threshold;
        let max_boost = self.config.git_churn_max_boost;

        let excess = commit_count.saturating_sub(threshold) as f64;
        1.0 + (1.0 + excess).ln() * (max_boost - 1.0) / 5.0
    }

    /// Classify file lifecycle phase based on age and activity.
    ///
    /// **Crystal**: Old and quiet - settled, safe to depend on
    /// - Age >= 180 days, quiet >= 30 days
    ///
    /// **Rotting**: Old but churning - tech debt surfacing
    /// - Age >= 90 days, touched in last 14 days, commits > median * 1.5
    ///
    /// **Emergent**: Brand new - still finding its shape
    /// - Age <= 30 days
    ///
    /// **Evolving**: Normal active development
    /// - Everything else
    ///
    /// This classification helps identify:
    /// - Safe foundation code (Crystal)
    /// - Potential refactoring targets (Rotting)
    /// - Unstable new code (Emergent)
    pub fn classify_phase(&self, stats: &FileStats) -> FilePhase {
        let age = stats.first_seen_days;
        let quiet = stats.last_modified_days;

        // Crystal: old and stable
        if age >= self.config.phase_crystal_min_age_days
            && quiet >= self.config.phase_crystal_min_quiet_days
        {
            return FilePhase::Crystal;
        }

        // Emergent: brand new
        if age <= self.config.phase_emergent_max_age_days {
            return FilePhase::Emergent;
        }

        // Rotting: old but recently churning
        // We'd ideally check if commit_count > median * 1.5, but computing
        // the median requires all stats. For now, use a simple heuristic:
        // old + recently active + high commit count
        if age >= self.config.phase_rotting_min_age_days
            && quiet <= self.config.phase_rotting_max_quiet_days
            && stats.commit_count >= self.config.git_badge_churn_commits
        {
            return FilePhase::Rotting;
        }

        // Default: evolving
        FilePhase::Evolving
    }

    /// Compute badges for files based on temporal signals.
    ///
    /// Badges are lightweight annotations that appear in grepmap output:
    /// - `[recent]`: Modified in last N days
    /// - `[high-churn]`: Commits >= threshold
    /// - `[crystal]`, `[rotting]`, `[emergent]`, `[evolving]`: Lifecycle phase
    ///
    /// Badges help users quickly identify temporal patterns at a glance.
    pub fn get_badges(&self, stats: &HashMap<String, FileStats>) -> HashMap<String, Vec<String>> {
        let mut badges = HashMap::new();

        for (fname, file_stats) in stats {
            let mut file_badges = Vec::new();

            // Recent badge
            if file_stats.last_modified_days <= self.config.git_badge_recent_days {
                file_badges.push("recent".to_string());
            }

            // High-churn badge
            if file_stats.commit_count >= self.config.git_badge_churn_commits {
                file_badges.push("high-churn".to_string());
            }

            // Lifecycle phase badge
            let phase = self.classify_phase(file_stats);
            file_badges.push(phase.badge().to_string());

            badges.insert(fname.clone(), file_badges);
        }

        badges
    }

    /// Compute temporal coupling: files that change together.
    ///
    /// Temporal coupling reveals hidden dependencies not visible in the code:
    /// - Config files that must stay in sync
    /// - Tests that track implementation files
    /// - Related modules that evolve together
    ///
    /// We use **Jaccard similarity** on commit sets:
    /// ```text
    /// coupling(A, B) = |commits_A âˆ© commits_B| / |commits_A âˆª commits_B|
    /// ```
    ///
    /// Returns for each file a list of (other_file, score) pairs sorted by
    /// score descending. Only includes pairs with score >= 0.3 to reduce noise.
    ///
    /// # Implementation
    ///
    /// We re-parse git log to build commit sets per file, then compute
    /// pairwise Jaccard similarity. For N files, this is O(N^2) comparisons,
    /// but N is typically small (files in focus) and Jaccard is very fast
    /// (set intersection/union on small sets).
    pub fn compute_temporal_coupling(
        &self,
        root: &Path,
        rel_fnames: &[String],
    ) -> Result<HashMap<String, Vec<(String, f64)>>> {
        if rel_fnames.is_empty() {
            return Ok(HashMap::new());
        }

        // Run git log to get commit hashes per file
        let output = Command::new("git")
            .arg("log")
            .arg("--format=%H") // Commit hash only
            .arg("--name-only")
            .arg("--all")
            .arg("-n")
            .arg("500")
            .arg("--")
            .args(rel_fnames)
            .current_dir(root)
            .output()
            .context("Failed to execute git log for temporal coupling")?;

        if !output.status.success() {
            return Ok(HashMap::new());
        }

        let log_text = String::from_utf8_lossy(&output.stdout);

        // Parse into file -> set of commit hashes
        let mut file_commits: HashMap<String, HashSet<String>> = HashMap::new();
        let fname_set: HashSet<&str> = rel_fnames.iter().map(|s| s.as_str()).collect();

        let mut current_commit: Option<String> = None;

        for line in log_text.lines() {
            let line = line.trim();

            if line.is_empty() {
                current_commit = None;
                continue;
            }

            // If line is 40 hex chars, it's a commit hash
            if line.len() == 40 && line.chars().all(|c| c.is_ascii_hexdigit()) {
                current_commit = Some(line.to_string());
            } else if let Some(ref commit) = current_commit {
                // This is a file path
                if fname_set.contains(line) {
                    file_commits
                        .entry(line.to_string())
                        .or_insert_with(HashSet::new)
                        .insert(commit.clone());
                }
            }
        }

        // Compute pairwise Jaccard similarity
        let mut coupling: HashMap<String, Vec<(String, f64)>> = HashMap::new();

        for (file_a, commits_a) in &file_commits {
            let mut pairs = Vec::new();

            for (file_b, commits_b) in &file_commits {
                if file_a == file_b {
                    continue;
                }

                let intersection = commits_a.intersection(commits_b).count();
                let union = commits_a.union(commits_b).count();

                if union > 0 {
                    let score = intersection as f64 / union as f64;

                    // Filter low scores to reduce noise
                    if score >= 0.3 {
                        pairs.push((file_b.clone(), score));
                    }
                }
            }

            // Sort by score descending
            pairs.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));

            if !pairs.is_empty() {
                coupling.insert(file_a.clone(), pairs);
            }
        }

        Ok(coupling)
    }
}

/// Parse ISO 8601 timestamp to Unix epoch seconds.
///
/// Handles the format output by `git log --format=%aI`:
/// `2024-01-15T10:30:00+00:00`
///
/// We do a simple manual parse instead of pulling in a datetime crate
/// to keep dependencies lean. This works for git's consistent format.
fn parse_iso8601(s: &str) -> Result<i64> {
    // Format: YYYY-MM-DDTHH:MM:SS+HH:MM
    // Split on 'T'
    let parts: Vec<&str> = s.split('T').collect();
    if parts.len() != 2 {
        anyhow::bail!("Invalid ISO 8601 format");
    }

    let date = parts[0];
    let time_tz = parts[1];

    // Parse date: YYYY-MM-DD
    let date_parts: Vec<&str> = date.split('-').collect();
    if date_parts.len() != 3 {
        anyhow::bail!("Invalid date format");
    }
    let year: i32 = date_parts[0].parse()?;
    let month: i32 = date_parts[1].parse()?;
    let day: i32 = date_parts[2].parse()?;

    // Parse time: HH:MM:SS+TZ or HH:MM:SS-TZ
    let tz_split_pos = time_tz.find('+').or_else(|| time_tz.find('-'));
    let (time, _tz) = if let Some(pos) = tz_split_pos {
        (&time_tz[..pos], &time_tz[pos..])
    } else {
        (time_tz, "")
    };

    let time_parts: Vec<&str> = time.split(':').collect();
    if time_parts.len() != 3 {
        anyhow::bail!("Invalid time format");
    }
    let hour: i32 = time_parts[0].parse()?;
    let minute: i32 = time_parts[1].parse()?;
    let second: i32 = time_parts[2].parse()?;

    // Simplified Unix epoch calculation (ignoring leap years, timezones)
    // This is approximate but sufficient for day-level granularity
    let days_since_epoch = (year - 1970) * 365 + (year - 1969) / 4 // Leap years
        + days_in_months(month - 1)
        + day
        - 1;

    let seconds = i64::from(days_since_epoch) * 86400
        + i64::from(hour) * 3600
        + i64::from(minute) * 60
        + i64::from(second);

    Ok(seconds)
}

/// Days in each month (non-leap year).
fn days_in_months(months: i32) -> i32 {
    const DAYS: [i32; 12] = [31, 28, 31, 30, 31, 30, 31, 31, 30, 31, 30, 31];
    DAYS.iter().take(months as usize).sum()
}

/// Compute days between two Unix timestamps.
fn days_since(now_secs: i64, then_secs: i64) -> u32 {
    let diff = now_secs.saturating_sub(then_secs);
    (diff / 86400) as u32
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_recency_boost() {
        let config = RankingConfig::default();
        let calc = GitWeightCalculator::new(config.clone());

        // Today: should be close to MAX_BOOST
        let boost_today = calc.recency_boost(0);
        assert!(boost_today > config.git_recency_max_boost * 0.99);

        // 30 days: should be around 4-5x
        let boost_30d = calc.recency_boost(30);
        assert!(boost_30d > 3.0 && boost_30d < 6.0);

        // 180 days: should be close to 1.0
        let boost_180d = calc.recency_boost(180);
        assert!(boost_180d < 2.0);
    }

    #[test]
    fn test_churn_boost() {
        let config = RankingConfig::default();
        let calc = GitWeightCalculator::new(config.clone());

        // At threshold: should be 1.0
        let boost_threshold = calc.churn_boost(config.git_churn_threshold);
        assert!((boost_threshold - 1.0).abs() < 0.01);

        // 10 commits: excess=5, ln(6)â‰ˆ1.79, boostâ‰ˆ2.79
        let boost_10 = calc.churn_boost(10);
        assert!(boost_10 > 2.5 && boost_10 < 3.0);

        // 50 commits: should approach MAX_BOOST
        let boost_50 = calc.churn_boost(50);
        assert!(boost_50 > 4.0 && boost_50 < config.git_churn_max_boost);
    }

    #[test]
    fn test_classify_phase() {
        let config = RankingConfig::default();
        let calc = GitWeightCalculator::new(config);

        // Crystal: old and stable
        let crystal = FileStats {
            last_modified_days: 60,
            first_seen_days: 200,
            commit_count: 5,
            authors: HashSet::new(),
        };
        assert_eq!(calc.classify_phase(&crystal), FilePhase::Crystal);

        // Emergent: brand new
        let emergent = FileStats {
            last_modified_days: 1,
            first_seen_days: 10,
            commit_count: 3,
            authors: HashSet::new(),
        };
        assert_eq!(calc.classify_phase(&emergent), FilePhase::Emergent);

        // Rotting: old but churning
        let rotting = FileStats {
            last_modified_days: 2,
            first_seen_days: 100,
            commit_count: 15,
            authors: HashSet::new(),
        };
        assert_eq!(calc.classify_phase(&rotting), FilePhase::Rotting);

        // Evolving: normal
        let evolving = FileStats {
            last_modified_days: 20,
            first_seen_days: 60,
            commit_count: 7,
            authors: HashSet::new(),
        };
        assert_eq!(calc.classify_phase(&evolving), FilePhase::Evolving);
    }

    #[test]
    fn test_parse_iso8601() {
        let timestamp = "2024-01-15T10:30:00+00:00";
        let result = parse_iso8601(timestamp);
        assert!(result.is_ok());

        // Approximate check - should be in 2024 range
        let secs = result.unwrap();
        assert!(secs > 1_700_000_000); // After 2023
        assert!(secs < 1_800_000_000); // Before 2027
    }

    #[test]
    fn test_days_since() {
        let now = 1_700_000_000;
        let then = now - 86400 * 7; // 7 days ago
        assert_eq!(days_since(now, then), 7);

        let same = days_since(now, now);
        assert_eq!(same, 0);
    }

    #[test]
    fn test_get_badges() {
        let config = RankingConfig::default();
        let calc = GitWeightCalculator::new(config);

        let mut stats = HashMap::new();

        // Recent and high-churn file
        stats.insert(
            "hot.rs".to_string(),
            FileStats {
                last_modified_days: 2,
                first_seen_days: 100,
                commit_count: 15,
                authors: HashSet::new(),
            },
        );

        // Old stable file
        stats.insert(
            "stable.rs".to_string(),
            FileStats {
                last_modified_days: 60,
                first_seen_days: 200,
                commit_count: 5,
                authors: HashSet::new(),
            },
        );

        let badges = calc.get_badges(&stats);

        // hot.rs should have [recent], [high-churn], [rotting]
        let hot_badges = &badges["hot.rs"];
        assert!(hot_badges.contains(&"recent".to_string()));
        assert!(hot_badges.contains(&"high-churn".to_string()));
        assert!(hot_badges.contains(&"rotting".to_string()));

        // stable.rs should have [crystal]
        let stable_badges = &badges["stable.rs"];
        assert!(stable_badges.contains(&"crystal".to_string()));
    }

    #[test]
    fn test_parse_git_log() {
        let config = RankingConfig::default();
        let calc = GitWeightCalculator::new(config);

        let log = r#"2024-01-15T10:30:00+00:00|alice@example.com
src/lib.rs
src/main.rs

2024-01-14T09:00:00+00:00|bob@example.com
src/lib.rs

2024-01-10T15:20:00+00:00|alice@example.com
src/main.rs
"#;

        let files = vec!["src/lib.rs".to_string(), "src/main.rs".to_string()];
        let stats = calc.parse_git_log(log, &files).unwrap();

        // src/lib.rs: 2 commits, 2 authors
        let lib_stats = &stats["src/lib.rs"];
        assert_eq!(lib_stats.commit_count, 2);
        assert_eq!(lib_stats.authors.len(), 2);

        // src/main.rs: 2 commits, 1 author
        let main_stats = &stats["src/main.rs"];
        assert_eq!(main_stats.commit_count, 2);
        assert_eq!(main_stats.authors.len(), 1);
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/ranking/intent.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Intent classification stub.

pub struct IntentClassifier;

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/ranking/mod.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Ranking pipeline - from tags to importance scores.
//!
//! The ranking system combines:
//! - PageRank on the symbol/file reference graph
//! - Contextual boosts (chat files, mentions, temporal coupling)
//! - Git-based weighting (recency, churn)
//! - Focus expansion via graph traversal
//! - Intent-driven recipe selection

mod pagerank;
mod symbols;
mod boosts;
mod focus;
mod git;
mod bridges;
mod intent;
mod coupling;

pub use pagerank::PageRanker;
pub use symbols::SymbolRanker;
pub use boosts::BoostCalculator;
pub use focus::FocusResolver;
pub use git::{GitWeightCalculator, FileStats};
pub use bridges::BridgeDetector;
pub use intent::IntentClassifier;
pub use coupling::TestCouplingDetector;

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/ranking/pagerank.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! PageRank algorithm for file importance ranking.
//!
//! This module implements PageRank calculation for ranking files by their
//! interconnectedness within the repository. It builds a directed graph where:
//! - Nodes represent files
//! - Edges represent references (file A references a symbol defined in file B)
//! - Edge weights are based on reference counts
//!
//! The PageRank incorporates depth-aware personalization:
//! - Root/shallow files get higher base weight
//! - Vendor/third-party code is heavily penalized
//! - Chat files receive additional boost
//!
//! This depth-aware approach ensures important root files rank high while still
//! allowing deeply nested files to rank well if they're heavily interconnected.

use std::collections::{HashMap, HashSet};
use std::sync::Arc;

use petgraph::graph::{DiGraph, NodeIndex};
use petgraph::Direction;

use crate::types::{RankingConfig, Tag};

/// PageRank-based file importance calculator.
///
/// Builds a graph of file references and computes importance scores using
/// the PageRank algorithm with depth-aware personalization.
///
/// The graph structure captures how files reference each other:
/// - Definitions create "def" tags in files
/// - References create edges from referencing file to defining file
/// - Multiple references to the same symbol strengthen the edge
///
/// Personalization is depth-aware to bias toward root files while allowing
/// graph structure to override for truly important deep files.
pub struct PageRanker {
    config: RankingConfig,
}

impl PageRanker {
    /// Create a new PageRanker with the given configuration.
    pub fn new(config: RankingConfig) -> Self {
        Self { config }
    }

    /// Compute PageRank scores for all files.
    ///
    /// The algorithm:
    /// 1. Build graph with files as nodes
    /// 2. Add edges based on references (ref file -> def file)
    /// 3. Compute depth-aware personalization weights
    /// 4. Run PageRank with personalization (power iteration)
    /// 5. Return rank scores as dict[rel_fname -> score]
    ///
    /// # Arguments
    /// * `tags_by_file` - Map from absolute file path to its list of tags
    /// * `chat_fnames` - List of chat file absolute paths (for boost)
    ///
    /// # Returns
    /// Map from relative filename to PageRank score (0.0-1.0)
    pub fn compute_ranks(
        &self,
        tags_by_file: &HashMap<String, Vec<Tag>>,
        chat_fnames: &[String],
    ) -> HashMap<String, f64> {
        // Build symbol index: maps symbol names to files that define them
        let defines = self.build_defines_index(tags_by_file);

        // Build the reference graph
        let (graph, node_map, index_map) = self.build_graph(tags_by_file, &defines);

        if graph.node_count() == 0 {
            return HashMap::new();
        }

        // Build depth-aware personalization vector
        let chat_rel_fnames: HashSet<String> = chat_fnames
            .iter()
            .map(|f| self.extract_rel_fname(f))
            .collect();

        let personalization = self.build_personalization(&node_map, &chat_rel_fnames);

        // Run PageRank power iteration
        let ranks = self.pagerank(&graph, &personalization, &index_map);

        // Convert from NodeIndex back to filenames
        let mut result = HashMap::new();
        for (node_idx, rank) in ranks {
            if let Some(rel_fname) = index_map.get(&node_idx) {
                result.insert(rel_fname.clone(), rank);
            }
        }

        result
    }

    /// Build index of symbol definitions: symbol_name -> set of files that define it.
    ///
    /// This enables efficient lookup when building edges: for each reference,
    /// we need to find which file(s) define that symbol.
    fn build_defines_index(
        &self,
        tags_by_file: &HashMap<String, Vec<Tag>>,
    ) -> HashMap<Arc<str>, HashSet<String>> {
        let mut defines: HashMap<Arc<str>, HashSet<String>> = HashMap::new();

        for (fname, tags) in tags_by_file {
            let rel_fname = self.extract_rel_fname(fname);
            for tag in tags {
                if tag.is_def() {
                    defines
                        .entry(Arc::clone(&tag.name))
                        .or_insert_with(HashSet::new)
                        .insert(rel_fname.clone());
                }
            }
        }

        defines
    }

    /// Build the reference graph.
    ///
    /// Returns:
    /// - The petgraph DiGraph
    /// - Map from rel_fname to NodeIndex
    /// - Map from NodeIndex to rel_fname (inverse)
    ///
    /// Graph construction:
    /// - Add all files as nodes
    /// - For each reference tag, add edge from ref_file -> def_file
    /// - Allow multi-edges (multiple refs between same file pair strengthen connection)
    fn build_graph(
        &self,
        tags_by_file: &HashMap<String, Vec<Tag>>,
        defines: &HashMap<Arc<str>, HashSet<String>>,
    ) -> (DiGraph<(), ()>, HashMap<String, NodeIndex>, HashMap<NodeIndex, String>) {
        let mut graph = DiGraph::new();
        let mut node_map: HashMap<String, NodeIndex> = HashMap::new();
        let mut index_map: HashMap<NodeIndex, String> = HashMap::new();

        // Add all files as nodes
        for fname in tags_by_file.keys() {
            let rel_fname = self.extract_rel_fname(fname);
            if !node_map.contains_key(&rel_fname) {
                let idx = graph.add_node(());
                node_map.insert(rel_fname.clone(), idx);
                index_map.insert(idx, rel_fname);
            }
        }

        // Add edges based on references
        // For each reference tag in file A that references symbol S defined in file B:
        // Add edge A -> B
        for (fname, tags) in tags_by_file {
            let ref_fname = self.extract_rel_fname(fname);
            let ref_node = match node_map.get(&ref_fname) {
                Some(n) => *n,
                None => continue,
            };

            for tag in tags {
                if tag.is_ref() {
                    // Find which file(s) define this symbol
                    if let Some(def_fnames) = defines.get(&tag.name) {
                        for def_fname in def_fnames {
                            // Don't create self-loops
                            if def_fname != &ref_fname {
                                if let Some(&def_node) = node_map.get(def_fname) {
                                    // Add edge: referencing file -> defining file
                                    graph.add_edge(ref_node, def_node, ());
                                }
                            }
                        }
                    }
                }
            }
        }

        (graph, node_map, index_map)
    }

    /// Build depth-aware personalization weights for PageRank.
    ///
    /// Personalization biases the random walk toward certain nodes using weights from config:
    /// - Root/shallow files: depth_weight_root (1.0)
    /// - Moderate depth: depth_weight_moderate (0.5)
    /// - Deep files: depth_weight_deep (0.1)
    /// - Vendor/third-party: depth_weight_vendor (0.01)
    /// - Chat files: multiply by pagerank_chat_multiplier (100x)
    ///
    /// Returns a map from rel_fname to personalization weight.
    fn build_personalization(
        &self,
        node_map: &HashMap<String, NodeIndex>,
        chat_fnames: &HashSet<String>,
    ) -> HashMap<NodeIndex, f64> {
        let mut personalization = HashMap::new();

        for (rel_fname, &node_idx) in node_map {
            let weight = self.personalization_weight(rel_fname, chat_fnames);
            personalization.insert(node_idx, weight);
        }

        personalization
    }

    /// Calculate personalization weight for a single file.
    ///
    /// Weight is based on:
    /// - File depth (number of '/' in path)
    /// - Whether it's vendor code
    /// - Whether it's a chat file (current context)
    fn personalization_weight(&self, rel_fname: &str, chat_fnames: &HashSet<String>) -> f64 {
        let depth = rel_fname.matches('/').count();

        // Check if vendor/third-party
        let is_vendor = self
            .config
            .vendor_patterns
            .iter()
            .any(|pattern| rel_fname.contains(pattern.as_str()));

        // Determine base weight by depth and vendor status
        let base_weight = if is_vendor {
            self.config.depth_weight_vendor
        } else if depth <= self.config.depth_threshold_shallow {
            self.config.depth_weight_root
        } else if depth <= self.config.depth_threshold_moderate {
            self.config.depth_weight_moderate
        } else {
            self.config.depth_weight_deep
        };

        // Apply chat file multiplier if applicable
        if chat_fnames.contains(rel_fname) {
            base_weight * self.config.pagerank_chat_multiplier
        } else {
            base_weight
        }
    }

    /// Run PageRank using power iteration.
    ///
    /// PageRank formula with personalization:
    /// ```text
    /// PR(v) = (1-Î±) * personalization[v] + Î± * Î£(PR(u) / out_degree[u])
    ///                                          for all u pointing to v
    /// ```
    ///
    /// Where:
    /// - Î± = damping factor (0.85)
    /// - personalization[v] = normalized depth-aware weight (teleportation distribution)
    ///
    /// The personalization vector determines where random teleportation lands.
    /// High-weight nodes (root files, chat files) get more teleportation probability.
    ///
    /// Iterates until convergence (max change < epsilon) or max iterations reached.
    fn pagerank(
        &self,
        graph: &DiGraph<(), ()>,
        personalization: &HashMap<NodeIndex, f64>,
        _index_map: &HashMap<NodeIndex, String>,
    ) -> HashMap<NodeIndex, f64> {
        let alpha = self.config.pagerank_alpha;
        let epsilon = 1e-8;
        let max_iterations = 100;

        let n = graph.node_count();
        if n == 0 {
            return HashMap::new();
        }

        // Normalize personalization vector to sum to 1.0
        // This represents the probability distribution for random teleportation
        let total_personalization: f64 = personalization.values().sum();
        let normalized_personalization: HashMap<NodeIndex, f64> = personalization
            .iter()
            .map(|(&idx, &weight)| (idx, weight / total_personalization))
            .collect();

        // Initialize ranks uniformly
        let init_rank = 1.0 / n as f64;
        let mut ranks: HashMap<NodeIndex, f64> = graph.node_indices().map(|idx| (idx, init_rank)).collect();
        let mut new_ranks = ranks.clone();

        // Power iteration
        for _iteration in 0..max_iterations {
            // Handle dangling nodes (nodes with no outgoing edges)
            // Their rank needs to be redistributed according to personalization
            let mut dangling_sum = 0.0;
            for node in graph.node_indices() {
                let out_degree = graph.neighbors_directed(node, Direction::Outgoing).count();
                if out_degree == 0 {
                    dangling_sum += ranks[&node];
                }
            }

            for node in graph.node_indices() {
                // Calculate incoming contribution from following edges
                let mut incoming_sum = 0.0;

                // Sum over all incoming edges
                for predecessor in graph.neighbors_directed(node, Direction::Incoming) {
                    let pred_rank = ranks[&predecessor];
                    let out_degree = graph.neighbors_directed(predecessor, Direction::Outgoing).count();

                    if out_degree > 0 {
                        // Each outgoing edge contributes equally (standard PageRank)
                        incoming_sum += pred_rank / out_degree as f64;
                    }
                }

                // Apply PageRank formula with personalization
                // (1-Î±) portion: teleport according to personalization distribution
                // Î± portion: follow edges from predecessors
                // Also handle dangling node mass redistribution
                let personalization_value = normalized_personalization.get(&node).copied().unwrap_or(1.0 / n as f64);
                new_ranks.insert(
                    node,
                    (1.0 - alpha) * personalization_value
                        + alpha * incoming_sum
                        + alpha * dangling_sum * personalization_value,  // Redistribute dangling mass
                );
            }

            // Check convergence
            let max_change = ranks
                .iter()
                .map(|(node, &old_rank)| (new_ranks[node] - old_rank).abs())
                .fold(0.0_f64, f64::max);

            if max_change < epsilon {
                break;
            }

            // Swap for next iteration
            std::mem::swap(&mut ranks, &mut new_ranks);
        }

        ranks
    }

    /// Extract relative filename from absolute path.
    ///
    /// This is a simplified version - in production, would use proper
    /// path resolution relative to repo root.
    fn extract_rel_fname(&self, abs_fname: &str) -> String {
        // Simple heuristic: strip common prefixes
        // In practice, would use proper path canonicalization
        abs_fname
            .strip_prefix("/")
            .unwrap_or(abs_fname)
            .to_string()
    }

    /// Compute PageRank scores on a call graph (function-level ranking).
    ///
    /// This uses the CallGraph's precise call relationships for more accurate
    /// function importance scoring than file-level ranking.
    ///
    /// # Arguments
    /// * `call_graph` - The resolved call graph from CallResolver
    /// * `focus_functions` - Optional set of function names to boost (like chat files)
    ///
    /// # Returns
    /// Map from FunctionId to PageRank score
    pub fn compute_function_ranks(
        &self,
        call_graph: &crate::callgraph::CallGraph,
    ) -> HashMap<crate::callgraph::FunctionId, f64> {
        use petgraph::visit::EdgeRef;

        let inner = call_graph.inner();
        let n = inner.node_count();

        if n == 0 {
            return HashMap::new();
        }

        let alpha = self.config.pagerank_alpha;
        let epsilon = 1e-8;
        let max_iterations = 100;

        // Build personalization based on file depth (functions inherit file depth)
        let mut personalization: HashMap<petgraph::graph::NodeIndex, f64> = HashMap::new();
        for node_idx in inner.node_indices() {
            if let Some(func) = inner.node_weight(node_idx) {
                let weight = self.personalization_weight(func.file.as_ref(), &HashSet::new());
                personalization.insert(node_idx, weight);
            }
        }

        // Normalize personalization
        let total: f64 = personalization.values().sum();
        if total > 0.0 {
            for v in personalization.values_mut() {
                *v /= total;
            }
        }

        // Initialize ranks uniformly
        let init_rank = 1.0 / n as f64;
        let mut ranks: HashMap<petgraph::graph::NodeIndex, f64> =
            inner.node_indices().map(|idx| (idx, init_rank)).collect();
        let mut new_ranks = ranks.clone();

        // Power iteration
        for _iteration in 0..max_iterations {
            // Handle dangling nodes
            let mut dangling_sum = 0.0;
            for node in inner.node_indices() {
                let out_degree = inner.edges(node).count();
                if out_degree == 0 {
                    dangling_sum += ranks[&node];
                }
            }

            for node in inner.node_indices() {
                let mut incoming_sum = 0.0;

                // Sum contributions from callers (incoming edges = functions that call us)
                for edge in inner.edges_directed(node, petgraph::Direction::Incoming) {
                    let caller = edge.source();
                    let caller_rank = ranks[&caller];
                    let out_degree = inner.edges(caller).count();

                    if out_degree > 0 {
                        // Weight by edge confidence for more accurate ranking
                        let confidence = edge.weight().confidence;
                        incoming_sum += (caller_rank * confidence) / out_degree as f64;
                    }
                }

                let p_value = personalization.get(&node).copied().unwrap_or(1.0 / n as f64);
                new_ranks.insert(
                    node,
                    (1.0 - alpha) * p_value
                        + alpha * incoming_sum
                        + alpha * dangling_sum * p_value,
                );
            }

            // Check convergence
            let max_change = ranks
                .iter()
                .map(|(node, &old_rank)| (new_ranks[node] - old_rank).abs())
                .fold(0.0_f64, f64::max);

            if max_change < epsilon {
                break;
            }

            std::mem::swap(&mut ranks, &mut new_ranks);
        }

        // Convert to FunctionId keys
        let mut result = HashMap::new();
        for (node_idx, rank) in ranks {
            if let Some(func_id) = inner.node_weight(node_idx) {
                result.insert(func_id.clone(), rank);
            }
        }

        result
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::types::TagKind;

    fn make_tag(rel_fname: &str, name: &str, kind: TagKind) -> Tag {
        Tag {
            rel_fname: Arc::from(rel_fname),
            fname: Arc::from(format!("/{}", rel_fname)),
            line: 1,
            name: Arc::from(name),
            kind,
            node_type: Arc::from("function"),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
        metadata: None,
        }
    }

    #[test]
    fn test_simple_pagerank() {
        let config = RankingConfig::default();
        let ranker = PageRanker::new(config);

        // Create simple graph:
        // a.rs defines "foo"
        // b.rs references "foo" (b -> a)
        // c.rs references "foo" (c -> a)
        // a.rs should have highest rank (referenced by both)

        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![make_tag("a.rs", "foo", TagKind::Def)],
        );
        tags_by_file.insert(
            "/b.rs".to_string(),
            vec![make_tag("b.rs", "foo", TagKind::Ref)],
        );
        tags_by_file.insert(
            "/c.rs".to_string(),
            vec![make_tag("c.rs", "foo", TagKind::Ref)],
        );

        let chat_fnames = vec![];
        let ranks = ranker.compute_ranks(&tags_by_file, &chat_fnames);

        // a.rs should have highest rank
        assert!(ranks["a.rs"] > ranks["b.rs"]);
        assert!(ranks["a.rs"] > ranks["c.rs"]);
    }

    #[test]
    fn test_depth_aware_personalization() {
        let config = RankingConfig::default();
        let ranker = PageRanker::new(config.clone());

        let chat_fnames = HashSet::new();

        // Root file (0 slashes)
        let weight_root = ranker.personalization_weight("main.rs", &chat_fnames);
        assert_eq!(weight_root, config.depth_weight_root);

        // Shallow file (1 slash)
        let weight_shallow = ranker.personalization_weight("src/lib.rs", &chat_fnames);
        assert_eq!(weight_shallow, config.depth_weight_root);

        // Deep file (5 slashes)
        let weight_deep = ranker.personalization_weight("src/a/b/c/d/e.rs", &chat_fnames);
        assert_eq!(weight_deep, config.depth_weight_deep);

        // Vendor file
        let weight_vendor = ranker.personalization_weight("vendor/lib.rs", &chat_fnames);
        assert_eq!(weight_vendor, config.depth_weight_vendor);
    }

    #[test]
    fn test_chat_file_boost() {
        let config = RankingConfig::default();
        let ranker = PageRanker::new(config.clone());

        let mut chat_fnames = HashSet::new();
        chat_fnames.insert("main.rs".to_string());

        // Chat file should get multiplier
        let weight_chat = ranker.personalization_weight("main.rs", &chat_fnames);
        assert_eq!(
            weight_chat,
            config.depth_weight_root * config.pagerank_chat_multiplier
        );

        // Non-chat file should not
        let weight_normal = ranker.personalization_weight("other.rs", &chat_fnames);
        assert_eq!(weight_normal, config.depth_weight_root);
    }

    #[test]
    fn test_vendor_patterns() {
        let config = RankingConfig::default();
        let ranker = PageRanker::new(config.clone());

        let chat_fnames = HashSet::new();

        // Test various vendor patterns
        assert_eq!(
            ranker.personalization_weight("node_modules/lib.js", &chat_fnames),
            config.depth_weight_vendor
        );
        assert_eq!(
            ranker.personalization_weight("src/vendor/lib.rs", &chat_fnames),
            config.depth_weight_vendor
        );
        assert_eq!(
            ranker.personalization_weight("third_party/lib.c", &chat_fnames),
            config.depth_weight_vendor
        );
    }

    #[test]
    fn test_empty_graph() {
        let config = RankingConfig::default();
        let ranker = PageRanker::new(config);

        let tags_by_file = HashMap::new();
        let chat_fnames = vec![];
        let ranks = ranker.compute_ranks(&tags_by_file, &chat_fnames);

        assert!(ranks.is_empty());
    }

    #[test]
    fn test_pagerank_convergence() {
        let config = RankingConfig::default();
        let ranker = PageRanker::new(config);

        // Create a chain: a -> b -> c
        let mut tags_by_file = HashMap::new();
        tags_by_file.insert(
            "/a.rs".to_string(),
            vec![make_tag("a.rs", "func_b", TagKind::Ref)],
        );
        tags_by_file.insert(
            "/b.rs".to_string(),
            vec![
                make_tag("b.rs", "func_b", TagKind::Def),
                make_tag("b.rs", "func_c", TagKind::Ref),
            ],
        );
        tags_by_file.insert(
            "/c.rs".to_string(),
            vec![make_tag("c.rs", "func_c", TagKind::Def)],
        );

        let chat_fnames = vec![];
        let ranks = ranker.compute_ranks(&tags_by_file, &chat_fnames);

        // All ranks should sum to approximately 1.0 (standard PageRank property)
        // Our implementation follows the standard formula which preserves this invariant
        let total: f64 = ranks.values().sum();
        assert!((total - 1.0).abs() < 0.01, "Total rank should be close to 1.0, got {}", total);

        // c should have highest rank (pointed to by b)
        // b should have second (pointed to by a)
        // a should have lowest (points but not pointed to)
        assert!(ranks["c.rs"] >= ranks["b.rs"], "c.rs rank {} should be >= b.rs rank {}", ranks["c.rs"], ranks["b.rs"]);
        assert!(ranks["b.rs"] >= ranks["a.rs"], "b.rs rank {} should be >= a.rs rank {}", ranks["b.rs"], ranks["a.rs"]);
    }

    #[test]
    fn test_function_level_pagerank() {
        use crate::callgraph::{CallGraph, CallEdge, FunctionId};

        let config = RankingConfig::default();
        let ranker = PageRanker::new(config);

        // Build a simple call graph:
        // main() -> helper() -> util()
        // main() -> util()
        // util() should rank highest (called by both)
        let mut graph = CallGraph::new();

        let main = FunctionId::new("test.rs", "main", 1);
        let helper = FunctionId::new("test.rs", "helper", 10);
        let util = FunctionId::new("test.rs", "util", 20);

        graph.add_call(
            main.clone(),
            helper.clone(),
            CallEdge::new(0.9, "same_file", 5),
        );
        graph.add_call(
            main.clone(),
            util.clone(),
            CallEdge::new(0.9, "same_file", 6),
        );
        graph.add_call(
            helper.clone(),
            util.clone(),
            CallEdge::new(0.9, "same_file", 15),
        );

        let ranks = ranker.compute_function_ranks(&graph);

        // util should have highest rank (most called)
        assert!(
            ranks[&util] >= ranks[&helper],
            "util rank {} should be >= helper rank {}",
            ranks[&util], ranks[&helper]
        );
        assert!(
            ranks[&util] >= ranks[&main],
            "util rank {} should be >= main rank {}",
            ranks[&util], ranks[&main]
        );
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/ranking/symbols.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Symbol ranking stub.

pub struct SymbolRanker;

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/rendering/colors.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! ANSI color utilities and badge rendering for ripmap.
//!
//! Badge system reveals structural and temporal patterns:
//! - Structural: [bridge], [api] - what role it plays in the graph
//! - Temporal: [recent], [high-churn] - what's happening now
//! - Lifecycle: [crystal], [rotting], [emergent], [evolving] - where it is in its arc
//!
//! Color scheme optimized for both light and dark terminals:
//! - High contrast for critical info (file headers, class names)
//! - Muted colors for metadata (badges, decorators)
//! - Semantic colors for syntax (functions=green, types=cyan)

use owo_colors::{OwoColorize, Style};
use std::fmt;

/// Badge types for file and symbol annotations.
/// Each badge reveals different information about structure and evolution.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum Badge {
    // Structural badges - role in the graph
    /// Load-bearing component - high betweenness centrality
    /// Removing this disconnects the graph
    Bridge,

    /// Public API surface - called from many places
    /// Entry points and facades
    Api,

    // Temporal badges - recent activity
    /// Modified in the last 7 days (configurable)
    Recent,

    /// High commit frequency (10+ commits, configurable)
    HighChurn,

    // Lifecycle phase badges - maturity arc
    /// Old (180+ days) and stable (30+ days quiet)
    /// Settled code, safe to rely on
    Crystal,

    /// Old (90+ days) but recently churning
    /// Tech debt surfacing, refactor candidate
    Rotting,

    /// New file (< 30 days)
    /// Still finding its shape
    Emergent,

    /// Normal development state
    /// Actively being worked on
    Evolving,
}

impl Badge {
    /// Get the badge label for display
    pub fn label(&self) -> &'static str {
        match self {
            Badge::Bridge => "bridge",
            Badge::Api => "api",
            Badge::Recent => "recent",
            Badge::HighChurn => "high-churn",
            Badge::Crystal => "crystal",
            Badge::Rotting => "rotting",
            Badge::Emergent => "emergent",
            Badge::Evolving => "evolving",
        }
    }

    /// Get the badge's display color/style
    pub fn style(&self) -> Style {
        match self {
            // Structural badges - bright to catch attention
            Badge::Bridge => Style::new().bright_red().bold(),
            Badge::Api => Style::new().bright_blue().bold(),

            // Temporal badges - yellow/orange tones
            Badge::Recent => Style::new().yellow(),
            Badge::HighChurn => Style::new().bright_yellow(),

            // Lifecycle badges - muted, informational
            Badge::Crystal => Style::new().bright_cyan().dimmed(),
            Badge::Rotting => Style::new().bright_magenta(),
            Badge::Emergent => Style::new().green(),
            Badge::Evolving => Style::new().dimmed(),
        }
    }

    /// Render the badge with color
    pub fn render(&self) -> String {
        format!("[{}]", self.label().style(self.style()))
    }
}

impl fmt::Display for Badge {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "{}", self.render())
    }
}

/// Colorize different symbol types for directory rendering.
/// Provides semantic highlighting similar to LSP-based editors.
pub struct Colorizer;

impl Colorizer {
    /// Colorize a file path (bold blue for headers)
    pub fn file_path(s: &str) -> String {
        s.bright_blue().bold().to_string()
    }

    /// Colorize a class/struct name (magenta)
    pub fn class_name(s: &str) -> String {
        s.magenta().to_string()
    }

    /// Colorize a function/method name (green)
    pub fn function_name(s: &str) -> String {
        s.green().to_string()
    }

    /// Colorize a constant name (bright cyan)
    pub fn constant_name(s: &str) -> String {
        s.bright_cyan().to_string()
    }

    /// Colorize a type annotation (cyan)
    pub fn type_name(s: &str) -> String {
        s.cyan().to_string()
    }

    /// Colorize field names (normal white/default)
    pub fn field_name(s: &str) -> String {
        s.to_string()
    }

    /// Colorize decorators/attributes (dimmed yellow)
    pub fn decorator(s: &str) -> String {
        s.yellow().dimmed().to_string()
    }

    /// Colorize badges (dim yellow for low visual weight)
    pub fn badge_group(badges: &[Badge]) -> String {
        if badges.is_empty() {
            return String::new();
        }

        let rendered: Vec<_> = badges.iter().map(|b| b.render()).collect();
        rendered.join(" ")
    }

    /// Colorize temporal coupling info (e.g., "â‡„ changes with:")
    pub fn coupling_label() -> String {
        "â‡„ changes with:".dimmed().to_string()
    }

    /// Colorize a coupling entry (file name + percentage)
    pub fn coupling_entry(file: &str, percentage: f64) -> String {
        format!("{}({}%)", file.cyan(), (percentage * 100.0) as u32)
    }

    /// Dim text for secondary information (call relationships, metadata)
    pub fn dim(s: &str) -> String {
        s.dimmed().to_string()
    }
}

/// Convenience function for colorizing based on node type.
/// Maps AST node types to appropriate color schemes.
pub fn colorize(node_type: &str, name: &str) -> String {
    match node_type {
        "class" | "class_definition" | "struct" | "struct_item" => {
            Colorizer::class_name(name)
        }
        "function" | "function_definition" | "method" | "method_definition" => {
            Colorizer::function_name(name)
        }
        "constant" | "const_item" => {
            Colorizer::constant_name(name)
        }
        "type" | "type_alias" | "type_definition" => {
            Colorizer::type_name(name)
        }
        "decorator" | "attribute" => {
            Colorizer::decorator(name)
        }
        _ => name.to_string(),
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_badge_labels() {
        assert_eq!(Badge::Bridge.label(), "bridge");
        assert_eq!(Badge::Api.label(), "api");
        assert_eq!(Badge::Recent.label(), "recent");
        assert_eq!(Badge::HighChurn.label(), "high-churn");
        assert_eq!(Badge::Crystal.label(), "crystal");
        assert_eq!(Badge::Rotting.label(), "rotting");
        assert_eq!(Badge::Emergent.label(), "emergent");
        assert_eq!(Badge::Evolving.label(), "evolving");
    }

    #[test]
    fn test_badge_render() {
        // Just ensure they don't panic
        let badges = vec![
            Badge::Bridge,
            Badge::Api,
            Badge::Recent,
            Badge::HighChurn,
            Badge::Crystal,
            Badge::Rotting,
            Badge::Emergent,
            Badge::Evolving,
        ];

        for badge in badges {
            let rendered = badge.render();
            assert!(rendered.contains(badge.label()));
        }
    }

    #[test]
    fn test_colorize_node_types() {
        // These should apply different colors (we can't test actual ANSI codes easily)
        // but we verify the function doesn't panic
        colorize("class", "MyClass");
        colorize("function", "my_func");
        colorize("constant", "MAX_SIZE");
        colorize("type", "UserId");
        colorize("unknown", "something");
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/rendering/directory.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Directory-style hierarchical rendering for ripmap.
//!
//! Renders a structured symbol map organized by file, then by symbol type:
//!
//! ```text
//! grepmap/facade.py: [bridge] [emergent] [recent] [high-churn]
//!   â‡„ changes with: grepmap/core/config.py(32%), grepmap/rendering/directory.py(30%)
//!   class GrepMap [api]:
//!     def: get_rel_fname(self, fname: str) -> str [api]
//!          _build_file_graph_from_symbol_graph(self, symbol_graph)
//!          get_ranked_tags(self, focus_targets: List[str], ...) -> Tuple[...]
//!     const: DEFAULT_MAP_TOKENS
//! ```
//!
//! Key design decisions:
//! - Group by file first (natural mental model for navigating codebases)
//! - Separate classes â†’ methods â†’ functions â†’ constants (structure hierarchy)
//! - Badge visibility shows file/symbol role at a glance
//! - Temporal coupling shows implicit dependencies
//! - Token counting enables binary search for budget fitting

use crate::types::{DetailLevel, RankedTag, Tag};
use crate::callgraph::{CallGraph, FunctionId};
use super::colors::{Badge, Colorizer};
use std::collections::HashMap;
use std::sync::Arc;

/// Directory-style renderer - hierarchical symbol overview with badges.
///
/// Optimized for:
/// - Quick orientation ("what's in this codebase?")
/// - Understanding file roles (badges reveal structure)
/// - Spotting temporal patterns (coupling, churn)
/// - Fitting within LLM token budgets
pub struct DirectoryRenderer {
    /// Token counter for budget-aware rendering.
    /// Different LLMs use different tokenizers (cl100k_base, o200k_base, etc).
    token_counter: Box<dyn Fn(&str) -> usize + Send + Sync>,
}

impl DirectoryRenderer {
    /// Create a new directory renderer with a token counting function.
    ///
    /// The token counter is used for binary search to fit within budgets.
    /// Pass a closure that uses tiktoken-rs or your tokenizer of choice.
    pub fn new(token_counter: Box<dyn Fn(&str) -> usize + Send + Sync>) -> Self {
        Self { token_counter }
    }

    /// Create with a simple character-based token estimator (1 token â‰ˆ 4 chars).
    /// Fast but less accurate than tiktoken.
    pub fn with_char_estimator() -> Self {
        Self::new(Box::new(|s: &str| (s.len() + 3) / 4))
    }

    /// Render ranked tags as a hierarchical directory-style map.
    ///
    /// # Arguments
    /// - `tags`: Ranked symbols to render (should be pre-sorted by rank)
    /// - `detail`: Level of detail for signatures/types
    /// - `badges`: File/symbol badges (structural, temporal, lifecycle)
    /// - `temporal_mates`: Files that change together (file -> [(other_file, coupling_score)])
    ///
    /// # Returns
    /// Formatted string with ANSI colors, ready for terminal output.
    pub fn render(
        &self,
        tags: &[RankedTag],
        detail: DetailLevel,
        badges: &HashMap<String, Vec<Badge>>,
        temporal_mates: &HashMap<String, Vec<(String, f64)>>,
    ) -> String {
        // Group tags by file
        let grouped = self.group_by_file(tags);

        let mut output = String::new();

        for (file_path, file_tags) in grouped {
            // Render file header with badges
            output.push_str(&self.render_file_header(&file_path, badges));
            output.push('\n');

            // Render temporal coupling if present
            if let Some(mates) = temporal_mates.get(file_path.as_ref()) {
                if !mates.is_empty() {
                    output.push_str(&self.render_temporal_coupling(mates));
                    output.push('\n');
                }
            }

            // Organize symbols by type (classes, functions, constants)
            let organized = self.organize_symbols(&file_tags);

            // Render classes with their methods
            for (class_name, class_tag, methods, fields) in organized.classes {
                output.push_str(&self.render_class(
                    &class_name,
                    &class_tag,
                    &methods,
                    &fields,
                    detail,
                    badges,
                ));
            }

            // Render standalone functions
            if !organized.functions.is_empty() {
                output.push_str(&self.render_functions(&organized.functions, detail, badges));
            }

            // Render constants
            if !organized.constants.is_empty() {
                output.push_str(&self.render_constants(&organized.constants, badges));
            }

            output.push('\n'); // Blank line between files
        }

        output
    }

    /// Estimate token count of rendered output.
    /// Uses the provided token counter for accurate estimation.
    pub fn estimate_tokens(&self, output: &str) -> usize {
        (self.token_counter)(output)
    }

    /// Render with call graph information showing calls/called-by for functions.
    ///
    /// This enhanced render shows:
    /// - `â†’ calls: foo(), bar()` for functions this function calls
    /// - `â† called by: main(), helper()` for callers of this function
    ///
    /// # Arguments
    /// - `tags`: Ranked symbols to render
    /// - `detail`: Level of detail for signatures/types
    /// - `badges`: File/symbol badges
    /// - `temporal_mates`: Files that change together
    /// - `call_graph`: Optional call graph for showing call relationships
    pub fn render_with_calls(
        &self,
        tags: &[RankedTag],
        detail: DetailLevel,
        badges: &HashMap<String, Vec<Badge>>,
        temporal_mates: &HashMap<String, Vec<(String, f64)>>,
        call_graph: Option<&CallGraph>,
    ) -> String {
        // Group tags by file
        let grouped = self.group_by_file(tags);

        let mut output = String::new();

        for (file_path, file_tags) in grouped {
            // Render file header with badges
            output.push_str(&self.render_file_header(&file_path, badges));
            output.push('\n');

            // Render temporal coupling if present
            if let Some(mates) = temporal_mates.get(file_path.as_ref()) {
                if !mates.is_empty() {
                    output.push_str(&self.render_temporal_coupling(mates));
                    output.push('\n');
                }
            }

            // Organize symbols by type (classes, functions, constants)
            let organized = self.organize_symbols(&file_tags);

            // Render classes with their methods
            for (class_name, class_tag, methods, fields) in &organized.classes {
                output.push_str(&self.render_class_with_calls(
                    class_name,
                    class_tag,
                    methods,
                    fields,
                    detail,
                    badges,
                    call_graph,
                ));
            }

            // Render standalone functions with call info
            if !organized.functions.is_empty() {
                output.push_str(&self.render_functions_with_calls(
                    &organized.functions,
                    detail,
                    badges,
                    call_graph,
                ));
            }

            // Render constants
            if !organized.constants.is_empty() {
                output.push_str(&self.render_constants(&organized.constants, badges));
            }

            output.push('\n'); // Blank line between files
        }

        output
    }

    /// Render functions with call graph information.
    fn render_functions_with_calls(
        &self,
        functions: &[&RankedTag],
        detail: DetailLevel,
        badges: &HashMap<String, Vec<Badge>>,
        call_graph: Option<&CallGraph>,
    ) -> String {
        let mut output = String::from("    def:\n");

        for f in functions {
            // Function name and signature
            let mut line = String::from("      ");
            line.push_str(&Colorizer::function_name(&f.tag.name));

            if let Some(sig) = &f.tag.signature {
                line.push_str(&sig.render(detail));
            } else if detail >= DetailLevel::Medium {
                line.push_str("(...)");
            }

            // Add symbol-level badges if present
            let badge_key = format!("{}::{}", f.tag.rel_fname, f.tag.name);
            if let Some(symbol_badges) = badges.get(&badge_key) {
                if !symbol_badges.is_empty() {
                    line.push(' ');
                    line.push_str(&Colorizer::badge_group(symbol_badges));
                }
            }

            output.push_str(&line);
            output.push('\n');

            // Add call relationships if call graph is available
            if let Some(graph) = call_graph {
                let func_id = FunctionId::new(
                    f.tag.rel_fname.clone(),
                    f.tag.name.clone(),
                    f.tag.line,
                );

                // Show what this function calls
                let calls = graph.calls_from(&func_id);
                if !calls.is_empty() {
                    let call_names: Vec<_> = calls
                        .iter()
                        .take(5)
                        .map(|(target, edge)| {
                            let conf_str = if edge.confidence < 1.0 {
                                format!("({}%)", (edge.confidence * 100.0) as u32)
                            } else {
                                String::new()
                            };
                            format!("{}{}", target.qualified_name(), conf_str)
                        })
                        .collect();
                    output.push_str(&format!(
                        "        {} {}\n",
                        Colorizer::dim("â†’ calls:"),
                        call_names.join(", ")
                    ));
                }

                // Show what calls this function
                let callers = graph.calls_to(&func_id);
                if !callers.is_empty() {
                    let caller_names: Vec<_> = callers
                        .iter()
                        .take(5)
                        .map(|(source, _)| source.qualified_name())
                        .collect();
                    output.push_str(&format!(
                        "        {} {}\n",
                        Colorizer::dim("â† called by:"),
                        caller_names.join(", ")
                    ));
                }
            }
        }

        output
    }

    /// Render a class with call graph info for methods.
    fn render_class_with_calls(
        &self,
        class_name: &str,
        class_tag: &Option<&RankedTag>,
        methods: &[&RankedTag],
        fields: &[&RankedTag],
        detail: DetailLevel,
        badges: &HashMap<String, Vec<Badge>>,
        call_graph: Option<&CallGraph>,
    ) -> String {
        let mut output = String::from("    ");

        // Class header
        output.push_str("class ");
        output.push_str(&Colorizer::class_name(class_name));

        // Class-level badges
        let badge_key = format!(
            "{}::{}",
            class_tag.map(|t| t.tag.rel_fname.as_ref()).unwrap_or(""),
            class_name
        );
        if let Some(class_badges) = badges.get(&badge_key) {
            if !class_badges.is_empty() {
                output.push(' ');
                output.push_str(&Colorizer::badge_group(class_badges));
            }
        }

        output.push_str(":\n");

        // Render fields
        if !fields.is_empty() && detail >= DetailLevel::Medium {
            output.push_str("      fields: ");
            let field_names: Vec<_> = fields
                .iter()
                .map(|f| {
                    if let Some(field_info) = f.tag.fields.as_ref().and_then(|fs| fs.first()) {
                        field_info.render(detail)
                    } else {
                        f.tag.name.to_string()
                    }
                })
                .collect();
            output.push_str(&field_names.join(", "));
            output.push('\n');
        }

        // Render methods with call info
        if !methods.is_empty() {
            output.push_str("      methods:\n");
            for m in methods {
                let method_name = self.render_method_inline(&m.tag, detail);
                output.push_str(&format!("        {}\n", method_name));

                // Add call relationships for methods
                if let Some(graph) = call_graph {
                    let func_id = FunctionId::new(
                        m.tag.rel_fname.clone(),
                        m.tag.name.clone(),
                        m.tag.line,
                    ).with_parent(class_name);

                    // Show what this method calls
                    let calls = graph.calls_from(&func_id);
                    if !calls.is_empty() {
                        let call_names: Vec<_> = calls
                            .iter()
                            .take(3)
                            .map(|(target, _)| target.qualified_name())
                            .collect();
                        output.push_str(&format!(
                            "          {} {}\n",
                            Colorizer::dim("â†’"),
                            call_names.join(", ")
                        ));
                    }
                }
            }
        }

        output
    }

    // ============ Internal helpers ============

    /// Group tags by file path, preserving rank order within each file.
    fn group_by_file<'a>(&self, tags: &'a [RankedTag]) -> Vec<(Arc<str>, Vec<&'a RankedTag>)> {
        let mut grouped: HashMap<Arc<str>, Vec<&'a RankedTag>> = HashMap::new();

        for tag in tags {
            grouped
                .entry(tag.tag.rel_fname.clone())
                .or_default()
                .push(tag);
        }

        // Sort files by highest rank in each file (most important files first)
        let mut files: Vec<_> = grouped.into_iter().collect();
        files.sort_by(|a, b| {
            let max_a = a.1.iter().map(|t| t.rank).fold(0.0_f64, f64::max);
            let max_b = b.1.iter().map(|t| t.rank).fold(0.0_f64, f64::max);
            max_b.partial_cmp(&max_a).unwrap_or(std::cmp::Ordering::Equal)
        });

        files
    }

    /// Render file header with path and badges.
    fn render_file_header(&self, file_path: &str, badges: &HashMap<String, Vec<Badge>>) -> String {
        let mut header = format!("  {}", Colorizer::file_path(file_path));

        // Add file-level badges
        if let Some(file_badges) = badges.get(file_path) {
            if !file_badges.is_empty() {
                header.push_str(": ");
                header.push_str(&Colorizer::badge_group(file_badges));
            }
        }

        header
    }

    /// Render temporal coupling information (files that change together).
    fn render_temporal_coupling(&self, mates: &[(String, f64)]) -> String {
        let mut output = String::from("    ");
        output.push_str(&Colorizer::coupling_label());
        output.push(' ');

        let entries: Vec<_> = mates
            .iter()
            .take(3) // Show top 3 most coupled files
            .map(|(file, score)| Colorizer::coupling_entry(file, *score))
            .collect();

        output.push_str(&entries.join(", "));
        output
    }

    /// Organize symbols into classes, functions, constants.
    fn organize_symbols<'a>(&self, tags: &[&'a RankedTag]) -> OrganizedSymbols<'a> {
        let mut classes: HashMap<Arc<str>, (Option<&'a RankedTag>, Vec<&'a RankedTag>, Vec<&'a RankedTag>)> = HashMap::new();
        let mut functions = Vec::new();
        let mut constants = Vec::new();

        for ranked_tag in tags {
            let tag = &ranked_tag.tag;

            // Only process definitions
            if !tag.is_def() {
                continue;
            }

            match tag.node_type.as_ref() {
                "class" | "class_definition" | "struct" | "struct_item" => {
                    // Class definition
                    classes.entry(tag.name.clone()).or_default().0 = Some(*ranked_tag);
                }
                "method" | "method_definition" => {
                    // Method inside a class
                    if let Some(parent) = &tag.parent_name {
                        classes.entry(parent.clone()).or_default().1.push(*ranked_tag);
                    } else {
                        // Orphan method, treat as function
                        functions.push(*ranked_tag);
                    }
                }
                "field" | "field_definition" => {
                    // Field inside a class
                    if let Some(parent) = &tag.parent_name {
                        classes.entry(parent.clone()).or_default().2.push(*ranked_tag);
                    }
                }
                "function" | "function_definition" => {
                    // Standalone function
                    functions.push(*ranked_tag);
                }
                "constant" | "const_item" | "variable" => {
                    // Check if it looks like a constant (uppercase naming convention)
                    if tag.name.chars().all(|c| c.is_uppercase() || c == '_' || c.is_numeric()) {
                        constants.push(*ranked_tag);
                    } else {
                        // Regular variable, might be a function-level thing
                        // Skip or treat as function depending on context
                    }
                }
                _ => {
                    // Unknown node type, skip for now
                }
            }
        }

        // Convert HashMap to sorted Vec for deterministic output
        let mut class_list = Vec::new();
        for (class_name, (class_tag, methods, fields)) in classes {
            class_list.push((class_name, class_tag, methods, fields));
        }
        // Sort classes by name
        class_list.sort_by(|a, b| a.0.cmp(&b.0));

        OrganizedSymbols {
            classes: class_list,
            functions,
            constants,
        }
    }

    /// Render a class with its methods and fields.
    fn render_class(
        &self,
        class_name: &str,
        class_tag: &Option<&RankedTag>,
        methods: &[&RankedTag],
        fields: &[&RankedTag],
        detail: DetailLevel,
        badges: &HashMap<String, Vec<Badge>>,
    ) -> String {
        let mut output = String::from("    ");

        // Class header
        output.push_str("class ");
        output.push_str(&Colorizer::class_name(class_name));

        // Class-level badges (e.g., [api])
        let badge_key = format!("{}::{}",
            class_tag.map(|t| t.tag.rel_fname.as_ref()).unwrap_or(""),
            class_name
        );
        if let Some(class_badges) = badges.get(&badge_key) {
            if !class_badges.is_empty() {
                output.push(' ');
                output.push_str(&Colorizer::badge_group(class_badges));
            }
        }

        output.push_str(":\n");

        // Render fields if detail level is high enough
        if !fields.is_empty() && detail >= DetailLevel::Medium {
            output.push_str("      fields: ");
            let field_names: Vec<_> = fields
                .iter()
                .map(|f| {
                    if let Some(field_info) = f.tag.fields.as_ref().and_then(|fs| fs.first()) {
                        field_info.render(detail)
                    } else {
                        f.tag.name.to_string()
                    }
                })
                .collect();
            output.push_str(&field_names.join(", "));
            output.push('\n');
        }

        // Render methods
        if !methods.is_empty() {
            output.push_str("      def: ");
            let method_strs: Vec<_> = methods
                .iter()
                .map(|m| self.render_method_inline(&m.tag, detail))
                .collect();
            output.push_str(&method_strs.join("\n           "));
            output.push('\n');
        }

        output
    }

    /// Render a method inline (one line per method).
    fn render_method_inline(&self, tag: &Tag, detail: DetailLevel) -> String {
        let mut output = Colorizer::function_name(&tag.name);

        // Add signature if available
        if let Some(sig) = &tag.signature {
            output.push_str(&sig.render(detail));
        } else if detail >= DetailLevel::Medium {
            output.push_str("(...)");
        }

        output
    }

    /// Render standalone functions.
    fn render_functions(
        &self,
        functions: &[&RankedTag],
        detail: DetailLevel,
        badges: &HashMap<String, Vec<Badge>>,
    ) -> String {
        let mut output = String::from("    def: ");

        let func_strs: Vec<_> = functions
            .iter()
            .map(|f| {
                let mut s = Colorizer::function_name(&f.tag.name);
                if let Some(sig) = &f.tag.signature {
                    s.push_str(&sig.render(detail));
                } else if detail >= DetailLevel::Medium {
                    s.push_str("(...)");
                }

                // Add symbol-level badges if present
                let badge_key = format!("{}::{}", f.tag.rel_fname, f.tag.name);
                if let Some(symbol_badges) = badges.get(&badge_key) {
                    if !symbol_badges.is_empty() {
                        s.push(' ');
                        s.push_str(&Colorizer::badge_group(symbol_badges));
                    }
                }

                s
            })
            .collect();

        output.push_str(&func_strs.join("\n         "));
        output.push('\n');
        output
    }

    /// Render constants.
    fn render_constants(&self, constants: &[&RankedTag], badges: &HashMap<String, Vec<Badge>>) -> String {
        let mut output = String::from("    const: ");

        let const_strs: Vec<_> = constants
            .iter()
            .map(|c| {
                let mut s = Colorizer::constant_name(&c.tag.name);

                // Add symbol-level badges if present
                let badge_key = format!("{}::{}", c.tag.rel_fname, c.tag.name);
                if let Some(symbol_badges) = badges.get(&badge_key) {
                    if !symbol_badges.is_empty() {
                        s.push(' ');
                        s.push_str(&Colorizer::badge_group(symbol_badges));
                    }
                }

                s
            })
            .collect();

        output.push_str(&const_strs.join(", "));
        output.push('\n');
        output
    }
}

/// Organized symbols by type for rendering.
struct OrganizedSymbols<'a> {
    /// Classes with (name, class_tag, methods, fields)
    classes: Vec<(Arc<str>, Option<&'a RankedTag>, Vec<&'a RankedTag>, Vec<&'a RankedTag>)>,
    /// Standalone functions
    functions: Vec<&'a RankedTag>,
    /// Constants
    constants: Vec<&'a RankedTag>,
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::types::{SignatureInfo, TagKind};
    use std::sync::Arc;

    fn make_tag(
        file: &str,
        name: &str,
        node_type: &str,
        parent: Option<&str>,
        signature: Option<SignatureInfo>,
    ) -> Tag {
        Tag {
            rel_fname: file.into(),
            fname: format!("/{}", file).into(),
            line: 1,
            name: name.into(),
            kind: TagKind::Def,
            node_type: node_type.into(),
            parent_name: parent.map(|p| p.into()),
            parent_line: None,
            signature,
            fields: None,
        metadata: None,
        }
    }

    #[test]
    fn test_group_by_file() {
        let renderer = DirectoryRenderer::with_char_estimator();

        let tags = vec![
            RankedTag::new(0.9, make_tag("a.rs", "foo", "function", None, None)),
            RankedTag::new(0.8, make_tag("b.rs", "bar", "function", None, None)),
            RankedTag::new(0.7, make_tag("a.rs", "baz", "function", None, None)),
        ];

        let grouped = renderer.group_by_file(&tags);

        // Should have 2 files
        assert_eq!(grouped.len(), 2);

        // a.rs should come first (higher max rank)
        assert_eq!(grouped[0].0.as_ref(), "a.rs");
        assert_eq!(grouped[0].1.len(), 2);
    }

    #[test]
    fn test_organize_symbols() {
        let renderer = DirectoryRenderer::with_char_estimator();

        let tags = vec![
            RankedTag::new(0.9, make_tag("test.rs", "MyClass", "class", None, None)),
            RankedTag::new(0.8, make_tag("test.rs", "method1", "method", Some("MyClass"), None)),
            RankedTag::new(0.7, make_tag("test.rs", "standalone", "function", None, None)),
            RankedTag::new(0.6, make_tag("test.rs", "MAX_SIZE", "constant", None, None)),
        ];

        let tag_refs: Vec<_> = tags.iter().collect();
        let organized = renderer.organize_symbols(&tag_refs);

        assert_eq!(organized.classes.len(), 1);
        assert_eq!(organized.functions.len(), 1);
        assert_eq!(organized.constants.len(), 1);

        // Check class has method
        assert_eq!(organized.classes[0].2.len(), 1);
    }

    #[test]
    fn test_render_empty() {
        let renderer = DirectoryRenderer::with_char_estimator();
        let badges = HashMap::new();
        let temporal = HashMap::new();

        let output = renderer.render(&[], DetailLevel::Medium, &badges, &temporal);

        // Should produce empty or minimal output
        assert!(output.trim().is_empty());
    }

    #[test]
    fn test_estimate_tokens() {
        let renderer = DirectoryRenderer::with_char_estimator();
        let text = "Hello world!"; // 12 chars â†’ ~3 tokens

        let tokens = renderer.estimate_tokens(text);

        assert!(tokens > 0);
        assert!(tokens <= 12); // Should be reasonable
    }

    #[test]
    fn test_render_with_signature() {
        let renderer = DirectoryRenderer::with_char_estimator();

        let sig = SignatureInfo {
            parameters: vec![("x".into(), Some("int".into()))],
            return_type: Some("bool".into()),
            decorators: vec![],
            raw: None,
        };

        let tags = vec![
            RankedTag::new(0.9, make_tag("test.py", "check", "function", None, Some(sig))),
        ];

        let badges = HashMap::new();
        let temporal = HashMap::new();

        let output = renderer.render(&tags, DetailLevel::High, &badges, &temporal);

        // Should contain function name
        assert!(output.contains("check"));
        // Should contain signature elements at high detail
        assert!(output.contains("int"));
        assert!(output.contains("bool"));
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/rendering/mod.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Output rendering - from ranked tags to terminal/text output.
//!
//! Supports multiple rendering modes:
//! - Directory mode: hierarchical symbol overview with badges
//! - Tree mode: code snippets with syntax highlighting
//!
//! Optimizes for token budget via binary search.

mod directory;
mod tree;
mod colors;

pub use directory::DirectoryRenderer;
pub use tree::TreeRenderer;
pub use colors::{colorize, Badge};

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/rendering/tree.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Tree rendering stub.

pub struct TreeRenderer;

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/training/git_oracle.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Git history as ground truth oracle for ranking evaluation.
//!
//! ## The Retrocausal Principle
//!
//! Every git commit encodes a developer's cognitive state: "these files were
//! connected in my mind when I made this change." We exploit this as training
//! signal - given one file from a commit as "focus", the other files SHOULD
//! rank highly in our output.
//!
//! ## Commit Quality Hierarchy
//!
//! Not all commits provide equal signal quality:
//!
//! | Commit Type       | Signal Quality | Reasoning                              |
//! |-------------------|----------------|----------------------------------------|
//! | Bugfix (2-6)      | GOLD (1.5x)    | Causal: these files caused/fixed issue |
//! | Feature (3-8)     | Strong (1.2x)  | Semantic: implementation unit          |
//! | Test+Impl         | Moderate (1.0x)| Mechanical but validates coupling      |
//! | Refactor (10+)    | Weak (0.4x)    | Often mechanical rename/move           |
//! | WIP/checkpoint    | Skip (0.0x)    | Incomplete thought, noise              |
//! | Formatting        | Skip (0.0x)    | No semantic content                    |
//!
//! ## Coupling Strength
//!
//! Files that change together frequently have higher coupling weight.
//! We use a Jaccard-like metric:
//!
//! ```text
//! coupling(A,B) = cochange_count(A,B) / (changes(A) + changes(B) - cochange_count(A,B))
//! ```
//!
//! This normalizes for file activity - a pair that changes together 5/10 times
//! is more coupled than a pair that changes together 5/100 times.
//!
//! ## Session Clustering
//!
//! Commits within a short time window (default 30min) are likely part of the
//! same cognitive task. We can cluster these into "sessions" for stronger
//! signal - files touched across commits in a session are still related.

use std::collections::{HashMap, HashSet};
use std::path::Path;
use std::process::Command;

use serde::{Deserialize, Serialize};

use crate::types::Intent;

/// A single training case extracted from git history.
///
/// Each case represents: "given `seed_file` as focus, `expected_related`
/// files should rank highly because they were changed together."
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct GitCase {
    /// The commit this case was derived from
    pub commit_sha: String,

    /// Commit timestamp (Unix epoch seconds)
    pub timestamp: i64,

    /// The file we treat as "focus" / query seed
    pub seed_file: String,

    /// Other files from the commit that should rank high
    pub expected_related: Vec<String>,

    /// Original commit message
    pub message: String,

    /// Inferred intent from commit message parsing
    pub inferred_intent: Intent,

    /// Quality weight for this commit (0.0-2.0)
    /// Based on commit type, file count, message clarity
    pub quality_weight: f64,

    /// Number of files in original commit
    pub commit_file_count: usize,
}

/// A case with coupling-weighted expected files.
///
/// Instead of binary "related or not", each expected file has a weight
/// based on historical co-change frequency. Files that ALWAYS change
/// with the seed have weight ~1.0, occasional co-changes have lower weight.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct WeightedCase {
    pub seed_file: String,
    pub commit_sha: String,
    pub inferred_intent: Intent,
    /// (file, coupling_weight) - higher = more historically coupled
    pub expected_related: Vec<(String, f64)>,
    /// Overall case quality (commit quality Ã— coupling strength)
    pub case_weight: f64,
}

/// Raw commit data from git log
#[derive(Debug, Clone)]
struct RawCommit {
    sha: String,
    timestamp: i64,
    message: String,
    files: Vec<String>,
}

/// Extract training cases from a git repository.
///
/// # Algorithm
///
/// 1. Walk git log in reverse chronological order
/// 2. Filter commits by quality (skip WIP, formatting, huge refactors)
/// 3. For each qualifying commit, generate N cases (one per file as seed)
/// 4. Weight cases by commit quality signals
///
/// # Arguments
///
/// * `repo` - Path to git repository root
/// * `max_commits` - Maximum commits to process (for speed)
/// * `min_files` - Minimum files per commit (default 2)
/// * `max_files` - Maximum files per commit (default 15)
///
/// # Returns
///
/// Vector of GitCase, each representing one (seed, expected) pair
pub fn extract_cases(
    repo: &Path,
    max_commits: usize,
    min_files: usize,
    max_files: usize,
) -> Vec<GitCase> {
    let commits = parse_git_log(repo, max_commits);
    let mut cases = Vec::new();

    for commit in commits {
        // Filter by file count
        if commit.files.len() < min_files || commit.files.len() > max_files {
            continue;
        }

        // Compute commit quality weight
        let quality = compute_commit_quality(&commit);

        // Skip low-quality commits entirely
        if quality < 0.1 {
            continue;
        }

        let intent = parse_intent(&commit.message);

        // Generate one case per file (each takes turn as seed)
        for (i, seed) in commit.files.iter().enumerate() {
            // Skip non-source files as seeds
            if !is_source_file(seed) {
                continue;
            }

            let expected: Vec<_> = commit.files
                .iter()
                .enumerate()
                .filter(|(j, f)| *j != i && is_source_file(f))
                .map(|(_, f)| f.clone())
                .collect();

            // Need at least one expected file
            if expected.is_empty() {
                continue;
            }

            cases.push(GitCase {
                commit_sha: commit.sha.clone(),
                timestamp: commit.timestamp,
                seed_file: seed.clone(),
                expected_related: expected,
                message: commit.message.clone(),
                inferred_intent: intent,
                quality_weight: quality,
                commit_file_count: commit.files.len(),
            });
        }
    }

    cases
}

/// Compute file coupling strengths from co-change history.
///
/// Uses Jaccard-like metric normalized by file activity:
/// ```text
/// coupling(A,B) = cochange(A,B) / (count(A) + count(B) - cochange(A,B))
/// ```
///
/// # Arguments
///
/// * `repo` - Path to git repository
/// * `max_commits` - How far back to look
///
/// # Returns
///
/// HashMap from (file_a, file_b) -> coupling strength (0.0-1.0)
/// Keys are normalized so file_a < file_b lexicographically.
pub fn compute_coupling_weights(
    repo: &Path,
    max_commits: usize,
) -> HashMap<(String, String), f64> {
    let commits = parse_git_log(repo, max_commits);

    let mut cochange_counts: HashMap<(String, String), usize> = HashMap::new();
    let mut file_counts: HashMap<String, usize> = HashMap::new();

    for commit in commits {
        // Skip huge commits (refactors add noise to coupling)
        if commit.files.len() > 20 {
            continue;
        }

        let source_files: Vec<_> = commit.files
            .iter()
            .filter(|f| is_source_file(f))
            .cloned()
            .collect();

        // Count individual file appearances
        for f in &source_files {
            *file_counts.entry(f.clone()).or_default() += 1;
        }

        // Count co-changes (all pairs)
        for i in 0..source_files.len() {
            for j in (i + 1)..source_files.len() {
                let pair = normalize_pair(&source_files[i], &source_files[j]);
                *cochange_counts.entry(pair).or_default() += 1;
            }
        }
    }

    // Convert to Jaccard-like coupling strength
    cochange_counts
        .into_iter()
        .filter_map(|((a, b), co)| {
            let ca = file_counts.get(&a)?;
            let cb = file_counts.get(&b)?;
            let union = ca + cb - co;
            if union == 0 {
                return None;
            }
            let jaccard = co as f64 / union as f64;
            Some(((a, b), jaccard))
        })
        .collect()
}

/// Enhance cases with coupling weights.
///
/// Transforms GitCase (binary related) into WeightedCase (graded relevance)
/// by looking up historical coupling strength for each expected file.
pub fn weight_cases(
    cases: Vec<GitCase>,
    coupling: &HashMap<(String, String), f64>,
) -> Vec<WeightedCase> {
    cases
        .into_iter()
        .map(|case| {
            let weighted_expected: Vec<_> = case.expected_related
                .iter()
                .map(|f| {
                    let pair = normalize_pair(&case.seed_file, f);
                    let weight = coupling.get(&pair).copied().unwrap_or(0.1);
                    (f.clone(), weight)
                })
                .collect();

            // Case weight = commit quality Ã— average coupling
            let avg_coupling: f64 = if weighted_expected.is_empty() {
                0.0
            } else {
                weighted_expected.iter().map(|(_, w)| w).sum::<f64>()
                    / weighted_expected.len() as f64
            };

            WeightedCase {
                seed_file: case.seed_file,
                commit_sha: case.commit_sha,
                inferred_intent: case.inferred_intent,
                expected_related: weighted_expected,
                case_weight: case.quality_weight * (0.5 + avg_coupling),
            }
        })
        .collect()
}

/// Cluster commits into sessions by time proximity.
///
/// Commits within `session_gap` seconds of each other are grouped.
/// This captures multi-commit tasks where files across commits are related.
pub fn cluster_into_sessions(
    cases: &[GitCase],
    session_gap_secs: i64,
) -> Vec<Vec<&GitCase>> {
    if cases.is_empty() {
        return vec![];
    }

    // Sort by timestamp
    let mut sorted: Vec<_> = cases.iter().collect();
    sorted.sort_by_key(|c| c.timestamp);

    let mut sessions = Vec::new();
    let mut current_session = vec![sorted[0]];

    for case in sorted.into_iter().skip(1) {
        let last_ts = current_session.last().unwrap().timestamp;
        if case.timestamp - last_ts <= session_gap_secs {
            current_session.push(case);
        } else {
            if !current_session.is_empty() {
                sessions.push(current_session);
            }
            current_session = vec![case];
        }
    }

    if !current_session.is_empty() {
        sessions.push(current_session);
    }

    sessions
}

// === Private implementation ===

/// Parse git log output into structured commits.
fn parse_git_log(repo: &Path, max_commits: usize) -> Vec<RawCommit> {
    // Format: SHA|timestamp|message
    // Followed by list of files
    let output = Command::new("git")
        .current_dir(repo)
        .args([
            "log",
            &format!("-{}", max_commits),
            "--pretty=format:%H|%ct|%s",
            "--name-only",
        ])
        .output();

    let output = match output {
        Ok(o) => o,
        Err(e) => {
            eprintln!("Failed to run git log: {}", e);
            return vec![];
        }
    };

    if !output.status.success() {
        eprintln!("git log failed: {}", String::from_utf8_lossy(&output.stderr));
        return vec![];
    }

    let stdout = String::from_utf8_lossy(&output.stdout);
    let mut commits = Vec::new();
    let mut current_commit: Option<RawCommit> = None;

    for line in stdout.lines() {
        if line.is_empty() {
            continue;
        }

        // Check if this is a commit header line (SHA|timestamp|message)
        if line.contains('|') && line.len() >= 40 {
            let parts: Vec<_> = line.splitn(3, '|').collect();
            if parts.len() == 3 && parts[0].len() == 40 {
                // Save previous commit
                if let Some(commit) = current_commit.take() {
                    if !commit.files.is_empty() {
                        commits.push(commit);
                    }
                }

                // Parse new commit
                let sha = parts[0].to_string();
                let timestamp = parts[1].parse().unwrap_or(0);
                let message = parts[2].to_string();

                current_commit = Some(RawCommit {
                    sha,
                    timestamp,
                    message,
                    files: Vec::new(),
                });
                continue;
            }
        }

        // Otherwise it's a file path
        if let Some(ref mut commit) = current_commit {
            commit.files.push(line.to_string());
        }
    }

    // Don't forget the last commit
    if let Some(commit) = current_commit {
        if !commit.files.is_empty() {
            commits.push(commit);
        }
    }

    commits
}

/// Compute quality weight for a commit (0.0 - 2.0).
///
/// High quality signals:
/// - Bugfix commit message (1.5x)
/// - Feature/implement message (1.2x)
/// - Cross-directory changes (1.2x)
/// - Moderate file count (2-8 optimal)
///
/// Low quality signals:
/// - WIP/checkpoint message (0.2x)
/// - Formatting/lint commits (0.0x)
/// - Huge refactors (0.4x)
/// - Merge commits (0.3x)
fn compute_commit_quality(commit: &RawCommit) -> f64 {
    let mut weight: f64 = 1.0;
    let msg = commit.message.to_lowercase();

    // === Message-based signals ===

    // Bugfixes are gold - clear causal relationship
    if msg.starts_with("fix")
        || msg.contains("bugfix")
        || msg.contains("hotfix")
        || msg.starts_with("bug:")
    {
        weight *= 1.5;
    }
    // Features are strong signal
    else if msg.starts_with("feat")
        || msg.contains("implement")
        || msg.starts_with("add ")
        || msg.starts_with("add:")
    {
        weight *= 1.2;
    }
    // Refactors are weaker (often mechanical)
    else if msg.starts_with("refactor")
        || msg.contains("rename")
        || msg.contains("move ")
    {
        weight *= 0.6;
    }

    // WIP/checkpoint = incomplete thought = noise
    if msg.contains("wip")
        || msg.contains("work in progress")
        || msg.contains("save")
        || msg.contains("checkpoint")
        || msg.contains("tmp")
    {
        weight *= 0.2;
    }

    // Formatting/lint = no semantic content
    if msg.contains("format")
        || msg.contains("lint")
        || msg.contains("prettier")
        || msg.contains("style:")
        || msg.contains("chore: format")
    {
        return 0.0; // Skip entirely
    }

    // Merge commits are often noisy
    if msg.starts_with("merge") {
        weight *= 0.3;
    }

    // === File count signals ===

    let n = commit.files.len();
    if n <= 1 {
        return 0.0; // No relational signal
    } else if n <= 6 {
        weight *= 1.2; // Sweet spot
    } else if n <= 10 {
        weight *= 1.0; // Normal
    } else if n <= 15 {
        weight *= 0.7; // Getting noisy
    } else {
        weight *= 0.4; // Probably a refactor
    }

    // === Directory diversity ===
    // Files in different directories = meaningful cross-cutting change
    let unique_dirs: HashSet<_> = commit.files
        .iter()
        .filter_map(|f| Path::new(f).parent())
        .filter_map(|p| p.to_str())
        .collect();

    if unique_dirs.len() >= 3 {
        weight *= 1.2; // Cross-cutting changes are meaningful
    }

    // === Source vs config ratio ===
    // Pure test additions or pure config changes are less interesting
    let source_count = commit.files.iter().filter(|f| is_source_file(f)).count();
    let source_ratio = source_count as f64 / n as f64;

    if source_ratio < 0.3 {
        weight *= 0.5; // Mostly non-source
    }

    weight.min(2.0) // Cap at 2.0
}

/// Parse commit message to infer developer intent.
fn parse_intent(message: &str) -> Intent {
    let msg = message.to_lowercase();

    if msg.contains("fix")
        || msg.contains("bug")
        || msg.contains("issue")
        || msg.contains("error")
        || msg.contains("crash")
        || msg.contains("debug")
    {
        Intent::Debug
    } else if msg.contains("refactor")
        || msg.contains("clean")
        || msg.contains("rename")
        || msg.contains("reorganize")
        || msg.contains("restructure")
    {
        Intent::Refactor
    } else if msg.contains("add")
        || msg.contains("implement")
        || msg.contains("feature")
        || msg.contains("new")
        || msg.contains("create")
        || msg.contains("support")
    {
        Intent::Extend
    } else {
        Intent::Explore
    }
}

/// Check if a file is a source code file (not config, docs, etc.)
fn is_source_file(path: &str) -> bool {
    let source_extensions = [
        // Rust
        ".rs",
        // Python
        ".py",
        // JavaScript/TypeScript
        ".js", ".ts", ".jsx", ".tsx", ".mjs", ".cjs",
        // Go
        ".go",
        // C/C++
        ".c", ".h", ".cpp", ".hpp", ".cc", ".hh",
        // Java/Kotlin
        ".java", ".kt", ".kts",
        // Ruby
        ".rb",
        // PHP
        ".php",
        // Swift
        ".swift",
        // C#
        ".cs",
        // Scala
        ".scala",
        // Elixir
        ".ex", ".exs",
        // Haskell
        ".hs",
        // OCaml
        ".ml", ".mli",
        // Zig
        ".zig",
    ];

    // Check extension
    let has_source_ext = source_extensions.iter().any(|ext| path.ends_with(ext));
    if !has_source_ext {
        return false;
    }

    // Exclude tests (optional - they do provide coupling signal)
    // For now, include tests as they validate coupling

    // Exclude vendor/deps
    let vendor_patterns = [
        "node_modules/",
        "vendor/",
        "third_party/",
        "__pycache__/",
        "target/",
        ".git/",
        "dist/",
        "build/",
    ];

    !vendor_patterns.iter().any(|p| path.contains(p))
}

/// Normalize a file pair for consistent map keys.
/// Returns (smaller, larger) lexicographically.
fn normalize_pair(a: &str, b: &str) -> (String, String) {
    if a < b {
        (a.to_string(), b.to_string())
    } else {
        (b.to_string(), a.to_string())
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_is_source_file() {
        assert!(is_source_file("src/main.rs"));
        assert!(is_source_file("lib/parser.py"));
        assert!(is_source_file("components/Button.tsx"));
        assert!(!is_source_file("README.md"));
        assert!(!is_source_file("package.json"));
        assert!(!is_source_file("node_modules/lodash/index.js"));
    }

    #[test]
    fn test_normalize_pair() {
        assert_eq!(
            normalize_pair("b.rs", "a.rs"),
            ("a.rs".to_string(), "b.rs".to_string())
        );
        assert_eq!(
            normalize_pair("a.rs", "b.rs"),
            ("a.rs".to_string(), "b.rs".to_string())
        );
    }

    #[test]
    fn test_parse_intent() {
        assert_eq!(parse_intent("fix: null pointer in parser"), Intent::Debug);
        assert_eq!(parse_intent("refactor: clean up auth module"), Intent::Refactor);
        assert_eq!(parse_intent("add: new user registration"), Intent::Extend);
        assert_eq!(parse_intent("update docs"), Intent::Explore);
    }

    #[test]
    fn test_commit_quality_bugfix() {
        let commit = RawCommit {
            sha: "abc123".repeat(7),
            timestamp: 0,
            message: "fix: crash on empty input".to_string(),
            files: vec!["src/parser.rs".to_string(), "src/input.rs".to_string()],
        };
        let quality = compute_commit_quality(&commit);
        assert!(quality > 1.0, "Bugfix should have high quality: {}", quality);
    }

    #[test]
    fn test_commit_quality_wip() {
        let commit = RawCommit {
            sha: "abc123".repeat(7),
            timestamp: 0,
            message: "WIP: still working on this".to_string(),
            files: vec!["src/a.rs".to_string(), "src/b.rs".to_string()],
        };
        let quality = compute_commit_quality(&commit);
        assert!(quality < 0.5, "WIP should have low quality: {}", quality);
    }

    #[test]
    fn test_commit_quality_formatting() {
        let commit = RawCommit {
            sha: "abc123".repeat(7),
            timestamp: 0,
            message: "chore: format code with prettier".to_string(),
            files: vec!["src/a.rs".to_string(), "src/b.rs".to_string()],
        };
        let quality = compute_commit_quality(&commit);
        assert_eq!(quality, 0.0, "Formatting should be skipped");
    }

    #[test]
    fn test_commit_quality_large_refactor() {
        let commit = RawCommit {
            sha: "abc123".repeat(7),
            timestamp: 0,
            message: "refactor: rename foo to bar everywhere".to_string(),
            files: (0..20).map(|i| format!("src/file{}.rs", i)).collect(),
        };
        let quality = compute_commit_quality(&commit);
        assert!(quality < 0.5, "Large refactor should have low quality: {}", quality);
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/training/gridsearch.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Parameter space exploration for hyperparameter optimization.
//!
//! ## Search Strategies
//!
//! | Strategy         | When to use                                          |
//! |------------------|------------------------------------------------------|
//! | Grid             | Small parameter space (<1000 combinations)           |
//! | LatinHypercube   | Medium space, want uniform coverage                  |
//! | Random           | Large space, baseline comparison                     |
//! | Bayesian         | Expensive evaluations, want smart sampling           |
//!
//! ## Latin Hypercube Sampling
//!
//! LHS ensures uniform coverage of each dimension independently while
//! maintaining good space-filling properties. For N samples in D dimensions,
//! each dimension is divided into N equal strata, and exactly one sample
//! is placed in each stratum per dimension.
//!
//! ## Bayesian Optimization
//!
//! Uses a Gaussian Process surrogate model to predict metric values for
//! unsampled points, then samples where Expected Improvement is highest.
//! Good for expensive evaluations where we want to minimize samples needed.
//!
//! ## Parameter Encoding
//!
//! All parameters are normalized to [0, 1] internally, then decoded to
//! their actual ranges for evaluation. This simplifies search algorithms.

use std::collections::HashMap;

use rand::prelude::*;
use serde::{Deserialize, Serialize};

use crate::types::RankingConfig;

/// A single point in parameter space.
///
/// All parameters are f64 to enable continuous optimization.
/// Integer parameters (like max_hops) are rounded at decode time.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ParameterPoint {
    // === PageRank ===
    /// Damping factor (typically 0.70-0.95)
    pub pagerank_alpha: f64,
    /// Chat file multiplier (typically 10-200)
    pub pagerank_chat_multiplier: f64,

    // === Depth weights ===
    /// Weight for root/shallow files (typically 0.5-2.0)
    pub depth_weight_root: f64,
    /// Weight for moderate depth (typically 0.2-1.0)
    pub depth_weight_moderate: f64,
    /// Weight for deep files (typically 0.05-0.5)
    pub depth_weight_deep: f64,
    /// Weight for vendor code (typically 0.001-0.1)
    pub depth_weight_vendor: f64,

    // === Boosts ===
    /// Mentioned identifier boost (typically 2-50)
    pub boost_mentioned_ident: f64,
    /// Mentioned file boost (typically 2-20)
    pub boost_mentioned_file: f64,
    /// Chat file boost (typically 5-100)
    pub boost_chat_file: f64,
    /// Temporal coupling boost (typically 1-10)
    pub boost_temporal_coupling: f64,
    /// Focus expansion boost (typically 1-20)
    pub boost_focus_expansion: f64,

    // === Git ===
    /// Recency decay half-life in days (typically 7-90)
    pub git_recency_decay_days: f64,
    /// Max recency boost (typically 2-20)
    pub git_recency_max_boost: f64,
    /// Churn threshold for "high churn" (typically 3-15)
    pub git_churn_threshold: f64,
    /// Max churn boost (typically 2-15)
    pub git_churn_max_boost: f64,

    // === Focus expansion ===
    /// Graph traversal decay per hop (typically 0.2-0.8)
    pub focus_decay: f64,
    /// Max hops for expansion (typically 1-3)
    pub focus_max_hops: f64,
}

impl ParameterPoint {
    /// Convert to a RankingConfig for actual evaluation.
    pub fn to_ranking_config(&self) -> RankingConfig {
        let mut config = RankingConfig::default();

        config.pagerank_alpha = self.pagerank_alpha;
        config.pagerank_chat_multiplier = self.pagerank_chat_multiplier;

        config.depth_weight_root = self.depth_weight_root;
        config.depth_weight_moderate = self.depth_weight_moderate;
        config.depth_weight_deep = self.depth_weight_deep;
        config.depth_weight_vendor = self.depth_weight_vendor;

        config.boost_mentioned_ident = self.boost_mentioned_ident;
        config.boost_mentioned_file = self.boost_mentioned_file;
        config.boost_chat_file = self.boost_chat_file;
        config.boost_temporal_coupling = self.boost_temporal_coupling;
        config.boost_focus_expansion = self.boost_focus_expansion;

        config.git_recency_decay_days = self.git_recency_decay_days;
        config.git_recency_max_boost = self.git_recency_max_boost;
        config.git_churn_threshold = self.git_churn_threshold.round() as usize;
        config.git_churn_max_boost = self.git_churn_max_boost;

        config
    }

    /// Get focus expansion parameters (not in RankingConfig).
    pub fn focus_params(&self) -> (f64, usize) {
        (self.focus_decay, self.focus_max_hops.round() as usize)
    }
}

impl Default for ParameterPoint {
    fn default() -> Self {
        let config = RankingConfig::default();
        Self {
            pagerank_alpha: config.pagerank_alpha,
            pagerank_chat_multiplier: config.pagerank_chat_multiplier,
            depth_weight_root: config.depth_weight_root,
            depth_weight_moderate: config.depth_weight_moderate,
            depth_weight_deep: config.depth_weight_deep,
            depth_weight_vendor: config.depth_weight_vendor,
            boost_mentioned_ident: config.boost_mentioned_ident,
            boost_mentioned_file: config.boost_mentioned_file,
            boost_chat_file: config.boost_chat_file,
            boost_temporal_coupling: config.boost_temporal_coupling,
            boost_focus_expansion: config.boost_focus_expansion,
            git_recency_decay_days: config.git_recency_decay_days,
            git_recency_max_boost: config.git_recency_max_boost,
            git_churn_threshold: config.git_churn_threshold as f64,
            git_churn_max_boost: config.git_churn_max_boost,
            focus_decay: 0.5,
            focus_max_hops: 2.0,
        }
    }
}

/// Range specification for a parameter.
#[derive(Debug, Clone)]
pub struct ParamRange {
    pub min: f64,
    pub max: f64,
    /// If true, sample in log space (good for multipliers/boosts)
    pub log_scale: bool,
}

impl ParamRange {
    pub fn linear(min: f64, max: f64) -> Self {
        Self { min, max, log_scale: false }
    }

    pub fn log(min: f64, max: f64) -> Self {
        Self { min, max, log_scale: true }
    }

    /// Convert normalized [0, 1] value to actual parameter value.
    pub fn decode(&self, normalized: f64) -> f64 {
        let t = normalized.clamp(0.0, 1.0);
        if self.log_scale {
            // Log-linear interpolation
            let log_min = self.min.ln();
            let log_max = self.max.ln();
            (log_min + t * (log_max - log_min)).exp()
        } else {
            self.min + t * (self.max - self.min)
        }
    }

    /// Convert actual value to normalized [0, 1].
    pub fn encode(&self, value: f64) -> f64 {
        if self.log_scale {
            let log_min = self.min.ln();
            let log_max = self.max.ln();
            let log_val = value.clamp(self.min, self.max).ln();
            (log_val - log_min) / (log_max - log_min)
        } else {
            (value - self.min) / (self.max - self.min)
        }
    }
}

/// Full parameter space definition with ranges for each parameter.
#[derive(Debug, Clone)]
pub struct ParameterGrid {
    pub ranges: HashMap<String, ParamRange>,
}

impl Default for ParameterGrid {
    fn default() -> Self {
        let mut ranges = HashMap::new();

        // PageRank
        ranges.insert("pagerank_alpha".into(), ParamRange::linear(0.70, 0.95));
        ranges.insert("pagerank_chat_multiplier".into(), ParamRange::log(10.0, 200.0));

        // Depth weights (log scale - multiplicative effect)
        ranges.insert("depth_weight_root".into(), ParamRange::linear(0.5, 2.0));
        ranges.insert("depth_weight_moderate".into(), ParamRange::linear(0.2, 1.0));
        ranges.insert("depth_weight_deep".into(), ParamRange::linear(0.05, 0.5));
        ranges.insert("depth_weight_vendor".into(), ParamRange::log(0.001, 0.1));

        // Boosts (log scale)
        ranges.insert("boost_mentioned_ident".into(), ParamRange::log(2.0, 50.0));
        ranges.insert("boost_mentioned_file".into(), ParamRange::log(2.0, 20.0));
        ranges.insert("boost_chat_file".into(), ParamRange::log(5.0, 100.0));
        ranges.insert("boost_temporal_coupling".into(), ParamRange::log(1.0, 10.0));
        ranges.insert("boost_focus_expansion".into(), ParamRange::log(1.0, 20.0));

        // Git
        ranges.insert("git_recency_decay_days".into(), ParamRange::linear(7.0, 90.0));
        ranges.insert("git_recency_max_boost".into(), ParamRange::log(2.0, 20.0));
        ranges.insert("git_churn_threshold".into(), ParamRange::linear(3.0, 15.0));
        ranges.insert("git_churn_max_boost".into(), ParamRange::log(2.0, 15.0));

        // Focus expansion
        ranges.insert("focus_decay".into(), ParamRange::linear(0.2, 0.8));
        ranges.insert("focus_max_hops".into(), ParamRange::linear(1.0, 3.0));

        Self { ranges }
    }
}

impl ParameterGrid {
    /// Decode a normalized vector [0, 1]^D into a ParameterPoint.
    pub fn decode(&self, normalized: &[f64]) -> ParameterPoint {
        let names = self.param_names();
        assert_eq!(normalized.len(), names.len(), "Dimension mismatch");

        let values: HashMap<_, _> = names
            .iter()
            .zip(normalized.iter())
            .map(|(name, &n)| {
                let range = &self.ranges[name];
                (name.as_str(), range.decode(n))
            })
            .collect();

        ParameterPoint {
            pagerank_alpha: values["pagerank_alpha"],
            pagerank_chat_multiplier: values["pagerank_chat_multiplier"],
            depth_weight_root: values["depth_weight_root"],
            depth_weight_moderate: values["depth_weight_moderate"],
            depth_weight_deep: values["depth_weight_deep"],
            depth_weight_vendor: values["depth_weight_vendor"],
            boost_mentioned_ident: values["boost_mentioned_ident"],
            boost_mentioned_file: values["boost_mentioned_file"],
            boost_chat_file: values["boost_chat_file"],
            boost_temporal_coupling: values["boost_temporal_coupling"],
            boost_focus_expansion: values["boost_focus_expansion"],
            git_recency_decay_days: values["git_recency_decay_days"],
            git_recency_max_boost: values["git_recency_max_boost"],
            git_churn_threshold: values["git_churn_threshold"],
            git_churn_max_boost: values["git_churn_max_boost"],
            focus_decay: values["focus_decay"],
            focus_max_hops: values["focus_max_hops"],
        }
    }

    /// Sorted list of parameter names (for consistent ordering).
    pub fn param_names(&self) -> Vec<String> {
        let mut names: Vec<_> = self.ranges.keys().cloned().collect();
        names.sort();
        names
    }

    /// Number of dimensions.
    pub fn ndim(&self) -> usize {
        self.ranges.len()
    }
}

/// Search strategy for parameter exploration.
#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
pub enum SearchStrategy {
    /// Full grid search (cartesian product of discrete values)
    Grid { points_per_dim: usize },
    /// Latin Hypercube Sampling (uniform space-filling)
    LatinHypercube,
    /// Random uniform sampling
    Random,
    /// Bayesian optimization with Expected Improvement
    Bayesian,
}

/// Sample points from parameter space using specified strategy.
pub fn sample_points(
    grid: &ParameterGrid,
    strategy: SearchStrategy,
    n_samples: usize,
    seed: u64,
) -> Vec<ParameterPoint> {
    let mut rng = StdRng::seed_from_u64(seed);

    match strategy {
        SearchStrategy::Grid { points_per_dim } => {
            sample_grid(grid, points_per_dim)
        }
        SearchStrategy::LatinHypercube => {
            sample_lhs(grid, n_samples, &mut rng)
        }
        SearchStrategy::Random => {
            sample_random(grid, n_samples, &mut rng)
        }
        SearchStrategy::Bayesian => {
            // Bayesian needs iterative sampling - start with LHS
            // The actual BO loop is in the training runner
            sample_lhs(grid, n_samples.min(20), &mut rng)
        }
    }
}

/// Full grid search: cartesian product of discrete values per dimension.
fn sample_grid(grid: &ParameterGrid, points_per_dim: usize) -> Vec<ParameterPoint> {
    let names = grid.param_names();
    let ndim = names.len();

    // Total combinations
    let total = points_per_dim.pow(ndim as u32);

    (0..total)
        .map(|idx| {
            // Convert linear index to multi-index
            let mut normalized = Vec::with_capacity(ndim);
            let mut remaining = idx;

            for _ in 0..ndim {
                let dim_idx = remaining % points_per_dim;
                remaining /= points_per_dim;

                // Map to [0, 1]
                let t = if points_per_dim > 1 {
                    dim_idx as f64 / (points_per_dim - 1) as f64
                } else {
                    0.5
                };
                normalized.push(t);
            }

            grid.decode(&normalized)
        })
        .collect()
}

/// Latin Hypercube Sampling for uniform space-filling coverage.
///
/// Each dimension is divided into N equal strata, and exactly one
/// sample is placed in each stratum per dimension.
fn sample_lhs<R: Rng>(
    grid: &ParameterGrid,
    n_samples: usize,
    rng: &mut R,
) -> Vec<ParameterPoint> {
    let ndim = grid.ndim();

    // For each dimension, create a random permutation of strata
    let mut strata: Vec<Vec<usize>> = (0..ndim)
        .map(|_| {
            let mut perm: Vec<usize> = (0..n_samples).collect();
            perm.shuffle(rng);
            perm
        })
        .collect();

    // Generate samples
    (0..n_samples)
        .map(|i| {
            let normalized: Vec<f64> = (0..ndim)
                .map(|d| {
                    let stratum = strata[d][i];
                    // Random point within stratum
                    let lower = stratum as f64 / n_samples as f64;
                    let upper = (stratum + 1) as f64 / n_samples as f64;
                    lower + rng.r#gen::<f64>() * (upper - lower)
                })
                .collect();

            grid.decode(&normalized)
        })
        .collect()
}

/// Random uniform sampling.
fn sample_random<R: Rng>(
    grid: &ParameterGrid,
    n_samples: usize,
    rng: &mut R,
) -> Vec<ParameterPoint> {
    let ndim = grid.ndim();

    (0..n_samples)
        .map(|_| {
            let normalized: Vec<f64> = (0..ndim).map(|_| rng.r#gen()).collect();
            grid.decode(&normalized)
        })
        .collect()
}

/// Bayesian optimization: sample next point based on history.
///
/// Uses a simple Expected Improvement acquisition function.
/// For a production implementation, use a proper GP library.
pub fn bayesian_next_sample<R: Rng>(
    grid: &ParameterGrid,
    history: &[(ParameterPoint, f64)], // (params, score)
    rng: &mut R,
) -> ParameterPoint {
    if history.is_empty() {
        // No history, return random sample
        let normalized: Vec<f64> = (0..grid.ndim()).map(|_| rng.r#gen()).collect();
        return grid.decode(&normalized);
    }

    // Find best score so far
    let best_score = history.iter().map(|(_, s)| *s).fold(f64::NEG_INFINITY, f64::max);

    // Simple acquisition: generate candidates and pick highest EI
    // (This is a placeholder - real BO would use a GP)
    let n_candidates = 1000;
    let candidates: Vec<ParameterPoint> = sample_random(grid, n_candidates, rng);

    // For now, use a heuristic: prefer points far from explored regions
    // with slight bias toward high-scoring regions
    candidates
        .into_iter()
        .max_by(|a, b| {
            let dist_a = min_distance_to_history(a, history);
            let dist_b = min_distance_to_history(b, history);

            // Balance exploration (distance) and exploitation (near good points)
            let score_a = dist_a + 0.3 * similarity_to_best(a, history, best_score);
            let score_b = dist_b + 0.3 * similarity_to_best(b, history, best_score);

            score_a.partial_cmp(&score_b).unwrap_or(std::cmp::Ordering::Equal)
        })
        .unwrap_or_else(|| grid.decode(&vec![0.5; grid.ndim()]))
}

/// Minimum normalized distance from point to any point in history.
fn min_distance_to_history(point: &ParameterPoint, history: &[(ParameterPoint, f64)]) -> f64 {
    history
        .iter()
        .map(|(h, _)| normalized_distance(point, h))
        .fold(f64::INFINITY, f64::min)
}

/// Similarity to high-scoring points in history.
fn similarity_to_best(
    point: &ParameterPoint,
    history: &[(ParameterPoint, f64)],
    best_score: f64,
) -> f64 {
    // Weight by how good each historical point was
    let mut total = 0.0;
    let mut weight_sum = 0.0;

    for (h, score) in history {
        let weight = (*score / best_score).max(0.0);
        let sim = 1.0 / (1.0 + normalized_distance(point, h));
        total += weight * sim;
        weight_sum += weight;
    }

    if weight_sum > 0.0 {
        total / weight_sum
    } else {
        0.0
    }
}

/// Normalized L2 distance between parameter points.
fn normalized_distance(a: &ParameterPoint, b: &ParameterPoint) -> f64 {
    let grid = ParameterGrid::default();
    let names = grid.param_names();

    let a_vals = point_to_vec(a, &grid);
    let b_vals = point_to_vec(b, &grid);

    let sum_sq: f64 = a_vals
        .iter()
        .zip(b_vals.iter())
        .map(|(av, bv)| (av - bv).powi(2))
        .sum();

    (sum_sq / names.len() as f64).sqrt()
}

/// Convert ParameterPoint to normalized vector.
fn point_to_vec(point: &ParameterPoint, grid: &ParameterGrid) -> Vec<f64> {
    let names = grid.param_names();
    names
        .iter()
        .map(|name| {
            let range = &grid.ranges[name];
            let value = match name.as_str() {
                "pagerank_alpha" => point.pagerank_alpha,
                "pagerank_chat_multiplier" => point.pagerank_chat_multiplier,
                "depth_weight_root" => point.depth_weight_root,
                "depth_weight_moderate" => point.depth_weight_moderate,
                "depth_weight_deep" => point.depth_weight_deep,
                "depth_weight_vendor" => point.depth_weight_vendor,
                "boost_mentioned_ident" => point.boost_mentioned_ident,
                "boost_mentioned_file" => point.boost_mentioned_file,
                "boost_chat_file" => point.boost_chat_file,
                "boost_temporal_coupling" => point.boost_temporal_coupling,
                "boost_focus_expansion" => point.boost_focus_expansion,
                "git_recency_decay_days" => point.git_recency_decay_days,
                "git_recency_max_boost" => point.git_recency_max_boost,
                "git_churn_threshold" => point.git_churn_threshold,
                "git_churn_max_boost" => point.git_churn_max_boost,
                "focus_decay" => point.focus_decay,
                "focus_max_hops" => point.focus_max_hops,
                _ => 0.5,
            };
            range.encode(value)
        })
        .collect()
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_param_range_linear() {
        let range = ParamRange::linear(0.0, 10.0);
        assert!((range.decode(0.0) - 0.0).abs() < 1e-6);
        assert!((range.decode(0.5) - 5.0).abs() < 1e-6);
        assert!((range.decode(1.0) - 10.0).abs() < 1e-6);
    }

    #[test]
    fn test_param_range_log() {
        let range = ParamRange::log(1.0, 100.0);
        assert!((range.decode(0.0) - 1.0).abs() < 1e-6);
        assert!((range.decode(1.0) - 100.0).abs() < 1e-6);
        // Mid-point in log space: sqrt(1*100) = 10
        assert!((range.decode(0.5) - 10.0).abs() < 1e-6);
    }

    #[test]
    fn test_param_range_roundtrip() {
        let range = ParamRange::log(2.0, 50.0);
        for v in [2.0, 10.0, 25.0, 50.0] {
            let encoded = range.encode(v);
            let decoded = range.decode(encoded);
            assert!((decoded - v).abs() < 1e-6, "Roundtrip failed for {}", v);
        }
    }

    #[test]
    fn test_grid_decode() {
        let grid = ParameterGrid::default();
        let ndim = grid.ndim();

        // All zeros should give min values
        let min_point = grid.decode(&vec![0.0; ndim]);
        assert!(min_point.pagerank_alpha >= 0.69);

        // All ones should give max values
        let max_point = grid.decode(&vec![1.0; ndim]);
        assert!(max_point.pagerank_alpha <= 0.96);
    }

    #[test]
    fn test_lhs_coverage() {
        let grid = ParameterGrid::default();
        let mut rng = StdRng::seed_from_u64(42);
        let samples = sample_lhs(&grid, 10, &mut rng);

        assert_eq!(samples.len(), 10);

        // Check that alpha values span the range reasonably
        let alphas: Vec<_> = samples.iter().map(|s| s.pagerank_alpha).collect();
        let min_alpha = alphas.iter().cloned().fold(f64::INFINITY, f64::min);
        let max_alpha = alphas.iter().cloned().fold(f64::NEG_INFINITY, f64::max);

        assert!(max_alpha - min_alpha > 0.1, "LHS should cover the range");
    }

    #[test]
    fn test_default_point_to_config() {
        let point = ParameterPoint::default();
        let config = point.to_ranking_config();

        // Check that default values match
        assert!((config.pagerank_alpha - 0.85).abs() < 1e-6);
        assert!((config.boost_mentioned_ident - 10.0).abs() < 1e-6);
    }

    #[test]
    fn test_normalized_distance_same() {
        let p = ParameterPoint::default();
        let dist = normalized_distance(&p, &p);
        assert!(dist < 1e-6, "Distance to self should be 0");
    }

    #[test]
    fn test_normalized_distance_different() {
        let mut p1 = ParameterPoint::default();
        let mut p2 = ParameterPoint::default();

        p1.pagerank_alpha = 0.7;
        p2.pagerank_alpha = 0.95;

        let dist = normalized_distance(&p1, &p2);
        assert!(dist > 0.0, "Different points should have distance > 0");
        assert!(dist < 1.0, "Normalized distance should be < 1");
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/training/metrics.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Information retrieval evaluation metrics for ranking quality.
//!
//! ## Metrics Overview
//!
//! | Metric         | What it measures                                    | Range   |
//! |----------------|-----------------------------------------------------|---------|
//! | NDCG@k         | Quality of top-k ranking (graded relevance)         | 0.0-1.0 |
//! | Precision@k    | Fraction of top-k that are relevant (binary)        | 0.0-1.0 |
//! | Recall@k       | Fraction of relevant items in top-k                 | 0.0-1.0 |
//! | MRR            | Reciprocal rank of first relevant item              | 0.0-1.0 |
//! | MAP            | Mean average precision (all relevant items)         | 0.0-1.0 |
//!
//! ## Weighted vs Binary Relevance
//!
//! Git-derived ground truth has **graded relevance**:
//! - Files that always change together: relevance ~1.0
//! - Occasional co-changes: relevance ~0.3
//! - Never co-changed: relevance 0.0
//!
//! NDCG is the primary metric because it handles graded relevance.
//! Precision/Recall/MRR binarize at a threshold (default 0.1).
//!
//! ## Aggregation
//!
//! When evaluating over multiple cases, we report:
//! - Mean across cases (primary)
//! - Std dev (for significance testing)
//! - Median (robust to outliers)
//! - Weighted mean (by case quality)

use std::collections::HashMap;

use serde::{Deserialize, Serialize};

/// Aggregated evaluation metrics over a dataset.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct EvalMetrics {
    /// NDCG@10 (primary metric for graded relevance)
    pub ndcg_at_10: f64,
    /// NDCG@5 (stricter top ranking quality)
    pub ndcg_at_5: f64,

    /// Precision@k (binary relevance at threshold 0.1)
    pub precision_at_5: f64,
    pub precision_at_10: f64,

    /// Recall@k
    pub recall_at_5: f64,
    pub recall_at_10: f64,

    /// Mean Reciprocal Rank
    pub mrr: f64,

    /// Mean Average Precision
    pub map: f64,

    /// Number of cases evaluated
    pub n_cases: usize,

    /// Standard deviations (for significance testing)
    pub ndcg_at_10_std: f64,
    pub mrr_std: f64,
}

impl EvalMetrics {
    /// Aggregate metrics from per-case results.
    pub fn aggregate(per_case: &[CaseMetrics]) -> Self {
        if per_case.is_empty() {
            return Self::default();
        }

        let n = per_case.len() as f64;

        let ndcg_10: Vec<_> = per_case.iter().map(|c| c.ndcg_at_10).collect();
        let ndcg_5: Vec<_> = per_case.iter().map(|c| c.ndcg_at_5).collect();
        let p_5: Vec<_> = per_case.iter().map(|c| c.precision_at_5).collect();
        let p_10: Vec<_> = per_case.iter().map(|c| c.precision_at_10).collect();
        let r_5: Vec<_> = per_case.iter().map(|c| c.recall_at_5).collect();
        let r_10: Vec<_> = per_case.iter().map(|c| c.recall_at_10).collect();
        let mrr: Vec<_> = per_case.iter().map(|c| c.mrr).collect();
        let map: Vec<_> = per_case.iter().map(|c| c.map).collect();

        Self {
            ndcg_at_10: mean(&ndcg_10),
            ndcg_at_5: mean(&ndcg_5),
            precision_at_5: mean(&p_5),
            precision_at_10: mean(&p_10),
            recall_at_5: mean(&r_5),
            recall_at_10: mean(&r_10),
            mrr: mean(&mrr),
            map: mean(&map),
            n_cases: per_case.len(),
            ndcg_at_10_std: std_dev(&ndcg_10),
            mrr_std: std_dev(&mrr),
        }
    }

    /// Aggregate with case weighting (weight by case quality).
    pub fn aggregate_weighted(per_case: &[(CaseMetrics, f64)]) -> Self {
        if per_case.is_empty() {
            return Self::default();
        }

        let total_weight: f64 = per_case.iter().map(|(_, w)| w).sum();
        if total_weight == 0.0 {
            return Self::default();
        }

        let weighted_mean = |f: fn(&CaseMetrics) -> f64| -> f64 {
            per_case.iter().map(|(c, w)| f(c) * w).sum::<f64>() / total_weight
        };

        Self {
            ndcg_at_10: weighted_mean(|c| c.ndcg_at_10),
            ndcg_at_5: weighted_mean(|c| c.ndcg_at_5),
            precision_at_5: weighted_mean(|c| c.precision_at_5),
            precision_at_10: weighted_mean(|c| c.precision_at_10),
            recall_at_5: weighted_mean(|c| c.recall_at_5),
            recall_at_10: weighted_mean(|c| c.recall_at_10),
            mrr: weighted_mean(|c| c.mrr),
            map: weighted_mean(|c| c.map),
            n_cases: per_case.len(),
            ndcg_at_10_std: 0.0, // TODO: weighted std dev
            mrr_std: 0.0,
        }
    }
}

/// Metrics for a single evaluation case.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct CaseMetrics {
    pub ndcg_at_5: f64,
    pub ndcg_at_10: f64,
    pub precision_at_5: f64,
    pub precision_at_10: f64,
    pub recall_at_5: f64,
    pub recall_at_10: f64,
    pub mrr: f64,
    pub map: f64,
}

impl CaseMetrics {
    /// Compute all metrics for a single case.
    ///
    /// # Arguments
    ///
    /// * `ranking` - Our output ranking (file paths in order, best first)
    /// * `ground_truth` - Expected relevant files with weights: (file, relevance)
    /// * `relevance_threshold` - Minimum weight to count as "relevant" for binary metrics
    pub fn compute(
        ranking: &[String],
        ground_truth: &[(String, f64)],
        relevance_threshold: f64,
    ) -> Self {
        Self {
            ndcg_at_5: weighted_ndcg(ranking, ground_truth, 5),
            ndcg_at_10: weighted_ndcg(ranking, ground_truth, 10),
            precision_at_5: precision_at_k(ranking, ground_truth, 5, relevance_threshold),
            precision_at_10: precision_at_k(ranking, ground_truth, 10, relevance_threshold),
            recall_at_5: recall_at_k(ranking, ground_truth, 5, relevance_threshold),
            recall_at_10: recall_at_k(ranking, ground_truth, 10, relevance_threshold),
            mrr: mean_reciprocal_rank(ranking, ground_truth, relevance_threshold),
            map: mean_average_precision(ranking, ground_truth, relevance_threshold),
        }
    }
}

/// Normalized Discounted Cumulative Gain at position k.
///
/// NDCG handles **graded relevance**: files with higher coupling weight
/// contribute more to the score, and rank position matters (early = better).
///
/// ```text
/// DCG@k = Î£áµ¢ (rel[i] / logâ‚‚(i + 2))  for i in 0..k
/// NDCG@k = DCG@k / IDCG@k
/// ```
///
/// Where IDCG is the ideal DCG (perfect ranking by relevance).
///
/// # Arguments
///
/// * `ranking` - Our output (file paths in ranked order)
/// * `ground_truth` - (file, relevance_weight) pairs from git oracle
/// * `k` - Cutoff position
///
/// # Returns
///
/// NDCG score in range [0.0, 1.0]. Higher is better.
pub fn weighted_ndcg(
    ranking: &[String],
    ground_truth: &[(String, f64)],
    k: usize,
) -> f64 {
    if ground_truth.is_empty() {
        return 0.0;
    }

    let truth_map: HashMap<_, _> = ground_truth.iter().cloned().collect();

    // DCG: sum of relevance / logâ‚‚(rank + 2)
    let dcg: f64 = ranking
        .iter()
        .take(k)
        .enumerate()
        .map(|(rank, file)| {
            let relevance = truth_map.get(file).copied().unwrap_or(0.0);
            relevance / (rank as f64 + 2.0).log2()
        })
        .sum();

    // Ideal DCG: sort ground truth by relevance descending
    let mut ideal_weights: Vec<_> = ground_truth.iter().map(|(_, w)| *w).collect();
    ideal_weights.sort_by(|a, b| b.partial_cmp(a).unwrap_or(std::cmp::Ordering::Equal));

    let idcg: f64 = ideal_weights
        .iter()
        .take(k)
        .enumerate()
        .map(|(rank, &rel)| rel / (rank as f64 + 2.0).log2())
        .sum();

    if idcg == 0.0 {
        0.0
    } else {
        dcg / idcg
    }
}

/// Precision at position k.
///
/// Fraction of top-k results that are relevant (binary relevance).
/// Files with weight >= threshold count as relevant.
///
/// ```text
/// P@k = |relevant âˆ© top-k| / k
/// ```
pub fn precision_at_k(
    ranking: &[String],
    ground_truth: &[(String, f64)],
    k: usize,
    threshold: f64,
) -> f64 {
    let relevant: std::collections::HashSet<_> = ground_truth
        .iter()
        .filter(|(_, w)| *w >= threshold)
        .map(|(f, _)| f.as_str())
        .collect();

    if relevant.is_empty() || k == 0 {
        return 0.0;
    }

    let hits = ranking
        .iter()
        .take(k)
        .filter(|f| relevant.contains(f.as_str()))
        .count();

    hits as f64 / k.min(ranking.len()) as f64
}

/// Recall at position k.
///
/// Fraction of relevant items that appear in top-k.
///
/// ```text
/// R@k = |relevant âˆ© top-k| / |relevant|
/// ```
pub fn recall_at_k(
    ranking: &[String],
    ground_truth: &[(String, f64)],
    k: usize,
    threshold: f64,
) -> f64 {
    let relevant: std::collections::HashSet<_> = ground_truth
        .iter()
        .filter(|(_, w)| *w >= threshold)
        .map(|(f, _)| f.as_str())
        .collect();

    if relevant.is_empty() {
        return 0.0;
    }

    let top_k: std::collections::HashSet<_> = ranking
        .iter()
        .take(k)
        .map(|f| f.as_str())
        .collect();

    let hits = relevant.intersection(&top_k).count();

    hits as f64 / relevant.len() as f64
}

/// Mean Reciprocal Rank.
///
/// Reciprocal of the rank of the first relevant item.
/// Measures how quickly we surface ANY relevant result.
///
/// ```text
/// MRR = 1 / rank_of_first_relevant
/// ```
///
/// If no relevant item in ranking, returns 0.
pub fn mean_reciprocal_rank(
    ranking: &[String],
    ground_truth: &[(String, f64)],
    threshold: f64,
) -> f64 {
    let relevant: std::collections::HashSet<_> = ground_truth
        .iter()
        .filter(|(_, w)| *w >= threshold)
        .map(|(f, _)| f.as_str())
        .collect();

    for (rank, file) in ranking.iter().enumerate() {
        if relevant.contains(file.as_str()) {
            return 1.0 / (rank as f64 + 1.0);
        }
    }

    0.0
}

/// Mean Average Precision.
///
/// Average of precision values at each relevant item's rank.
/// Measures overall quality of ranking for all relevant items.
///
/// ```text
/// AP = (1/|relevant|) Ã— Î£áµ¢ P@i Ã— rel(i)
/// ```
///
/// Where the sum is over all positions and rel(i) = 1 if item i is relevant.
pub fn mean_average_precision(
    ranking: &[String],
    ground_truth: &[(String, f64)],
    threshold: f64,
) -> f64 {
    let relevant: std::collections::HashSet<_> = ground_truth
        .iter()
        .filter(|(_, w)| *w >= threshold)
        .map(|(f, _)| f.as_str())
        .collect();

    if relevant.is_empty() {
        return 0.0;
    }

    let mut relevant_seen = 0;
    let mut precision_sum = 0.0;

    for (rank, file) in ranking.iter().enumerate() {
        if relevant.contains(file.as_str()) {
            relevant_seen += 1;
            // Precision at this rank
            let precision = relevant_seen as f64 / (rank as f64 + 1.0);
            precision_sum += precision;
        }
    }

    precision_sum / relevant.len() as f64
}

// === Utility functions ===

fn mean(values: &[f64]) -> f64 {
    if values.is_empty() {
        return 0.0;
    }
    values.iter().sum::<f64>() / values.len() as f64
}

fn std_dev(values: &[f64]) -> f64 {
    if values.len() < 2 {
        return 0.0;
    }
    let m = mean(values);
    let variance = values.iter().map(|x| (x - m).powi(2)).sum::<f64>() / (values.len() - 1) as f64;
    variance.sqrt()
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_ndcg_perfect_ranking() {
        // Perfect ranking: items in order of relevance
        let ranking = vec!["a".to_string(), "b".to_string(), "c".to_string()];
        let truth = vec![
            ("a".to_string(), 1.0),
            ("b".to_string(), 0.5),
            ("c".to_string(), 0.25),
        ];

        let ndcg = weighted_ndcg(&ranking, &truth, 3);
        assert!((ndcg - 1.0).abs() < 1e-6, "Perfect ranking should have NDCG=1.0, got {}", ndcg);
    }

    #[test]
    fn test_ndcg_reversed_ranking() {
        // Worst ranking: items in reverse order
        let ranking = vec!["c".to_string(), "b".to_string(), "a".to_string()];
        let truth = vec![
            ("a".to_string(), 1.0),
            ("b".to_string(), 0.5),
            ("c".to_string(), 0.25),
        ];

        let ndcg = weighted_ndcg(&ranking, &truth, 3);
        assert!(ndcg < 1.0, "Reversed ranking should have NDCG < 1.0, got {}", ndcg);
        assert!(ndcg > 0.0, "Reversed ranking should have NDCG > 0.0");
    }

    #[test]
    fn test_ndcg_partial_match() {
        // Only some items in ranking
        let ranking = vec!["a".to_string(), "x".to_string(), "y".to_string()];
        let truth = vec![
            ("a".to_string(), 1.0),
            ("b".to_string(), 0.5),
        ];

        let ndcg = weighted_ndcg(&ranking, &truth, 3);
        assert!(ndcg > 0.0, "Should get credit for 'a'");
        assert!(ndcg < 1.0, "Missing 'b' should hurt score");
    }

    #[test]
    fn test_precision_at_k() {
        let ranking = vec![
            "a".to_string(),
            "b".to_string(),
            "x".to_string(),
            "c".to_string(),
            "y".to_string(),
        ];
        let truth = vec![
            ("a".to_string(), 1.0),
            ("b".to_string(), 0.8),
            ("c".to_string(), 0.6),
        ];

        // P@2 = 2/2 (both a and b are relevant)
        let p2 = precision_at_k(&ranking, &truth, 2, 0.5);
        assert!((p2 - 1.0).abs() < 1e-6, "P@2 should be 1.0, got {}", p2);

        // P@3 = 2/3 (a, b relevant, x not)
        let p3 = precision_at_k(&ranking, &truth, 3, 0.5);
        assert!((p3 - 2.0 / 3.0).abs() < 1e-6, "P@3 should be 0.667, got {}", p3);

        // P@5 = 3/5 (a, b, c relevant)
        let p5 = precision_at_k(&ranking, &truth, 5, 0.5);
        assert!((p5 - 0.6).abs() < 1e-6, "P@5 should be 0.6, got {}", p5);
    }

    #[test]
    fn test_recall_at_k() {
        let ranking = vec![
            "a".to_string(),
            "x".to_string(),
            "b".to_string(),
        ];
        let truth = vec![
            ("a".to_string(), 1.0),
            ("b".to_string(), 0.8),
            ("c".to_string(), 0.6), // not in ranking
        ];

        // R@1 = 1/3 (only a in top-1)
        let r1 = recall_at_k(&ranking, &truth, 1, 0.5);
        assert!((r1 - 1.0 / 3.0).abs() < 1e-6, "R@1 should be 0.333, got {}", r1);

        // R@3 = 2/3 (a, b in top-3, c missing)
        let r3 = recall_at_k(&ranking, &truth, 3, 0.5);
        assert!((r3 - 2.0 / 3.0).abs() < 1e-6, "R@3 should be 0.667, got {}", r3);
    }

    #[test]
    fn test_mrr() {
        // Relevant item at rank 3 (0-indexed: 2)
        let ranking = vec![
            "x".to_string(),
            "y".to_string(),
            "a".to_string(),
        ];
        let truth = vec![("a".to_string(), 1.0)];

        let mrr = mean_reciprocal_rank(&ranking, &truth, 0.5);
        assert!((mrr - 1.0 / 3.0).abs() < 1e-6, "MRR should be 0.333, got {}", mrr);
    }

    #[test]
    fn test_mrr_first_position() {
        let ranking = vec!["a".to_string(), "x".to_string()];
        let truth = vec![("a".to_string(), 1.0)];

        let mrr = mean_reciprocal_rank(&ranking, &truth, 0.5);
        assert!((mrr - 1.0).abs() < 1e-6, "MRR should be 1.0 when first is relevant");
    }

    #[test]
    fn test_map() {
        // a at rank 1, b at rank 3
        let ranking = vec![
            "a".to_string(),
            "x".to_string(),
            "b".to_string(),
            "y".to_string(),
        ];
        let truth = vec![
            ("a".to_string(), 1.0),
            ("b".to_string(), 0.8),
        ];

        // At rank 1: P=1/1=1.0, At rank 3: P=2/3
        // MAP = (1.0 + 0.667) / 2 = 0.833
        let map = mean_average_precision(&ranking, &truth, 0.5);
        let expected = (1.0 + 2.0 / 3.0) / 2.0;
        assert!((map - expected).abs() < 1e-6, "MAP should be {}, got {}", expected, map);
    }

    #[test]
    fn test_case_metrics() {
        let ranking = vec![
            "a".to_string(),
            "b".to_string(),
            "x".to_string(),
            "c".to_string(),
        ];
        let truth = vec![
            ("a".to_string(), 1.0),
            ("b".to_string(), 0.8),
            ("c".to_string(), 0.6),
        ];

        let metrics = CaseMetrics::compute(&ranking, &truth, 0.5);

        assert!(metrics.ndcg_at_5 > 0.9, "NDCG@5 should be high");
        assert!(metrics.precision_at_5 > 0.5, "P@5 should be decent");
        assert!(metrics.mrr > 0.9, "MRR should be ~1.0");
    }

    #[test]
    fn test_aggregate() {
        let cases = vec![
            CaseMetrics {
                ndcg_at_10: 0.8,
                ndcg_at_5: 0.9,
                precision_at_5: 0.6,
                precision_at_10: 0.5,
                recall_at_5: 0.4,
                recall_at_10: 0.6,
                mrr: 1.0,
                map: 0.7,
            },
            CaseMetrics {
                ndcg_at_10: 0.6,
                ndcg_at_5: 0.7,
                precision_at_5: 0.4,
                precision_at_10: 0.3,
                recall_at_5: 0.2,
                recall_at_10: 0.4,
                mrr: 0.5,
                map: 0.5,
            },
        ];

        let agg = EvalMetrics::aggregate(&cases);

        assert!((agg.ndcg_at_10 - 0.7).abs() < 1e-6);
        assert!((agg.mrr - 0.75).abs() < 1e-6);
        assert_eq!(agg.n_cases, 2);
    }

    #[test]
    fn test_std_dev() {
        // 0, 10 -> mean 5, std dev ~7.07
        let values = vec![0.0, 10.0];
        let sd = std_dev(&values);
        assert!((sd - 7.071).abs() < 0.01, "Std dev should be ~7.07, got {}", sd);
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/training/mod.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Hyperparameter training infrastructure.
//!
//! This module enables scientific training of ripmap's ranking quality by:
//! 1. Extracting ground truth from git history (retrocausal oracle)
//! 2. Defining quality metrics (NDCG, MRR, precision@k)
//! 3. Exploring parameter space (grid, LHS, Bayesian)
//! 4. Sensitivity analysis (which parameters actually matter?)
//! 5. **Reasoning-based training via Claude as universal function approximator**
//!
//! ## The Retrocausal Insight
//!
//! Git history records developer attention. Every commit is a trace of
//! "these files were cognitively connected in this moment." We use this
//! as ground truth: given file A as focus, files B,C,D from the same
//! commit SHOULD rank high.
//!
//! ## Commit Quality Weighting
//!
//! Not all commits provide equal signal:
//! - **Bugfixes** (2-6 files): GOLD - causal relationship to symptom
//! - **Features** (3-8 files): strong semantic coupling
//! - **Refactors** (10+ files): weaker signal, mechanical changes
//! - **WIP/save**: noise, skip entirely
//!
//! ## Reasoning-Based Training
//!
//! The paradigm shift from classical optimization:
//! - Classical: observe Loss(Î¸) â†’ infer âˆ‚Loss/âˆ‚Î¸ â†’ step Î¸ (WHY is lost?)
//! - Reasoning: observe Failure(Î¸) â†’ reason about WHY â†’ propose Î”Î¸ OR Î”structure
//!
//! Claude acts as a universal function approximator, understanding *why*
//! rankings fail and proposing semantically-informed adjustments. The
//! sidechain scratchpad accumulates insights into operator wisdom.
//!
//! ## Usage
//!
//! ```bash
//! # Classical training
//! ripmap-train --curated --strategy bayesian --budget 500
//!
//! # Reasoning-based training
//! ripmap-train --curated --reason --episodes 20
//!
//! # Distill accumulated wisdom
//! ripmap-train --distill --scratchpad scratchpad.json
//! ```

pub mod git_oracle;
pub mod metrics;
pub mod gridsearch;
pub mod sensitivity;
pub mod repos;
pub mod reasoning;
pub mod plots;

pub use git_oracle::{GitCase, WeightedCase, extract_cases, compute_coupling_weights, weight_cases};
pub use metrics::{EvalMetrics, CaseMetrics, weighted_ndcg, precision_at_k, mean_reciprocal_rank};
pub use gridsearch::{ParameterPoint, ParameterGrid, SearchStrategy, sample_points, bayesian_next_sample};
pub use sensitivity::{SensitivityAnalysis, ablation_study, full_analysis, print_summary};
pub use repos::{CURATED_REPOS, RepoSpec, quick_repos};
pub use reasoning::{
    Agent, RankingFailure, ReasoningEpisode, Scratchpad,
    call_claude, call_gemini, call_agent,
    reason_about_failures, update_scratchpad,
    apply_changes, distill_scratchpad, print_scratchpad_summary,
};
pub use plots::LiveProgress;

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/training/plots.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Training progress visualization.
//!
//! Two modes:
//! - **Live terminal**: Unicode sparklines during training
//! - **PNG export**: Publication-quality charts via plotters (optional feature)

#[cfg(feature = "plotters")]
use plotters::prelude::*;

use super::reasoning::Scratchpad;

/// Live training progress display for terminal.
/// Shows sparklines and key metrics as training progresses.
pub struct LiveProgress {
    ndcg_history: Vec<f64>,
    failure_history: Vec<usize>,
    confidence_history: Vec<f64>,
    alpha_history: Vec<f64>,
}

impl LiveProgress {
    pub fn new() -> Self {
        Self {
            ndcg_history: Vec::new(),
            failure_history: Vec::new(),
            confidence_history: Vec::new(),
            alpha_history: Vec::new(),
        }
    }

    /// Record metrics for current episode.
    pub fn record(&mut self, ndcg: f64, failures: usize, confidence: f64, alpha: f64) {
        self.ndcg_history.push(ndcg);
        self.failure_history.push(failures);
        self.confidence_history.push(confidence);
        self.alpha_history.push(alpha);
    }

    /// Render sparkline from values.
    fn sparkline(values: &[f64], width: usize) -> String {
        if values.is_empty() {
            return " ".repeat(width);
        }

        let chars = ['â–', 'â–‚', 'â–ƒ', 'â–„', 'â–…', 'â–†', 'â–‡', 'â–ˆ'];
        let min = values.iter().cloned().fold(f64::INFINITY, f64::min);
        let max = values.iter().cloned().fold(f64::NEG_INFINITY, f64::max);
        let range = (max - min).max(0.001);

        // Sample or pad to width
        let mut result = String::new();
        for i in 0..width {
            let idx = if values.len() <= width {
                if i < values.len() { Some(i) } else { None }
            } else {
                Some(i * values.len() / width)
            };

            if let Some(idx) = idx {
                let normalized = (values[idx] - min) / range;
                let char_idx = ((normalized * 7.0).round() as usize).min(7);
                result.push(chars[char_idx]);
            } else {
                result.push(' ');
            }
        }
        result
    }

    /// Print current progress to terminal.
    pub fn display(&self, episode: usize, total: usize) {
        let width = 20;

        // Clear line and move cursor
        print!("\x1b[2K\r");

        // Episode counter
        print!("E{:2}/{} ", episode, total);

        // NDCG sparkline
        if !self.ndcg_history.is_empty() {
            let last_ndcg = self.ndcg_history.last().unwrap();
            print!("NDCG[{}]{:.3} ", Self::sparkline(&self.ndcg_history, width), last_ndcg);
        }

        // Failures sparkline (convert to f64)
        if !self.failure_history.is_empty() {
            let fail_f64: Vec<f64> = self.failure_history.iter().map(|&f| f as f64).collect();
            let last_fail = self.failure_history.last().unwrap();
            print!("Fail[{}]{} ", Self::sparkline(&fail_f64, width), last_fail);
        }

        // Convergence indicator
        if self.ndcg_history.len() >= 3 {
            let recent: Vec<_> = self.ndcg_history.iter().rev().take(3).collect();
            let variance: f64 = recent.iter()
                .map(|&&x| (x - recent[0]).powi(2))
                .sum::<f64>() / 3.0;

            if variance < 0.0001 {
                print!("âš¡CONVERGED");
            } else if variance < 0.001 {
                print!("~stabilizing");
            }
        }

        // Flush without newline for live update
        use std::io::Write;
        std::io::stdout().flush().ok();
    }

    /// Print final summary with full sparklines.
    pub fn final_summary(&self) {
        use owo_colors::OwoColorize;

        println!("\n");
        println!("{}", " TRAINING COMPLETE ".bold().on_green());
        println!();

        if !self.ndcg_history.is_empty() {
            let first = self.ndcg_history.first().unwrap();
            let last = self.ndcg_history.last().unwrap();
            let delta = last - first;
            let (arrow, delta_str) = if delta > 0.0 {
                ("â†‘", format!("{:+.4}", delta).green().to_string())
            } else if delta < 0.0 {
                ("â†“", format!("{:+.4}", delta).red().to_string())
            } else {
                ("â†’", format!("{:+.4}", delta).dimmed().to_string())
            };
            println!("  {}: {:.4} {} {:.4}  ({})",
                     "NDCG@10".bold(), first, arrow, last, delta_str);
            println!("          [{}]", Self::sparkline(&self.ndcg_history, 40).cyan());
        }

        if !self.failure_history.is_empty() {
            let first = self.failure_history.first().unwrap();
            let last = self.failure_history.last().unwrap();
            let fail_f64: Vec<f64> = self.failure_history.iter().map(|&f| f as f64).collect();
            let delta = (*last as i32) - (*first as i32);
            let delta_str = if delta < 0 {
                format!("{:+}", delta).green().to_string()
            } else if delta > 0 {
                format!("{:+}", delta).red().to_string()
            } else {
                format!("{:+}", delta).dimmed().to_string()
            };
            println!("  {}: {:3} â†’ {:3}  ({})",
                     "Failures".bold(), first, last, delta_str);
            println!("          [{}]", Self::sparkline(&fail_f64, 40).cyan());
        }

        if !self.alpha_history.is_empty() {
            let first = self.alpha_history.first().unwrap();
            let last = self.alpha_history.last().unwrap();
            println!("  {}: {:.3} â†’ {:.3}", "Î±".bold(), first, last);
            println!("          [{}]", Self::sparkline(&self.alpha_history, 40).cyan());
        }
        println!();
    }
}

impl Default for LiveProgress {
    fn default() -> Self {
        Self::new()
    }
}

/// Generate training progress charts from scratchpad data.
#[cfg(feature = "plotters")]
pub fn plot_training_progress(scratchpad: &Scratchpad, output_path: &str) -> Result<(), Box<dyn std::error::Error>> {
    let root = BitMapBackend::new(output_path, (1200, 900)).into_drawing_area();
    root.fill(&WHITE)?;

    let episodes: Vec<_> = scratchpad.episodes.iter().enumerate().collect();
    let n = episodes.len();

    if n == 0 {
        return Ok(());
    }

    // Split into 2x3 grid
    let areas = root.split_evenly((3, 2));

    // 1. Failures per episode
    {
        let failures: Vec<_> = episodes.iter()
            .map(|(i, ep)| (*i as f64, ep.failures.len() as f64))
            .collect();

        let max_fail = failures.iter().map(|(_, f)| *f).fold(0.0_f64, f64::max);

        let mut chart = ChartBuilder::on(&areas[0])
            .caption("Ranking Failures per Episode", ("sans-serif", 20))
            .margin(10)
            .x_label_area_size(30)
            .y_label_area_size(40)
            .build_cartesian_2d(0.0..(n as f64), 0.0..max_fail.max(1.0))?;

        chart.configure_mesh().draw()?;

        chart.draw_series(
            failures.iter().map(|(x, y)| {
                Rectangle::new([(*x - 0.3, 0.0), (*x + 0.3, *y)], RED.mix(0.7).filled())
            })
        )?;
    }

    // 2. Confidence over time
    {
        let confidence: Vec<_> = episodes.iter()
            .map(|(i, ep)| (*i as f64, ep.confidence))
            .collect();

        let mut chart = ChartBuilder::on(&areas[1])
            .caption("Claude Confidence", ("sans-serif", 20))
            .margin(10)
            .x_label_area_size(30)
            .y_label_area_size(40)
            .build_cartesian_2d(0.0..(n as f64), 0.0..1.0)?;

        chart.configure_mesh().draw()?;

        chart.draw_series(LineSeries::new(confidence.clone(), &GREEN))?;
        chart.draw_series(confidence.iter().map(|(x, y)| {
            Circle::new((*x, *y), 4, GREEN.filled())
        }))?;
    }

    // 3. PageRank alpha
    {
        let alpha: Vec<_> = episodes.iter()
            .map(|(i, ep)| (*i as f64, ep.params.pagerank_alpha))
            .collect();

        let (min_a, max_a): (f64, f64) = alpha.iter()
            .map(|(_, a)| *a)
            .fold((1.0_f64, 0.0_f64), |(min, max), a| (min.min(a), max.max(a)));

        let mut chart = ChartBuilder::on(&areas[2])
            .caption("PageRank Î± (damping)", ("sans-serif", 20))
            .margin(10)
            .x_label_area_size(30)
            .y_label_area_size(40)
            .build_cartesian_2d(0.0..(n as f64), (min_a - 0.1)..(max_a + 0.1))?;

        chart.configure_mesh().draw()?;

        // Default line
        chart.draw_series(LineSeries::new(
            vec![(0.0, 0.85), (n as f64, 0.85)],
            &RGBColor(128, 128, 128).mix(0.5),
        ))?.label("Default").legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &RGBColor(128, 128, 128)));

        chart.draw_series(LineSeries::new(alpha.clone(), &BLUE))?;
        chart.draw_series(alpha.iter().map(|(x, y)| {
            Circle::new((*x, *y), 4, BLUE.filled())
        }))?;

        chart.configure_series_labels().draw()?;
    }

    // 4. Boost parameters
    {
        let temporal: Vec<_> = episodes.iter()
            .map(|(i, ep)| (*i as f64, ep.params.boost_temporal_coupling))
            .collect();
        let focus: Vec<_> = episodes.iter()
            .map(|(i, ep)| (*i as f64, ep.params.boost_focus_expansion))
            .collect();

        let max_boost = temporal.iter().chain(focus.iter())
            .map(|(_, b)| *b)
            .fold(0.0_f64, f64::max);

        let mut chart = ChartBuilder::on(&areas[3])
            .caption("Boost Parameters", ("sans-serif", 20))
            .margin(10)
            .x_label_area_size(30)
            .y_label_area_size(40)
            .build_cartesian_2d(0.0..(n as f64), 0.0..max_boost.max(1.0))?;

        chart.configure_mesh().draw()?;

        chart.draw_series(LineSeries::new(temporal.clone(), &BLUE))?
            .label("Temporal Coupling")
            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &BLUE));

        chart.draw_series(LineSeries::new(focus.clone(), &MAGENTA))?
            .label("Focus Expansion")
            .legend(|(x, y)| PathElement::new(vec![(x, y), (x + 20, y)], &MAGENTA));

        chart.configure_series_labels()
            .position(SeriesLabelPosition::UpperLeft)
            .draw()?;
    }

    // 5. Depth weight deep
    {
        let deep: Vec<_> = episodes.iter()
            .map(|(i, ep)| (*i as f64, ep.params.depth_weight_deep))
            .collect();

        let max_d = deep.iter().map(|(_, d)| *d).fold(0.0_f64, f64::max);

        let mut chart = ChartBuilder::on(&areas[4])
            .caption("Deep File Weight", ("sans-serif", 20))
            .margin(10)
            .x_label_area_size(30)
            .y_label_area_size(40)
            .build_cartesian_2d(0.0..(n as f64), 0.0..max_d.max(0.2))?;

        chart.configure_mesh().draw()?;

        chart.draw_series(LineSeries::new(
            vec![(0.0, 0.1), (n as f64, 0.1)],
            &RGBColor(128, 128, 128).mix(0.5),
        ))?;

        chart.draw_series(LineSeries::new(deep.clone(), &RGBColor(128, 0, 128)))?;
        chart.draw_series(deep.iter().map(|(x, y)| {
            Circle::new((*x, *y), 4, RGBColor(128, 0, 128).filled())
        }))?;
    }

    // 6. NDCG progression (if available)
    {
        let ndcg: Vec<_> = episodes.iter()
            .map(|(i, ep)| (*i as f64, ep.ndcg_before))
            .filter(|(_, n)| *n > 0.0)
            .collect();

        let mut chart = ChartBuilder::on(&areas[5])
            .caption("NDCG@10 (higher = better)", ("sans-serif", 20))
            .margin(10)
            .x_label_area_size(30)
            .y_label_area_size(40)
            .build_cartesian_2d(0.0..(n as f64), 0.8..1.0)?;

        chart.configure_mesh().draw()?;

        if !ndcg.is_empty() {
            chart.draw_series(LineSeries::new(ndcg.clone(), &GREEN))?;
            chart.draw_series(ndcg.iter().map(|(x, y)| {
                Circle::new((*x, *y), 4, GREEN.filled())
            }))?;
        } else {
            // No NDCG data yet - show message
            chart.draw_series(std::iter::once(Text::new(
                "Run with --reason to track NDCG",
                (n as f64 / 2.0, 0.9),
                ("sans-serif", 14).into_font().color(&RGBColor(128, 128, 128)),
            )))?;
        }
    }

    root.present()?;
    println!("Saved training chart to {}", output_path);

    Ok(())
}

/// Stub when plotters feature is disabled.
#[cfg(not(feature = "plotters"))]
pub fn plot_training_progress(_scratchpad: &Scratchpad, _output_path: &str) -> Result<(), Box<dyn std::error::Error>> {
    eprintln!("Plotting requires --features plotters");
    Ok(())
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/training/reasoning.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Reasoning-Based Hyperparameter Optimization via LLM as Universal Function Approximator.
//!
//! The paradigm shift:
//! - Classical: observe Loss(Î¸) â†’ infer âˆ‚Loss/âˆ‚Î¸ â†’ step Î¸ (black box: WHY is lost)
//! - Reasoning: observe Failure(Î¸) â†’ reason about WHY â†’ propose Î”Î¸ OR Î”structure
//!
//! The gradient isn't in parameter spaceâ€”it's in concept space.
//!
//! ## The Sidechain Architecture
//!
//! The scratchpad accumulates insights across optimization episodes, building a theory
//! of the hyperparameter manifold that can predict:
//! - "If you're in situation X, parameter Y will fail because Z"
//! - "When A is high, B must compensate or you'll see symptom C"
//!
//! This theory gets distilled into:
//! - PRESETS: clustered insights â†’ named configurations
//! - ADAPTIVE HEURISTICS: conditionals extracted from patterns
//! - OPERATOR INTUITIONS: crystallized warnings and wisdom
//!
//! ## Agent Support
//!
//! Supports multiple LLM backends via `--agent`:
//! - `claude` (default): Uses Claude CLI (`claude --print -p`)
//! - `gemini`: Uses Gemini CLI (`gemini -o json`)

use std::collections::HashMap;
use std::process::Command;
use std::str::FromStr;

use serde::{Deserialize, Serialize};

use super::gridsearch::ParameterPoint;

/// Which LLM agent to use for reasoning.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Default)]
pub enum Agent {
    #[default]
    Claude,
    Gemini,
    /// OpenAI Codex CLI (ChatGPT Pro / o3)
    Codex,
}

impl FromStr for Agent {
    type Err = String;

    fn from_str(s: &str) -> Result<Self, Self::Err> {
        match s.to_lowercase().as_str() {
            "claude" => Ok(Agent::Claude),
            "gemini" => Ok(Agent::Gemini),
            "codex" | "openai" | "o3" => Ok(Agent::Codex),
            _ => Err(format!("Unknown agent: {}. Use 'claude', 'gemini', or 'codex'", s)),
        }
    }
}

impl std::fmt::Display for Agent {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            Agent::Claude => write!(f, "claude"),
            Agent::Gemini => write!(f, "gemini"),
            Agent::Codex => write!(f, "codex"),
        }
    }
}

/// A ranking failure case for reasoning analysis.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RankingFailure {
    /// The focus query used
    pub query: String,
    /// The seed file (starting point)
    pub seed_file: String,
    /// Files that should have ranked high (ground truth)
    pub expected_top: Vec<String>,
    /// Files that actually ranked high
    pub actual_top: Vec<String>,
    /// NDCG score (lower = worse failure)
    pub ndcg: f64,
    /// Commit message providing context
    pub commit_context: String,
    /// Repository metadata
    pub repo_name: String,
    pub repo_file_count: usize,
}

/// One round of reasoning about failures.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ReasoningEpisode {
    /// Unix timestamp (epoch seconds) when this episode was created.
    /// Used for sorting runs by date and displaying temporal context.
    #[serde(default)]
    pub timestamp: i64,
    /// Duration of this episode in seconds (LLM call + evaluation).
    /// Useful for benchmarking different agents and tracking cost.
    #[serde(default)]
    pub duration_secs: f64,
    /// Failures analyzed in this episode
    pub failures: Vec<RankingFailure>,
    /// Parameters at time of failure
    pub params: ParameterPoint,
    /// NDCG@10 before this episode's changes (for tracking convergence)
    #[serde(default)]
    pub ndcg_before: f64,
    /// LLM's free-form reasoning about the trajectory and failures
    pub reasoning: String,
    /// 1-2 sentence capsule encoding the intent/strategy for this episode.
    /// E.g., "Testing counterfactual: reversing depth weights to see if shallow penalty helps"
    /// This lets future episodes understand not just WHAT changed but WHY.
    #[serde(default)]
    pub strategy_capsule: String,
    /// Proposed parameter changes: param_name -> (direction, magnitude, rationale)
    /// Direction: "increase" or "decrease"
    /// Magnitude: "small" (10%), "medium" (30%), "large" (2x)
    pub proposed_changes: HashMap<String, (String, String, String)>,
    /// Discovered parameter interactions (e.g., "low Î± + high boost = tunnel vision")
    #[serde(default)]
    pub param_interactions: Vec<String>,
    /// Structural insights beyond parameter tuning
    pub structural_insights: Vec<String>,
    /// Confidence in proposals (0.0 - 1.0)
    pub confidence: f64,
}

/// Accumulated mental model across optimization episodes.
/// This is the sidechain scratchpad that builds theory of the parameter space.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct Scratchpad {
    /// All reasoning episodes
    pub episodes: Vec<ReasoningEpisode>,

    /// Discovered parameter interactions
    /// e.g., "low Î± + high boost_chat = tunnel vision"
    pub param_interactions: Vec<String>,

    /// Recurring failure patterns
    pub failure_patterns: Vec<String>,

    /// Success patterns (what worked)
    pub success_patterns: Vec<String>,

    /// Proposals for structural changes (beyond tuning)
    pub structural_proposals: Vec<String>,

    /// Distilled presets: name -> (params, description)
    pub presets: HashMap<String, (ParameterPoint, String)>,

    /// Adaptive heuristics: "if CONDITION: ADJUSTMENT"
    pub heuristics: Vec<String>,

    /// Warnings about failure modes
    pub warnings: Vec<String>,
}

/// Call Claude CLI and return response.
///
/// Uses `claude --print -p "prompt"` for non-interactive output.
pub fn call_claude(prompt: &str, model: Option<&str>) -> Result<String, String> {
    let mut args = vec!["--print", "-p", prompt];
    let model_str;
    if let Some(m) = model {
        model_str = m.to_string();
        args.insert(0, "--model");
        args.insert(1, &model_str);
    }

    let output = Command::new("claude")
        .args(&args)
        .output()
        .map_err(|e| format!("Failed to execute claude: {}", e))?;

    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(format!("Claude returned error: {}", stderr));
    }

    Ok(String::from_utf8_lossy(&output.stdout).trim().to_string())
}

/// Call Gemini CLI and return response.
///
/// Uses `gemini -o text "prompt"` for non-interactive output.
/// Gemini outputs plain text by default, we ask for text mode explicitly.
pub fn call_gemini(prompt: &str, model: Option<&str>) -> Result<String, String> {
    let mut cmd = Command::new("gemini");
    cmd.args(["-o", "text", "-y"]);
    if let Some(m) = model {
        cmd.args(["-m", m]);
    }
    cmd.arg(prompt);

    let output = cmd.output()
        .map_err(|e| format!("Failed to execute gemini: {}", e))?;

    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        return Err(format!("Gemini returned error: {}", stderr));
    }

    Ok(String::from_utf8_lossy(&output.stdout).trim().to_string())
}

/// Call Codex CLI (OpenAI o3) and return response.
///
/// Uses `codex exec` for non-interactive output.
/// Reads prompt from stdin and writes output to a temp file.
pub fn call_codex(prompt: &str, model: Option<&str>) -> Result<String, String> {
    use std::io::Write;

    // Create temp file for output
    let output_file = std::env::temp_dir().join(format!("codex_out_{}.txt", std::process::id()));

    let mut args = vec![
        "exec".to_string(),
        "--skip-git-repo-check".to_string(),
        "--dangerously-bypass-approvals-and-sandbox".to_string(),
    ];
    if let Some(m) = model {
        args.push("-m".to_string());
        args.push(m.to_string());
    }
    args.push("-o".to_string());
    args.push(output_file.to_str().unwrap().to_string());
    args.push("-".to_string());  // read prompt from stdin

    // Run codex exec with prompt from stdin, output to file
    let mut child = Command::new("codex")
        .args(&args)
        .stdin(std::process::Stdio::piped())
        .stdout(std::process::Stdio::piped())
        .stderr(std::process::Stdio::piped())
        .spawn()
        .map_err(|e| format!("Failed to execute codex: {}", e))?;

    // Write prompt to stdin
    if let Some(mut stdin) = child.stdin.take() {
        stdin.write_all(prompt.as_bytes())
            .map_err(|e| format!("Failed to write to codex stdin: {}", e))?;
    }

    let output = child.wait_with_output()
        .map_err(|e| format!("Failed to wait for codex: {}", e))?;

    if !output.status.success() {
        let stderr = String::from_utf8_lossy(&output.stderr);
        // Clean up temp file
        let _ = std::fs::remove_file(&output_file);
        return Err(format!("Codex returned error: {}", stderr));
    }

    // Read output from file
    let response = std::fs::read_to_string(&output_file)
        .map_err(|e| format!("Failed to read codex output: {}", e))?;

    // Clean up temp file
    let _ = std::fs::remove_file(&output_file);

    Ok(response.trim().to_string())
}

/// Call the specified LLM agent and return response.
pub fn call_agent(agent: Agent, prompt: &str, model: Option<&str>) -> Result<String, String> {
    match agent {
        Agent::Claude => call_claude(prompt, model),
        Agent::Gemini => call_gemini(prompt, model),
        Agent::Codex => call_codex(prompt, model),
    }
}

/// Reason about ranking failures and propose parameter changes.
///
/// This is where the LLM acts as a universal function approximator:
/// f(failures, params, history) -> (reasoning, proposals, insights)
///
/// The `prompt_template` should contain placeholders:
/// - `{current_ndcg:.4}` - current NDCG score
/// - `{episode_num}` - current episode number
/// - `{episode_history}` - formatted history of recent episodes
/// - `{params_desc}` - current parameter values
/// - `{failure_desc}` - formatted failure cases
///
/// Supports multiple agents via the `agent` parameter.
/// Optionally specify a model (e.g., "opus", "o3", "gemini-2.0-flash").
pub fn reason_about_failures(
    prompt_template: &str,
    failures: &[RankingFailure],
    current_params: &ParameterPoint,
    scratchpad: &Scratchpad,
    current_ndcg: f64,
    agent: Agent,
    model: Option<&str>,
) -> Result<ReasoningEpisode, String> {
    let episode_start = std::time::Instant::now();

    // Format failures for Claude
    let failure_desc: String = failures
        .iter()
        .take(5) // Limit to 5 failures per episode
        .enumerate()
        .map(|(i, f)| {
            format!(
                r#"FAILURE {}:
Query: "{}"
Seed file: {}
Expected top files: {}
Actual top files: {}
NDCG score: {:.3}
Commit context: "{}"
Repo: {} ({} files)"#,
                i + 1,
                f.query,
                f.seed_file,
                f.expected_top.iter().take(5).cloned().collect::<Vec<_>>().join(", "),
                f.actual_top.iter().take(5).cloned().collect::<Vec<_>>().join(", "),
                f.ndcg,
                f.commit_context,
                f.repo_name,
                f.repo_file_count
            )
        })
        .collect::<Vec<_>>()
        .join("\n\n");

    // Format current parameters
    let params_desc = format!(
        r#"  pagerank_alpha: {:.3}
  pagerank_chat_multiplier: {:.1}
  depth_weight_root: {:.2}
  depth_weight_moderate: {:.2}
  depth_weight_deep: {:.2}
  depth_weight_vendor: {:.4}
  boost_mentioned_ident: {:.1}
  boost_mentioned_file: {:.1}
  boost_chat_file: {:.1}
  boost_temporal_coupling: {:.2}
  boost_focus_expansion: {:.2}
  git_recency_decay_days: {:.1}
  git_recency_max_boost: {:.2}
  git_churn_threshold: {:.1}
  git_churn_max_boost: {:.2}
  focus_decay: {:.2}
  focus_max_hops: {:.1}"#,
        current_params.pagerank_alpha,
        current_params.pagerank_chat_multiplier,
        current_params.depth_weight_root,
        current_params.depth_weight_moderate,
        current_params.depth_weight_deep,
        current_params.depth_weight_vendor,
        current_params.boost_mentioned_ident,
        current_params.boost_mentioned_file,
        current_params.boost_chat_file,
        current_params.boost_temporal_coupling,
        current_params.boost_focus_expansion,
        current_params.git_recency_decay_days,
        current_params.git_recency_max_boost,
        current_params.git_churn_threshold,
        current_params.git_churn_max_boost,
        current_params.focus_decay,
        current_params.focus_max_hops,
    );

    // Build FULL episode history - the model needs to see the trajectory
    let episode_history = if scratchpad.episodes.is_empty() {
        "This is the FIRST episode. No history yet.".to_string()
    } else {
        let mut history = String::new();
        let recent_episodes: Vec<_> = scratchpad.episodes.iter().rev().take(10).collect();

        // Show NDCG trajectory with strategy intent - the model sees both WHAT happened and WHY
        history.push_str("EPISODE HISTORY (recent â†’ older):\n");
        for (i, ep) in recent_episodes.iter().enumerate() {
            let trend = if i == 0 { "" } else {
                let prev_ndcg = recent_episodes.get(i - 1).map(|e| e.ndcg_before).unwrap_or(0.0);
                if ep.ndcg_before > prev_ndcg + 0.01 { " â†—" }
                else if ep.ndcg_before < prev_ndcg - 0.01 { " â†˜" }
                else { " â†’" }
            };
            let ep_num = scratchpad.episodes.len() - i;
            let strategy = if ep.strategy_capsule.is_empty() {
                String::new()
            } else {
                format!("\n      Strategy: \"{}\"", ep.strategy_capsule.chars().take(100).collect::<String>())
            };
            history.push_str(&format!("  E{}: NDCG={:.3}{} | failures={}{}\n",
                ep_num, ep.ndcg_before, trend, ep.failures.len(), strategy));
        }

        // Show recent parameter changes - THE GRADIENT
        history.push_str("\nRECENT PARAMETER CHANGES:\n");
        for (i, ep) in recent_episodes.iter().take(5).enumerate() {
            let ep_num = scratchpad.episodes.len() - i;
            if !ep.proposed_changes.is_empty() {
                let changes: Vec<_> = ep.proposed_changes.iter()
                    .map(|(k, (dir, mag, _))| format!("{} {} {}", k, dir, mag))
                    .take(3)
                    .collect();
                history.push_str(&format!("  E{}: {} (conf={:.2})\n",
                    ep_num, changes.join(", "), ep.confidence));
            }
        }

        // Trajectory analysis - help the model see the pattern
        if recent_episodes.len() >= 3 {
            let ndcgs: Vec<f64> = recent_episodes.iter().map(|e| e.ndcg_before).collect();
            let first = ndcgs.last().unwrap_or(&0.0);
            let last = ndcgs.first().unwrap_or(&0.0);
            let trend = last - first;

            history.push_str(&format!("\nâš ï¸ TRAJECTORY ANALYSIS:\n"));
            if trend < -0.05 {
                history.push_str(&format!("  ALERT: NDCG dropped {:.3} over last {} episodes!\n", -trend, recent_episodes.len()));
                history.push_str("  Consider: Are recent changes making things WORSE?\n");
                history.push_str("  Consider: Should we REVERT to earlier params?\n");
            } else if trend > 0.02 {
                history.push_str(&format!("  Good: NDCG improved {:.3} - current direction is working\n", trend));
            } else {
                history.push_str("  Plateau: NDCG stable - may need different approach\n");
            }
        }

        // Key structural insights discovered
        if !scratchpad.structural_proposals.is_empty() {
            history.push_str("\nKEY INSIGHTS FROM TRAINING:\n");
            for insight in scratchpad.structural_proposals.iter().rev().take(5) {
                history.push_str(&format!("  â€¢ {}\n", insight));
            }
        }

        history
    };

    // Inject dynamic context into the prompt template
    let prompt = prompt_template
        .replace("{current_ndcg:.4}", &format!("{:.4}", current_ndcg))
        .replace("{episode_num}", &(scratchpad.episodes.len() + 1).to_string())
        .replace("{episode_history}", &episode_history)
        .replace("{params_desc}", &params_desc)
        .replace("{failure_desc}", &failure_desc);

    let response = call_agent(agent, &prompt, model)?;

    // Extract the reasoning section (everything before JSON:) for storage
    let reasoning_text = if let Some(json_marker) = response.find("JSON:") {
        response[..json_marker].replace("REASONING:", "").trim().to_string()
    } else {
        // No explicit sections, use everything before first {
        response.split('{').next().unwrap_or("").trim().to_string()
    };

    // Parse JSON response - look for JSON after the "JSON:" marker or find first {
    let json_str = if let Some(json_marker) = response.find("JSON:") {
        &response[json_marker + 5..]
    } else {
        &response
    };

    let parsed: serde_json::Value = serde_json::from_str(json_str.trim())
        .or_else(|_| {
            // Try to find JSON in response
            if let Some(start) = json_str.find('{') {
                if let Some(end) = json_str.rfind('}') {
                    return serde_json::from_str(&json_str[start..=end]);
                }
            }
            Err(serde_json::Error::io(std::io::Error::new(
                std::io::ErrorKind::InvalidData,
                "No JSON found in response",
            )))
        })
        .map_err(|e| format!("Failed to parse response as JSON: {}\nResponse: {}", e, &response[..response.len().min(500)]))?;

    // Extract fields
    let mut proposed_changes = HashMap::new();
    if let Some(changes) = parsed.get("proposed_changes").and_then(|v| v.as_object()) {
        for (param, value) in changes {
            if let Some(arr) = value.as_array() {
                if arr.len() >= 3 {
                    let direction = arr[0].as_str().unwrap_or("").to_string();
                    let magnitude = arr[1].as_str().unwrap_or("").to_string();
                    let rationale = arr[2].as_str().unwrap_or("").to_string();
                    proposed_changes.insert(param.clone(), (direction, magnitude, rationale));
                }
            }
        }
    }

    let strategy_capsule = parsed
        .get("strategy_capsule")
        .and_then(|v| v.as_str())
        .unwrap_or("")
        .to_string();

    let param_interactions: Vec<String> = parsed
        .get("param_interactions")
        .and_then(|v| v.as_array())
        .map(|arr| {
            arr.iter()
                .filter_map(|v| v.as_str())
                .map(|s| s.to_string())
                .collect()
        })
        .unwrap_or_default();

    let structural_insights: Vec<String> = parsed
        .get("structural_insights")
        .and_then(|v| v.as_array())
        .map(|arr| {
            arr.iter()
                .filter_map(|v| v.as_str())
                .map(|s| s.to_string())
                .collect()
        })
        .unwrap_or_default();

    let confidence = parsed
        .get("confidence")
        .and_then(|v| v.as_f64())
        .unwrap_or(0.5);

    // Capture the current timestamp for this episode
    let timestamp = std::time::SystemTime::now()
        .duration_since(std::time::UNIX_EPOCH)
        .map(|d| d.as_secs() as i64)
        .unwrap_or(0);

    let duration_secs = episode_start.elapsed().as_secs_f64();

    Ok(ReasoningEpisode {
        timestamp,
        duration_secs,
        failures: failures.to_vec(),
        params: current_params.clone(),
        ndcg_before: current_ndcg,
        reasoning: reasoning_text,  // Store the free-form reasoning, not the raw JSON
        strategy_capsule,           // The intent/mode for this episode
        proposed_changes,
        param_interactions,
        structural_insights,
        confidence,
    })
}

/// Update scratchpad with insights from a reasoning episode.
pub fn update_scratchpad(scratchpad: &mut Scratchpad, episode: &ReasoningEpisode) {
    scratchpad.episodes.push(episode.clone());

    // Add parameter interactions (now stored directly in episode)
    for interaction in &episode.param_interactions {
        if !scratchpad.param_interactions.contains(interaction) {
            scratchpad.param_interactions.push(interaction.clone());
        }
    }

    // Add structural insights
    for insight in &episode.structural_insights {
        if !scratchpad.structural_proposals.contains(insight) {
            scratchpad.structural_proposals.push(insight.clone());
        }
    }
}

/// Apply proposed changes to create a new parameter point.
pub fn apply_changes(params: &ParameterPoint, changes: &HashMap<String, (String, String, String)>) -> ParameterPoint {
    let mut new_params = params.clone();

    for (param_name, (direction, magnitude, _rationale)) in changes {
        let mult = match magnitude.as_str() {
            "small" => 0.1,
            "medium" => 0.3,
            "large" => 1.0,
            _ => 0.3,
        };

        let factor = if direction == "increase" {
            1.0 + mult
        } else {
            1.0 - mult
        };

        match param_name.as_str() {
            "pagerank_alpha" => new_params.pagerank_alpha *= factor,
            "pagerank_chat_multiplier" => new_params.pagerank_chat_multiplier *= factor,
            "depth_weight_root" => new_params.depth_weight_root *= factor,
            "depth_weight_moderate" => new_params.depth_weight_moderate *= factor,
            "depth_weight_deep" => new_params.depth_weight_deep *= factor,
            "depth_weight_vendor" => new_params.depth_weight_vendor *= factor,
            "boost_mentioned_ident" => new_params.boost_mentioned_ident *= factor,
            "boost_mentioned_file" => new_params.boost_mentioned_file *= factor,
            "boost_chat_file" => new_params.boost_chat_file *= factor,
            "boost_temporal_coupling" => new_params.boost_temporal_coupling *= factor,
            "boost_focus_expansion" => new_params.boost_focus_expansion *= factor,
            "git_recency_decay_days" => new_params.git_recency_decay_days *= factor,
            "git_recency_max_boost" => new_params.git_recency_max_boost *= factor,
            "git_churn_threshold" => new_params.git_churn_threshold *= factor,
            "git_churn_max_boost" => new_params.git_churn_max_boost *= factor,
            "focus_decay" => new_params.focus_decay *= factor,
            "focus_max_hops" => new_params.focus_max_hops *= factor,
            _ => {}
        }
    }

    new_params
}

/// Distill accumulated insights into operator wisdom.
pub fn distill_scratchpad(scratchpad: &Scratchpad, agent: Agent, model: Option<&str>) -> Result<String, String> {
    if scratchpad.episodes.is_empty() {
        return Err("No episodes to distill".to_string());
    }

    // Gather all insights
    let all_interactions: Vec<_> = scratchpad.param_interactions.iter().take(20).collect();
    let all_structural: Vec<_> = scratchpad.structural_proposals.iter().take(10).collect();

    // Count proposed changes across episodes
    let mut change_counts: HashMap<String, usize> = HashMap::new();
    for ep in &scratchpad.episodes {
        for (param, (direction, _, _)) in &ep.proposed_changes {
            let key = format!("{}:{}", param, direction);
            *change_counts.entry(key).or_insert(0) += 1;
        }
    }

    let mut sorted_changes: Vec<_> = change_counts.iter().collect();
    sorted_changes.sort_by(|a, b| b.1.cmp(a.1));

    let prompt = format!(
        r#"You are distilling optimization insights into operator wisdom for ripmap.

You have accumulated {} reasoning episodes about the ranking system.

=== DISCOVERED PARAMETER INTERACTIONS ===
{}

=== STRUCTURAL PROPOSALS (beyond tuning) ===
{}

=== MOST COMMON PARAMETER CHANGES ===
{}

=== YOUR TASK ===

Distill these insights into operator wisdom. Return JSON:
{{
  "presets": {{
    "preset_name": {{
      "description": "when to use",
      "key_params": {{"param": value}}
    }}
  }},
  "heuristics": [
    "if CONDITION: ADJUSTMENT - rationale"
  ],
  "warnings": [
    "âš  what happens and why"
  ],
  "intuitions": [
    "Prose wisdom teaching the tool's use"
  ]
}}
"#,
        scratchpad.episodes.len(),
        all_interactions.iter().map(|i| format!("â€¢ {}", i)).collect::<Vec<_>>().join("\n"),
        all_structural.iter().map(|s| format!("â€¢ {}", s)).collect::<Vec<_>>().join("\n"),
        sorted_changes.iter().take(10).map(|(k, v)| format!("â€¢ {}: {} episodes", k, v)).collect::<Vec<_>>().join("\n"),
    );

    call_agent(agent, &prompt, model)
}

/// Print a summary of the scratchpad state.
pub fn print_scratchpad_summary(scratchpad: &Scratchpad) {
    println!("\n=== SCRATCHPAD SUMMARY ===\n");
    println!("Episodes: {}", scratchpad.episodes.len());
    println!("Parameter interactions: {}", scratchpad.param_interactions.len());
    println!("Structural proposals: {}", scratchpad.structural_proposals.len());

    if !scratchpad.param_interactions.is_empty() {
        println!("\nTop Interactions:");
        for interaction in scratchpad.param_interactions.iter().take(5) {
            println!("  â€¢ {}", interaction);
        }
    }

    if !scratchpad.structural_proposals.is_empty() {
        println!("\nStructural Proposals:");
        for proposal in scratchpad.structural_proposals.iter().take(5) {
            println!("  â€¢ {}", proposal);
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_apply_changes() {
        let params = ParameterPoint::default();
        let mut changes = HashMap::new();
        changes.insert(
            "boost_chat_file".to_string(),
            ("decrease".to_string(), "medium".to_string(), "too dominant".to_string()),
        );

        let new_params = apply_changes(&params, &changes);
        assert!(new_params.boost_chat_file < params.boost_chat_file);
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/training/repos.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Curated repository list for benchmark evaluation.
//!
//! ## Selection Criteria
//!
//! The ideal benchmark repository has:
//!
//! 1. **Atomic commit discipline** - Each commit is a logical unit, not a WIP dump
//! 2. **Moderate size** - 50-500 files, enough structure but not too slow
//! 3. **Active development** - Recent commits provide relevant patterns
//! 4. **Multi-file commits** - Average 2-6 files per commit (relational signal)
//! 5. **Diverse intents** - Mix of bugfix, feature, refactor commits
//! 6. **Clear architecture** - Well-structured code has meaningful PageRank
//!
//! ## Commit Rhythm Analysis
//!
//! Different projects have different commit patterns:
//!
//! | Pattern              | Signal Quality | Typical Projects           |
//! |----------------------|----------------|----------------------------|
//! | Squash-merge         | High           | Feature-branch workflows   |
//! | Atomic commits       | High           | Solo/small team projects   |
//! | Conventional commits | High           | OSS with contributors      |
//! | WIP commits          | Low            | Personal projects          |
//! | Mega-commits         | Low            | Infrequent committers      |
//!
//! ## Language Diversity
//!
//! Include repos from multiple languages to ensure the ranking
//! algorithm generalizes and isn't overfit to one ecosystem.

use serde::{Deserialize, Serialize};

/// Specification for a benchmark repository.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RepoSpec {
    /// GitHub URL (for cloning)
    pub url: &'static str,

    /// Short name for logging
    pub name: &'static str,

    /// Primary language
    pub language: Language,

    /// Why this repo was selected
    pub rationale: &'static str,

    /// Expected commit quality (0.0-1.0)
    pub expected_quality: f64,

    /// Approximate size category
    pub size: RepoSize,

    /// Is it actively maintained?
    pub active: bool,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize, PartialEq, Eq)]
pub enum Language {
    Rust,
    Python,
    TypeScript,
    Go,
    Mixed,
}

#[derive(Debug, Clone, Copy, Serialize, Deserialize)]
pub enum RepoSize {
    /// <100 files
    Small,
    /// 100-300 files
    Medium,
    /// 300-600 files
    Large,
}

/// Curated list of repositories for benchmarking.
///
/// Selection rationale for each:
///
/// ## Rust Ecosystem
///
/// - **ripgrep**: BurntSushi's meticulous commit hygiene, excellent
///   documentation, clear module structure. Gold standard for Rust.
///
/// - **bat**: Well-structured file viewer, clean separation of concerns,
///   active development. Good PageRank signal from dependency structure.
///
/// - **just**: Task runner with very clean commits, small codebase makes
///   it good for fast iteration during benchmark development.
///
/// - **starship**: Prompt customizer with excellent modular architecture.
///   Many modules with clear dependencies = strong graph signal.
///
/// - **tokei**: Code counter, small but well-architected. Fast to process.
///
/// ## Python Ecosystem
///
/// - **httpx**: Async HTTP library, encode team's excellent code quality.
///   Clean commits, modern Python, good module structure.
///
/// - **rich**: Terminal rendering library, active development, very clear
///   commit messages. Will Mcgugan's disciplined style.
///
/// - **textual**: TUI framework, very active, excellent commit discipline.
///   Complex enough to have meaningful graph structure.
///
/// - **pydantic**: Data validation, extremely active, clear conventional
///   commit style. Good mix of features/fixes.
///
/// ## TypeScript Ecosystem
///
/// - **zod**: Validation library, clean functional style, good commits.
///   Small but high quality.
///
/// ## Go Ecosystem
///
/// - **bubbletea**: TUI library from Charm, excellent commit quality,
///   idiomatic Go with clean architecture.
///
/// - **lazygit**: Git UI, active development, good commit messages.
///   Practical codebase with real complexity.
pub const CURATED_REPOS: &[RepoSpec] = &[
    // === Rust ===
    RepoSpec {
        url: "https://github.com/BurntSushi/ripgrep",
        name: "ripgrep",
        language: Language::Rust,
        rationale: "Gold standard commit hygiene. BurntSushi's meticulous style with clear atomic commits, excellent documentation. Moderate size with rich dependency graph.",
        expected_quality: 0.95,
        size: RepoSize::Medium,
        active: true,
    },
    RepoSpec {
        url: "https://github.com/sharkdp/bat",
        name: "bat",
        language: Language::Rust,
        rationale: "Well-structured file viewer with clean module separation. Active development, good conventional commit style. Syntax highlighting system provides complex dep graph.",
        expected_quality: 0.85,
        size: RepoSize::Medium,
        active: true,
    },
    RepoSpec {
        url: "https://github.com/casey/just",
        name: "just",
        language: Language::Rust,
        rationale: "Task runner with very clean, focused commits. Small codebase ideal for fast iteration. Casey's disciplined style.",
        expected_quality: 0.90,
        size: RepoSize::Small,
        active: true,
    },
    RepoSpec {
        url: "https://github.com/starship/starship",
        name: "starship",
        language: Language::Rust,
        rationale: "Prompt customizer with highly modular architecture. Many independent modules with clear dependencies. Excellent for PageRank signal.",
        expected_quality: 0.80,
        size: RepoSize::Large,
        active: true,
    },
    RepoSpec {
        url: "https://github.com/XAMPPRocky/tokei",
        name: "tokei",
        language: Language::Rust,
        rationale: "Code counter, compact but well-architected. Fast to process, good for quick iterations. Clean language-focused modules.",
        expected_quality: 0.85,
        size: RepoSize::Small,
        active: true,
    },
    RepoSpec {
        url: "https://github.com/eza-community/eza",
        name: "eza",
        language: Language::Rust,
        rationale: "Modern ls replacement, fork of exa. Active community, good commit quality. File system logic provides interesting coupling patterns.",
        expected_quality: 0.80,
        size: RepoSize::Medium,
        active: true,
    },

    // === Python ===
    RepoSpec {
        url: "https://github.com/encode/httpx",
        name: "httpx",
        language: Language::Python,
        rationale: "Async HTTP library from encode team. Excellent code quality, clean commits, modern Python. Good module structure for graph analysis.",
        expected_quality: 0.90,
        size: RepoSize::Medium,
        active: true,
    },
    RepoSpec {
        url: "https://github.com/Textualize/rich",
        name: "rich",
        language: Language::Python,
        rationale: "Terminal rendering by Will McGugan. Very active, disciplined commit style, clear messages. Complex rendering system = rich graph.",
        expected_quality: 0.85,
        size: RepoSize::Medium,
        active: true,
    },
    RepoSpec {
        url: "https://github.com/Textualize/textual",
        name: "textual",
        language: Language::Python,
        rationale: "TUI framework, extremely active development. Excellent conventional commits. Widget system provides clear dependency graph.",
        expected_quality: 0.85,
        size: RepoSize::Large,
        active: true,
    },
    RepoSpec {
        url: "https://github.com/pydantic/pydantic",
        name: "pydantic",
        language: Language::Python,
        rationale: "Data validation library, massive adoption. Very active with clear conventional commits. Mix of core logic and integrations.",
        expected_quality: 0.80,
        size: RepoSize::Large,
        active: true,
    },
    RepoSpec {
        url: "https://github.com/astral-sh/ruff",
        name: "ruff",
        language: Language::Rust,
        rationale: "Python linter in Rust. Extremely active, excellent commit discipline. Rule system provides clear module structure. Cross-ecosystem expertise.",
        expected_quality: 0.90,
        size: RepoSize::Large,
        active: true,
    },

    // === TypeScript ===
    RepoSpec {
        url: "https://github.com/colinhacks/zod",
        name: "zod",
        language: Language::TypeScript,
        rationale: "Schema validation library. Clean functional style, focused codebase. Good commits, popular enough for real-world patterns.",
        expected_quality: 0.80,
        size: RepoSize::Small,
        active: true,
    },

    // === Go ===
    RepoSpec {
        url: "https://github.com/charmbracelet/bubbletea",
        name: "bubbletea",
        language: Language::Go,
        rationale: "TUI library from Charm. Excellent commit quality, idiomatic Go. Elm-style architecture provides clear message flow graph.",
        expected_quality: 0.90,
        size: RepoSize::Small,
        active: true,
    },
    RepoSpec {
        url: "https://github.com/charmbracelet/lipgloss",
        name: "lipgloss",
        language: Language::Go,
        rationale: "Style definitions for TUIs. Very clean, focused codebase. Charm team's disciplined style. Good for testing small repo behavior.",
        expected_quality: 0.90,
        size: RepoSize::Small,
        active: true,
    },
    RepoSpec {
        url: "https://github.com/jesseduffield/lazygit",
        name: "lazygit",
        language: Language::Go,
        rationale: "Git UI, practical codebase with real complexity. Active development, good commit messages. Tests real-world code patterns.",
        expected_quality: 0.75,
        size: RepoSize::Large,
        active: true,
    },
];

impl RepoSpec {
    /// Get clone command for this repo.
    pub fn clone_cmd(&self, dest: &str) -> String {
        format!("git clone --depth 1000 {} {}", self.url, dest)
    }

    /// Estimated number of commits to analyze (based on depth clone).
    pub fn estimated_commits(&self) -> usize {
        match self.size {
            RepoSize::Small => 500,
            RepoSize::Medium => 800,
            RepoSize::Large => 1000,
        }
    }
}

/// Filter repos by criteria.
pub fn filter_repos(
    language: Option<Language>,
    min_quality: Option<f64>,
    max_size: Option<RepoSize>,
) -> Vec<&'static RepoSpec> {
    CURATED_REPOS
        .iter()
        .filter(|r| {
            if let Some(lang) = language {
                if r.language != lang {
                    return false;
                }
            }
            if let Some(min_q) = min_quality {
                if r.expected_quality < min_q {
                    return false;
                }
            }
            if let Some(max_s) = max_size {
                let size_ord = |s: RepoSize| match s {
                    RepoSize::Small => 0,
                    RepoSize::Medium => 1,
                    RepoSize::Large => 2,
                };
                if size_ord(r.size) > size_ord(max_s) {
                    return false;
                }
            }
            true
        })
        .collect()
}

/// Get a quick subset for development/testing.
pub fn quick_repos() -> Vec<&'static RepoSpec> {
    filter_repos(None, Some(0.85), Some(RepoSize::Medium))
        .into_iter()
        .take(5)
        .collect()
}

/// Get all Rust repos (for Rust-specific analysis).
pub fn rust_repos() -> Vec<&'static RepoSpec> {
    filter_repos(Some(Language::Rust), None, None)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_curated_repos_not_empty() {
        assert!(!CURATED_REPOS.is_empty());
    }

    #[test]
    fn test_all_repos_have_url() {
        for repo in CURATED_REPOS {
            assert!(repo.url.starts_with("https://github.com/"));
        }
    }

    #[test]
    fn test_filter_by_language() {
        let rust_repos = filter_repos(Some(Language::Rust), None, None);
        assert!(!rust_repos.is_empty());
        for repo in rust_repos {
            assert_eq!(repo.language, Language::Rust);
        }
    }

    #[test]
    fn test_filter_by_quality() {
        let high_quality = filter_repos(None, Some(0.90), None);
        for repo in high_quality {
            assert!(repo.expected_quality >= 0.90);
        }
    }

    #[test]
    fn test_quick_repos() {
        let quick = quick_repos();
        assert!(!quick.is_empty());
        assert!(quick.len() <= 5);
    }

    #[test]
    fn test_clone_cmd() {
        let repo = &CURATED_REPOS[0];
        let cmd = repo.clone_cmd("/tmp/test");
        assert!(cmd.contains("git clone"));
        assert!(cmd.contains("--depth"));
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/training/sensitivity.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Sensitivity analysis for understanding parameter importance.
//!
//! ## Ablation Studies
//!
//! Measure the impact of each parameter by:
//! 1. Setting it to its "neutral" value (1.0 for multipliers, midpoint for others)
//! 2. Measuring degradation in NDCG
//! 3. Ranking parameters by impact
//!
//! Parameters with high ablation impact are "load-bearing" - the ranking
//! quality depends heavily on them being tuned correctly.
//!
//! ## One-at-a-Time Sensitivity
//!
//! Sweep each parameter while holding others fixed at baseline:
//! - Generates a curve of metric vs parameter value
//! - Reveals non-linear effects, optimal ranges, and plateaus
//!
//! ## Interaction Effects
//!
//! Some parameters interact - their combined effect is non-additive.
//! We detect this by comparing:
//! - Effect of A alone + Effect of B alone
//! - Effect of A and B together
//!
//! Strong interactions suggest these parameters should be tuned jointly.

use std::collections::HashMap;

use serde::{Deserialize, Serialize};

use super::gridsearch::{ParameterGrid, ParameterPoint};

/// Results of sensitivity analysis.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct SensitivityAnalysis {
    /// Parameter importance scores (higher = more impactful)
    /// Computed via ablation: how much does NDCG drop when we neutralize this param?
    pub importance: HashMap<String, f64>,

    /// Rank of each parameter by importance (1 = most important)
    pub importance_rank: HashMap<String, usize>,

    /// One-at-a-time sensitivity curves: param -> [(value, ndcg)]
    pub oat_curves: HashMap<String, Vec<(f64, f64)>>,

    /// Detected interactions: "param_a|param_b" -> interaction strength
    /// Positive = synergistic, Negative = antagonistic
    /// Key format is "param_a|param_b" for JSON compatibility
    pub interactions: HashMap<String, f64>,
}

/// Perform ablation study to measure parameter importance.
///
/// For each parameter, we set it to a "neutral" value and measure
/// how much the metric degrades. Larger degradation = more important.
///
/// # Arguments
///
/// * `baseline` - The baseline parameter configuration (e.g., best found)
/// * `baseline_score` - NDCG of baseline configuration
/// * `evaluator` - Function that evaluates a parameter point and returns NDCG
///
/// # Returns
///
/// Map from parameter name to importance score (baseline_ndcg - ablated_ndcg)
pub fn ablation_study<F>(
    baseline: &ParameterPoint,
    baseline_score: f64,
    evaluator: F,
) -> HashMap<String, f64>
where
    F: Fn(&ParameterPoint) -> f64,
{
    let grid = ParameterGrid::default();
    let mut importance = HashMap::new();

    for name in grid.param_names() {
        let ablated = ablate_param(baseline, &name);
        let ablated_score = evaluator(&ablated);
        let impact = baseline_score - ablated_score;
        importance.insert(name, impact.max(0.0)); // Clamp to non-negative
    }

    importance
}

/// Create a copy of the parameter point with one parameter set to neutral.
///
/// Neutral values:
/// - Multipliers/boosts: 1.0 (no effect)
/// - Decay factors: 0.5 (middle)
/// - Thresholds: midpoint of range
fn ablate_param(baseline: &ParameterPoint, param: &str) -> ParameterPoint {
    let mut ablated = baseline.clone();

    match param {
        // Boosts -> 1.0 means no boost
        "boost_mentioned_ident" => ablated.boost_mentioned_ident = 1.0,
        "boost_mentioned_file" => ablated.boost_mentioned_file = 1.0,
        "boost_chat_file" => ablated.boost_chat_file = 1.0,
        "boost_temporal_coupling" => ablated.boost_temporal_coupling = 1.0,
        "boost_focus_expansion" => ablated.boost_focus_expansion = 1.0,

        // PageRank multiplier
        "pagerank_chat_multiplier" => ablated.pagerank_chat_multiplier = 1.0,

        // PageRank alpha -> middle of range
        "pagerank_alpha" => ablated.pagerank_alpha = 0.85,

        // Depth weights -> all equal (1.0)
        "depth_weight_root" => ablated.depth_weight_root = 1.0,
        "depth_weight_moderate" => ablated.depth_weight_moderate = 1.0,
        "depth_weight_deep" => ablated.depth_weight_deep = 1.0,
        "depth_weight_vendor" => ablated.depth_weight_vendor = 1.0,

        // Git -> neutral values
        "git_recency_decay_days" => ablated.git_recency_decay_days = 30.0,
        "git_recency_max_boost" => ablated.git_recency_max_boost = 1.0,
        "git_churn_threshold" => ablated.git_churn_threshold = 10.0,
        "git_churn_max_boost" => ablated.git_churn_max_boost = 1.0,

        // Focus expansion -> conservative
        "focus_decay" => ablated.focus_decay = 0.5,
        "focus_max_hops" => ablated.focus_max_hops = 1.0,

        _ => {} // Unknown param, leave unchanged
    }

    ablated
}

/// One-at-a-time sensitivity analysis.
///
/// For each parameter, sweep it across its range while holding others
/// at baseline values. Returns curves showing metric vs parameter value.
///
/// # Arguments
///
/// * `baseline` - Baseline parameter configuration
/// * `evaluator` - Function that evaluates a parameter point
/// * `n_points` - Number of points per parameter (default 11)
pub fn oat_sensitivity<F>(
    baseline: &ParameterPoint,
    evaluator: F,
    n_points: usize,
) -> HashMap<String, Vec<(f64, f64)>>
where
    F: Fn(&ParameterPoint) -> f64,
{
    let grid = ParameterGrid::default();
    let mut curves = HashMap::new();

    for name in grid.param_names() {
        let range = &grid.ranges[&name];
        let mut curve = Vec::with_capacity(n_points);

        for i in 0..n_points {
            let t = i as f64 / (n_points - 1) as f64;
            let value = range.decode(t);

            let point = set_param(baseline, &name, value);
            let score = evaluator(&point);

            curve.push((value, score));
        }

        curves.insert(name, curve);
    }

    curves
}

/// Set a single parameter value in a ParameterPoint.
fn set_param(baseline: &ParameterPoint, param: &str, value: f64) -> ParameterPoint {
    let mut point = baseline.clone();

    match param {
        "pagerank_alpha" => point.pagerank_alpha = value,
        "pagerank_chat_multiplier" => point.pagerank_chat_multiplier = value,
        "depth_weight_root" => point.depth_weight_root = value,
        "depth_weight_moderate" => point.depth_weight_moderate = value,
        "depth_weight_deep" => point.depth_weight_deep = value,
        "depth_weight_vendor" => point.depth_weight_vendor = value,
        "boost_mentioned_ident" => point.boost_mentioned_ident = value,
        "boost_mentioned_file" => point.boost_mentioned_file = value,
        "boost_chat_file" => point.boost_chat_file = value,
        "boost_temporal_coupling" => point.boost_temporal_coupling = value,
        "boost_focus_expansion" => point.boost_focus_expansion = value,
        "git_recency_decay_days" => point.git_recency_decay_days = value,
        "git_recency_max_boost" => point.git_recency_max_boost = value,
        "git_churn_threshold" => point.git_churn_threshold = value,
        "git_churn_max_boost" => point.git_churn_max_boost = value,
        "focus_decay" => point.focus_decay = value,
        "focus_max_hops" => point.focus_max_hops = value,
        _ => {}
    }

    point
}

/// Detect parameter interactions (non-additive effects).
///
/// For each pair of parameters, compare:
/// - Effect of A alone + Effect of B alone
/// - Effect of A and B together
///
/// Large difference indicates interaction.
/// Returns HashMap with string keys "param_a|param_b" for JSON compatibility.
pub fn detect_interactions<F>(
    baseline: &ParameterPoint,
    baseline_score: f64,
    evaluator: F,
    ablation: &HashMap<String, f64>,
) -> HashMap<String, f64>
where
    F: Fn(&ParameterPoint) -> f64,
{
    let grid = ParameterGrid::default();
    let params = grid.param_names();
    let mut interactions = HashMap::new();

    // Only check top parameters (interaction detection is expensive)
    let mut sorted_params: Vec<_> = params.iter()
        .map(|p| (p.clone(), ablation.get(p).copied().unwrap_or(0.0)))
        .collect();
    sorted_params.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap_or(std::cmp::Ordering::Equal));

    let top_params: Vec<_> = sorted_params.into_iter()
        .take(6) // Top 6 most important
        .map(|(p, _)| p)
        .collect();

    for (i, param_a) in top_params.iter().enumerate() {
        for param_b in top_params.iter().skip(i + 1) {
            // Effect of A alone
            let effect_a = ablation.get(param_a).copied().unwrap_or(0.0);

            // Effect of B alone
            let effect_b = ablation.get(param_b).copied().unwrap_or(0.0);

            // Effect of both together
            let ablated_both = ablate_param(&ablate_param(baseline, param_a), param_b);
            let score_both = evaluator(&ablated_both);
            let effect_both = baseline_score - score_both;

            // Interaction = difference from additivity
            // Positive = synergistic (together is worse than sum)
            // Negative = antagonistic (together is better than sum)
            let expected_additive = effect_a + effect_b;
            let interaction = effect_both - expected_additive;

            if interaction.abs() > 0.01 { // Only record significant interactions
                // Key format: "param_a|param_b" for JSON compatibility
                let key = format!("{}|{}", param_a, param_b);
                interactions.insert(key, interaction);
            }
        }
    }

    interactions
}

/// Compute full sensitivity analysis.
pub fn full_analysis<F>(
    baseline: &ParameterPoint,
    evaluator: F,
) -> SensitivityAnalysis
where
    F: Fn(&ParameterPoint) -> f64 + Copy,
{
    let baseline_score = evaluator(baseline);

    // Ablation importance
    let importance = ablation_study(baseline, baseline_score, evaluator);

    // Rank by importance
    let mut sorted: Vec<_> = importance.iter().collect();
    sorted.sort_by(|a, b| b.1.partial_cmp(a.1).unwrap_or(std::cmp::Ordering::Equal));

    let importance_rank: HashMap<_, _> = sorted
        .iter()
        .enumerate()
        .map(|(rank, (name, _))| ((*name).clone(), rank + 1))
        .collect();

    // OAT curves (expensive - could skip for quick analysis)
    let oat_curves = oat_sensitivity(baseline, evaluator, 7);

    // Interactions (most expensive)
    let interactions = detect_interactions(baseline, baseline_score, evaluator, &importance);

    SensitivityAnalysis {
        importance,
        importance_rank,
        oat_curves,
        interactions,
    }
}

/// Print a summary of sensitivity analysis results.
pub fn print_summary(analysis: &SensitivityAnalysis) {
    println!("\n=== Parameter Importance (Ablation) ===\n");

    // Sort by importance
    let mut sorted: Vec<_> = analysis.importance.iter().collect();
    sorted.sort_by(|a, b| b.1.partial_cmp(a.1).unwrap_or(std::cmp::Ordering::Equal));

    for (name, importance) in sorted.iter().take(10) {
        let rank = analysis.importance_rank.get(*name).unwrap_or(&0);
        let bar_len = (*importance * 50.0).round() as usize;
        let bar = "â–ˆ".repeat(bar_len.min(50));
        println!("{:>30}: {:>6.4} [{}] {}", name, importance, rank, bar);
    }

    if !analysis.interactions.is_empty() {
        println!("\n=== Significant Interactions ===\n");

        let mut sorted_interactions: Vec<_> = analysis.interactions.iter().collect();
        sorted_interactions.sort_by(|a, b| {
            b.1.abs().partial_cmp(&a.1.abs()).unwrap_or(std::cmp::Ordering::Equal)
        });

        for (key, strength) in sorted_interactions.iter().take(5) {
            // Parse "param_a|param_b" format
            let parts: Vec<&str> = key.split('|').collect();
            let (a, b) = if parts.len() == 2 {
                (parts[0], parts[1])
            } else {
                (key.as_str(), "?")
            };
            let direction = if **strength > 0.0 { "synergistic" } else { "antagonistic" };
            println!("  {} Ã— {} = {:+.4} ({})", a, b, strength, direction);
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_ablate_boost() {
        let baseline = ParameterPoint::default();
        let ablated = ablate_param(&baseline, "boost_mentioned_ident");
        assert!((ablated.boost_mentioned_ident - 1.0).abs() < 1e-6);
        // Other params should be unchanged
        assert!((ablated.pagerank_alpha - baseline.pagerank_alpha).abs() < 1e-6);
    }

    #[test]
    fn test_set_param() {
        let baseline = ParameterPoint::default();
        let modified = set_param(&baseline, "pagerank_alpha", 0.75);
        assert!((modified.pagerank_alpha - 0.75).abs() < 1e-6);
    }

    #[test]
    fn test_ablation_study() {
        let baseline = ParameterPoint::default();

        // Mock evaluator: score = sum of boost values (simplistic)
        let evaluator = |p: &ParameterPoint| {
            p.boost_mentioned_ident * 0.1 + p.boost_chat_file * 0.05
        };

        let importance = ablation_study(&baseline, evaluator(&baseline), evaluator);

        // boost_mentioned_ident should have importance > 0
        assert!(importance["boost_mentioned_ident"] > 0.0);
    }

    #[test]
    fn test_oat_sensitivity() {
        let baseline = ParameterPoint::default();

        // Simple evaluator
        let evaluator = |p: &ParameterPoint| p.pagerank_alpha;

        let curves = oat_sensitivity(&baseline, evaluator, 5);

        // Should have curves for all params
        assert!(curves.contains_key("pagerank_alpha"));

        // Alpha curve should be monotonically increasing (since evaluator = alpha)
        let alpha_curve = &curves["pagerank_alpha"];
        assert_eq!(alpha_curve.len(), 5);

        for i in 1..alpha_curve.len() {
            assert!(alpha_curve[i].1 >= alpha_curve[i - 1].1);
        }
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/training_outer/mesa.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Mesa Optimizer: Unified abstraction for inner/outer optimization loops.
//!
//! The same pattern applies at both levels:
//! - **Inner (L1)**: Optimizes params Î¸ based on ranking failures
//! - **Outer (L2)**: Optimizes prompts P based on inner run summaries
//!
//! Both levels:
//! 1. Observe current state (failures or run summaries)
//! 2. Reason about what to change (params or prompt sections)
//! 3. Propose changes with rationale
//! 4. Accumulate insights in a scratchpad
//!
//! The outer loop wraps the inner loop - when running L2, each outer episode
//! spawns multiple inner episodes. Directory structure auto-adjusts:
//!
//! ```text
//! training-outer/runs/<outer_run>/
//!   outer_scratchpad.json
//!   config.toml
//!   inner_runs/
//!     step_001/              # Inner run for outer step 1
//!       scratchpad.json
//!       results.json
//!       progress.png
//!     step_002/
//!       ...
//! ```

use std::path::{Path, PathBuf};
use serde::{de::DeserializeOwned, Serialize};

use super::schemas::*;
use super::promptgram::Promptgram;

/// A proposal from a mesa optimizer.
///
/// Generic over the type of changes being proposed.
pub trait Proposal: Serialize + DeserializeOwned {
    /// Get the confidence level of this proposal.
    fn confidence(&self) -> f64;

    /// Get the mode/intent of this proposal (explore, exploit, consolidate).
    fn mode(&self) -> &str;

    /// Get the strategy capsule summarizing intent.
    fn strategy_capsule(&self) -> &str;
}

/// Scratchpad for accumulating insights across episodes.
pub trait Scratchpad: Default + Serialize + DeserializeOwned {
    /// Number of episodes recorded.
    fn episode_count(&self) -> usize;

    /// Save to a JSON file.
    fn save(&self, path: impl AsRef<Path>) -> Result<(), String> {
        let content = serde_json::to_string_pretty(self)
            .map_err(|e| format!("Failed to serialize scratchpad: {}", e))?;
        std::fs::write(path.as_ref(), content)
            .map_err(|e| format!("Failed to write scratchpad: {}", e))
    }

    /// Load from a JSON file.
    fn load(path: impl AsRef<Path>) -> Result<Self, String>
    where
        Self: Sized,
    {
        let content = std::fs::read_to_string(path.as_ref())
            .map_err(|e| format!("Failed to read scratchpad: {}", e))?;
        serde_json::from_str(&content)
            .map_err(|e| format!("Failed to parse scratchpad: {}", e))
    }
}

/// Configuration for a mesa optimizer run.
#[derive(Debug, Clone)]
pub struct RunConfig {
    /// Name of this run (used for directory naming)
    pub run_name: String,

    /// Base directory for outputs
    pub base_dir: PathBuf,

    /// Number of episodes to run
    pub episodes: usize,

    /// Which agent to use
    pub agent: crate::training::reasoning::Agent,

    /// Optional model override
    pub model: Option<String>,

    /// Save interval (checkpoint every N episodes)
    pub save_interval: usize,
}

impl RunConfig {
    /// Get the output directory for this run.
    pub fn output_dir(&self) -> PathBuf {
        self.base_dir.join(&self.run_name)
    }

    /// Get the scratchpad path.
    pub fn scratchpad_path(&self) -> PathBuf {
        self.output_dir().join("scratchpad.json")
    }

    /// Get the results path.
    pub fn results_path(&self) -> PathBuf {
        self.output_dir().join("results.json")
    }

    /// Get the plot path.
    pub fn plot_path(&self) -> PathBuf {
        self.output_dir().join("progress.png")
    }

    /// Create inner run config for a given outer step.
    ///
    /// When running under L2, inner runs go into `inner_runs/step_NNN/`.
    pub fn inner_run_config(&self, outer_step: usize, inner_episodes: usize) -> RunConfig {
        RunConfig {
            run_name: format!("step_{:03}", outer_step),
            base_dir: self.output_dir().join("inner_runs"),
            episodes: inner_episodes,
            agent: self.agent,
            model: self.model.clone(),
            save_interval: self.save_interval,
        }
    }
}

/// Run context passed to the optimizer.
///
/// Contains paths and configuration for the current run.
pub struct RunContext {
    /// Configuration for this run
    pub config: RunConfig,

    /// Current episode number
    pub episode: usize,

    /// Is this an outer loop run?
    pub is_outer: bool,

    /// Parent context (if running inner under outer)
    pub parent_step: Option<usize>,
}

impl RunContext {
    /// Create directories for this run.
    pub fn setup_dirs(&self) -> Result<(), String> {
        std::fs::create_dir_all(self.config.output_dir())
            .map_err(|e| format!("Failed to create output directory: {}", e))?;

        if self.is_outer {
            std::fs::create_dir_all(self.config.output_dir().join("inner_runs"))
                .map_err(|e| format!("Failed to create inner_runs directory: {}", e))?;
        }

        Ok(())
    }

    /// Get path for inner run results.
    pub fn inner_run_path(&self, step: usize) -> PathBuf {
        self.config.output_dir()
            .join("inner_runs")
            .join(format!("step_{:03}", step))
    }
}

/// The MesaOptimizer trait: unified interface for inner/outer loops.
///
/// Type parameters:
/// - `State`: The state being optimized (ParameterPoint for L1, Promptgram for L2)
/// - `Observation`: What we observe (RankingFailures for L1, OuterEpisodeSummary for L2)
/// - `P`: The proposal type
/// - `S`: The scratchpad type
pub trait MesaOptimizer {
    /// The state being optimized.
    type State: Clone + Serialize + DeserializeOwned;

    /// What we observe to inform optimization.
    type Observation;

    /// Proposals for changes.
    type Proposal: Proposal;

    /// Scratchpad for accumulating insights.
    type Scratchpad: Scratchpad;

    /// Initialize default state.
    fn default_state() -> Self::State;

    /// Reason about observations and propose changes.
    fn reason(
        &self,
        state: &Self::State,
        observations: &[Self::Observation],
        scratchpad: &Self::Scratchpad,
        ctx: &RunContext,
    ) -> Result<Self::Proposal, String>;

    /// Apply a proposal to get new state.
    fn apply(&self, state: &Self::State, proposal: &Self::Proposal) -> Self::State;

    /// Update scratchpad with results of an episode.
    fn update_scratchpad(
        &self,
        scratchpad: &mut Self::Scratchpad,
        state_before: &Self::State,
        proposal: &Self::Proposal,
        state_after: &Self::State,
        metrics_delta: f64,
    );

    /// Evaluate current state, returning a metric (higher = better).
    fn evaluate(&self, state: &Self::State) -> Result<(f64, Vec<Self::Observation>), String>;
}

/// Selection mode for choosing promptgram from population.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum SelectionMode {
    /// Always pick the best-performing promptgram
    Best,
    /// Randomly explore a non-best promptgram
    Explore,
    /// Pick the most recently created (newest mutations)
    Recent,
}

impl SelectionMode {
    pub fn as_str(&self) -> &'static str {
        match self {
            SelectionMode::Best => "best",
            SelectionMode::Explore => "explore",
            SelectionMode::Recent => "recent",
        }
    }
}

/// Outer loop: wraps inner loop, evolves promptgrams.
pub struct OuterLoop {
    /// Configuration for inner runs
    pub inner_config: OuterConfig,

    /// Current population of promptgrams
    pub population: Vec<Promptgram>,

    /// The meta-promptgram (P_outer) for L2 reasoning
    pub meta_promptgram: Promptgram,

    /// Directory where evolved promptgrams are persisted
    pub promptgram_dir: PathBuf,
}

impl OuterLoop {
    /// Create a new outer loop with default configuration.
    pub fn new() -> Self {
        let promptgram_dir = PathBuf::from("training-outer/prompts/inner");
        OuterLoop {
            inner_config: OuterConfig::default(),
            population: vec![super::promptgram::baseline_promptgram()],
            meta_promptgram: create_meta_promptgram(),
            promptgram_dir,
        }
    }

    /// Select a promptgram from the population.
    ///
    /// Selection strategy balances exploitation (best performers) with exploration
    /// (trying newer or less-tested prompts). The exploration_quota in config
    /// controls how often we explore vs exploit.
    pub fn select_promptgram(
        &self,
        outer_scratchpad: &OuterScratchpad,
    ) -> (Promptgram, SelectionMode) {
        use rand::Rng;

        if self.population.is_empty() {
            panic!("Population is empty - cannot select promptgram");
        }

        if self.population.len() == 1 {
            // Only one option
            return (self.population[0].clone(), SelectionMode::Best);
        }

        let mut rng = rand::thread_rng();
        let should_explore = rng.r#gen::<f64>() < self.inner_config.exploration_quota;

        if should_explore {
            // Exploration: pick from non-best candidates
            // Prefer less-tested promptgrams or recent mutations
            let best_id = &outer_scratchpad.best_prompt_id;
            let candidates: Vec<&Promptgram> = self.population
                .iter()
                .filter(|p| &p.id != best_id)
                .collect();

            if candidates.is_empty() {
                // All are "best" (only one promptgram evaluated so far)
                return (self.population.last().unwrap().clone(), SelectionMode::Recent);
            }

            // Weight by recency and inverse run count
            let weights: Vec<f64> = candidates.iter().map(|p| {
                let stats = outer_scratchpad.promptgram_stats(&p.id);
                let run_count = stats.map(|s| s.run_count).unwrap_or(0);
                // Never-run prompts get high weight, heavily-run get low weight
                1.0 / (run_count as f64 + 1.0)
            }).collect();

            let total: f64 = weights.iter().sum();
            let mut pick = rng.r#gen::<f64>() * total;

            for (i, w) in weights.iter().enumerate() {
                pick -= w;
                if pick <= 0.0 {
                    return (candidates[i].clone(), SelectionMode::Explore);
                }
            }

            // Fallback to last candidate
            (candidates.last().unwrap().clone().clone(), SelectionMode::Explore)
        } else {
            // Exploitation: pick the best-performing promptgram
            let best_id = &outer_scratchpad.best_prompt_id;
            let best = self.population
                .iter()
                .find(|p| &p.id == best_id)
                .or_else(|| self.population.first())
                .unwrap();
            (best.clone(), SelectionMode::Best)
        }
    }

    /// Persist a promptgram to disk as markdown.
    ///
    /// Promptgrams are saved to `training-outer/prompts/inner/<id>.md`.
    pub fn persist_promptgram(&self, promptgram: &Promptgram) -> Result<PathBuf, String> {
        std::fs::create_dir_all(&self.promptgram_dir)
            .map_err(|e| format!("Failed to create promptgram dir: {}", e))?;

        let filename = format!("{}.md", promptgram.id);
        let path = self.promptgram_dir.join(&filename);

        std::fs::write(&path, promptgram.render())
            .map_err(|e| format!("Failed to write promptgram: {}", e))?;

        // Also save TOML metadata alongside
        let meta_path = self.promptgram_dir.join(format!("{}.toml", promptgram.id));
        promptgram.save(&meta_path)?;

        Ok(path)
    }

    /// Load all promptgrams from disk into the population.
    pub fn load_promptgrams(&mut self) -> Result<usize, String> {
        if !self.promptgram_dir.exists() {
            return Ok(0);
        }

        let mut loaded = 0;
        for entry in std::fs::read_dir(&self.promptgram_dir)
            .map_err(|e| format!("Failed to read promptgram dir: {}", e))?
        {
            let entry = entry.map_err(|e| format!("Failed to read dir entry: {}", e))?;
            let path = entry.path();

            // Load from TOML files (they have full metadata)
            if path.extension().map(|e| e == "toml").unwrap_or(false) {
                match Promptgram::load(&path) {
                    Ok(pg) => {
                        // Check if already in population
                        if !self.population.iter().any(|p| p.id == pg.id) {
                            self.population.push(pg);
                            loaded += 1;
                        }
                    }
                    Err(e) => {
                        eprintln!("Warning: Failed to load promptgram {:?}: {}", path, e);
                    }
                }
            }
        }

        Ok(loaded)
    }

    /// Run one outer episode.
    ///
    /// 1. Select a promptgram from population (explore/exploit)
    /// 2. Run K inner episodes with that promptgram
    /// 3. Summarize the inner run
    /// 4. Invoke L2 to propose promptgram edits
    /// 5. Update population and persist new promptgrams
    pub fn run_outer_episode(
        &mut self,
        outer_step: usize,
        ctx: &RunContext,
        outer_scratchpad: &mut OuterScratchpad,
    ) -> Result<OuterEpisodeSummary, String> {
        let episode_start = std::time::Instant::now();

        // 1. Select promptgram using selection strategy
        let (promptgram, selection_mode) = self.select_promptgram(outer_scratchpad);
        println!("  ğŸ“‹ Selected promptgram: {} (mode: {})", promptgram.id, selection_mode.as_str());

        // 2. Create inner run config
        let inner_ctx = RunContext {
            config: ctx.config.inner_run_config(outer_step, self.inner_config.inner_episodes),
            episode: 0,
            is_outer: false,
            parent_step: Some(outer_step),
        };
        inner_ctx.setup_dirs()?;

        // 3. Run inner episodes (this would call ripmap-train internally)
        // For Stage 0, we'll shell out to the existing binary
        let inner_result = self.run_inner_loop(&promptgram, &inner_ctx)?;

        // 4. Summarize the run (includes selection mode)
        let mut summary = self.summarize_inner_run(outer_step, &promptgram, &inner_result, &episode_start)?;
        summary.selection_mode = selection_mode.as_str().to_string();

        // 5. Update scratchpad first (so L2 can see this episode)
        if summary.final_metrics.ndcg > outer_scratchpad.best_ndcg {
            outer_scratchpad.best_ndcg = summary.final_metrics.ndcg;
            outer_scratchpad.best_prompt_id = promptgram.id.clone();
        }

        // 6. Invoke L2 reasoning if enabled
        if self.inner_config.edit_prompts {
            // Get recent summaries for context (last 3)
            let recent: Vec<&OuterEpisodeSummary> = outer_scratchpad
                .recent_episodes(3)
                .into_iter()
                .rev()  // Oldest first
                .collect();

            // Parse outer agent
            let outer_agent: crate::training::reasoning::Agent = self.inner_config.outer_agent
                .parse()
                .map_err(|e: String| e)?;

            // Call L2 reasoning with full history context
            match self.reason_about_prompt(&promptgram, &recent, outer_scratchpad, outer_agent) {
                Ok(proposal) => {
                    println!("  ğŸ“ L2 proposal: mode={}, confidence={:.2}", proposal.mode, proposal.confidence);
                    println!("     {} edits proposed", proposal.edits.len());
                    for edit in &proposal.edits {
                        println!("       â€¢ {}: {} in {}", edit.edit_type, edit.section,
                            if edit.target.is_empty() { "(new)" } else { &edit.target });
                    }

                    // Track proposal in summary
                    summary.proposal = Some(proposal.clone());

                    // Apply edits to create new promptgram
                    if !proposal.edits.is_empty() {
                        let new_id = format!("inner_v{:03}", self.population.len() + 1);
                        let mut new_promptgram = promptgram.fork(&new_id);

                        let mut edits_applied = 0;
                        for edit in &proposal.edits {
                            if let Err(e) = new_promptgram.apply_edit(edit) {
                                println!("     âš ï¸ Edit failed: {}", e);
                            } else {
                                edits_applied += 1;
                            }
                        }

                        if edits_applied > 0 {
                            // Persist the new promptgram to disk
                            match self.persist_promptgram(&new_promptgram) {
                                Ok(path) => {
                                    println!("     ğŸ’¾ Persisted to {:?}", path);
                                }
                                Err(e) => {
                                    println!("     âš ï¸ Failed to persist: {}", e);
                                }
                            }

                            // Add to population
                            self.population.push(new_promptgram);
                            println!("     âœ“ Created new promptgram: {}", new_id);
                        }
                    }
                }
                Err(e) => {
                    println!("  âš ï¸ L2 reasoning failed: {}", e);
                }
            }
        }

        // 7. Push summary to scratchpad (after L2 proposal is attached)
        outer_scratchpad.episodes.push(summary.clone());

        Ok(summary)
    }

    /// Run the inner loop with a given promptgram.
    ///
    /// For Stage 0, we shell out to `ripmap-train`. Later this could be
    /// integrated directly.
    fn run_inner_loop(
        &self,
        promptgram: &Promptgram,
        ctx: &RunContext,
    ) -> Result<InnerRunResult, String> {
        use std::process::Command;

        let output_dir = ctx.config.output_dir();

        // Determine prompt path: either use configured template or render promptgram
        let prompt_path = if let Some(template_path) = &self.inner_config.prompt_template_path {
            template_path.clone()
        } else {
            // Render promptgram to a temp file in the output directory
            let prompt_file = output_dir.join("prompt.md");
            std::fs::write(&prompt_file, promptgram.render())
                .map_err(|e| format!("Failed to write prompt file: {}", e))?;
            prompt_file.to_str().unwrap().to_string()
        };

        // Build command
        let mut cmd = Command::new("./target/release/ripmap-train");
        cmd.args([
            "--corpus", &self.inner_config.corpus,
            "--reason",
            "--prompt", &prompt_path,
            "--episodes", &ctx.config.episodes.to_string(),
            "--agent", &ctx.config.agent.to_string(),
            "--output", output_dir.join("results.json").to_str().unwrap(),
            "--scratchpad", output_dir.join("scratchpad.json").to_str().unwrap(),
            "--plot", output_dir.join("progress.png").to_str().unwrap(),
        ]);

        // Run and capture output
        let output = cmd.output()
            .map_err(|e| format!("Failed to run inner loop: {}", e))?;

        if !output.status.success() {
            let stderr = String::from_utf8_lossy(&output.stderr);
            return Err(format!("Inner loop failed: {}", stderr));
        }

        // Load results
        let scratchpad_path = output_dir.join("scratchpad.json");
        let scratchpad: crate::training::reasoning::Scratchpad =
            serde_json::from_str(&std::fs::read_to_string(&scratchpad_path)
                .map_err(|e| format!("Failed to read scratchpad: {}", e))?)
            .map_err(|e| format!("Failed to parse scratchpad: {}", e))?;

        // Extract metrics from scratchpad
        let first_ndcg = scratchpad.episodes.first()
            .map(|e| e.ndcg_before)
            .unwrap_or(0.0);
        let final_ndcg = scratchpad.episodes.last()
            .map(|e| e.ndcg_before)
            .unwrap_or(0.0);

        let strategy_capsules: Vec<String> = scratchpad.episodes.iter()
            .filter(|e| !e.strategy_capsule.is_empty())
            .map(|e| e.strategy_capsule.clone())
            .collect();

        let mean_confidence = if scratchpad.episodes.is_empty() {
            0.0
        } else {
            scratchpad.episodes.iter().map(|e| e.confidence).sum::<f64>()
                / scratchpad.episodes.len() as f64
        };

        Ok(InnerRunResult {
            baseline_ndcg: first_ndcg,
            final_ndcg,
            episodes: scratchpad.episodes.len(),
            mean_confidence,
            strategy_capsules,
            structural_insights: scratchpad.structural_proposals.clone(),
            scratchpad,
        })
    }

    /// Summarize an inner run into an OuterEpisodeSummary.
    fn summarize_inner_run(
        &self,
        outer_step: usize,
        promptgram: &Promptgram,
        result: &InnerRunResult,
        start_time: &std::time::Instant,
    ) -> Result<OuterEpisodeSummary, String> {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map(|d| d.as_secs() as i64)
            .unwrap_or(0);

        // Estimate meta-levers from final params
        let final_params = result.scratchpad.episodes.last()
            .map(|e| &e.params)
            .cloned()
            .unwrap_or_default();
        let meta_levers = MetaLevers::from_params(&final_params);

        // Count failures from last episode
        let final_failures = result.scratchpad.episodes.last()
            .map(|e| e.failures.len())
            .unwrap_or(0);

        let baseline = RunMetrics {
            ndcg: result.baseline_ndcg,
            failures: 10, // Approximate initial
            mean_confidence: 0.5,
        };

        let final_metrics = RunMetrics {
            ndcg: result.final_ndcg,
            failures: final_failures,
            mean_confidence: result.mean_confidence,
        };

        // Compute stability metrics
        let ndcg_values: Vec<f64> = result.scratchpad.episodes.iter()
            .map(|e| e.ndcg_before)
            .collect();
        let ndcg_variance = variance(&ndcg_values);

        let collapse_events = ndcg_values.windows(2)
            .filter(|w| w[1] < w[0] - 0.05)
            .count();

        let oscillations = ndcg_values.windows(3)
            .filter(|w| (w[1] > w[0] && w[2] < w[1]) || (w[1] < w[0] && w[2] > w[1]))
            .count();

        Ok(OuterEpisodeSummary {
            outer_step,
            prompt_id: promptgram.id.clone(),
            baseline_metrics: baseline.clone(),
            final_metrics: final_metrics.clone(),
            delta: final_metrics - baseline,
            stability: StabilityMetrics {
                collapse_events,
                ndcg_variance,
                converged: result.final_ndcg > 0.85 && ndcg_variance < 0.01,
                oscillations,
            },
            meta_levers_estimate: meta_levers,
            strategy_capsules: result.strategy_capsules.clone(),
            notable_failures: vec![], // TODO: extract from inner scratchpad
            structural_insights: result.structural_insights.clone(),
            inner_episodes: result.episodes,
            duration_secs: start_time.elapsed().as_secs_f64(),
            timestamp: now,
            proposal: None, // Will be set after L2 reasoning
            selection_mode: String::new(), // Will be set by caller
        })
    }

    /// Invoke L2 reasoning to propose prompt edits.
    ///
    /// The outer agent analyzes recent episodes and proposes changes to the
    /// inner promptgram to improve optimization performance.
    fn reason_about_prompt(
        &self,
        current_promptgram: &Promptgram,
        recent_summaries: &[&OuterEpisodeSummary],
        outer_scratchpad: &OuterScratchpad,
        outer_agent: crate::training::reasoning::Agent,
    ) -> Result<OuterProposal, String> {
        use crate::training::call_agent;

        // Build the L2 prompt with pre-queried history context
        let prompt = self.build_l2_prompt(current_promptgram, recent_summaries, outer_scratchpad);

        println!("  ğŸ§  Invoking L2 reasoning ({})...", outer_agent);

        // Call the outer agent
        let response = call_agent(outer_agent, &prompt, None)?;

        // Parse the response
        self.parse_l2_response(&response)
    }

    /// Build the prompt for L2 reasoning.
    ///
    /// Injects pre-queried history context so L2 can reason about patterns
    /// without needing interactive tool access.
    fn build_l2_prompt(
        &self,
        current_promptgram: &Promptgram,
        recent_summaries: &[&OuterEpisodeSummary],
        outer_scratchpad: &OuterScratchpad,
    ) -> String {
        let mut prompt = String::new();

        // Add meta-promptgram (L2's instructions)
        prompt.push_str(&self.meta_promptgram.render());
        prompt.push_str("\n\n");

        // Add current inner promptgram
        prompt.push_str("=== CURRENT INNER PROMPTGRAM ===\n");
        prompt.push_str(&format!("ID: {}\n", current_promptgram.id));
        prompt.push_str(&format!("Version: {}\n", current_promptgram.version));
        if let Some(ref parent) = current_promptgram.parent_id {
            prompt.push_str(&format!("Parent: {}\n", parent));
        }
        prompt.push_str("\n");
        prompt.push_str(&current_promptgram.render());
        prompt.push_str("\n\n");

        // === PRE-QUERIED HISTORY CONTEXT ===
        // This replaces interactive tool calls - we inject the context L2 needs
        prompt.push_str("=== HISTORY CONTEXT (auto-queried) ===\n\n");

        // Promptgram stats
        if let Some(stats) = outer_scratchpad.promptgram_stats(&current_promptgram.id) {
            prompt.push_str(&format!("Promptgram '{}' stats:\n", stats.prompt_id));
            prompt.push_str(&format!("  Runs: {} | Mean NDCG: {:.4} | Best: {:.4} | Worst: {:.4}\n",
                stats.run_count, stats.mean_ndcg, stats.best_ndcg, stats.worst_ndcg));
            prompt.push_str(&format!("  Active from step {} to {}\n\n", stats.first_step, stats.last_step));
        }

        // Diff against parent if exists
        if let Some(ref parent_id) = current_promptgram.parent_id {
            if let Some(parent) = self.population.iter().find(|p| &p.id == parent_id) {
                let diffs = super::diff_prompts(parent, current_promptgram);
                if !diffs.is_empty() {
                    prompt.push_str(&format!("Changes from parent '{}':\n", parent_id));
                    for diff in &diffs {
                        prompt.push_str(&format!("  {}\n", diff.summary()));
                    }
                    prompt.push_str("\n");
                }
            }
        }

        // Search for common failure patterns
        let failure_searches = ["depth", "temporal", "boost", "collapse", "oscillat"];
        let mut found_patterns = Vec::new();
        for pattern in failure_searches {
            let matches = outer_scratchpad.search_failures(pattern);
            if !matches.is_empty() {
                found_patterns.push((pattern, matches.len()));
            }
        }
        if !found_patterns.is_empty() {
            prompt.push_str("Recurring failure patterns:\n");
            for (pattern, count) in found_patterns {
                prompt.push_str(&format!("  '{}': {} episodes\n", pattern, count));
            }
            prompt.push_str("\n");
        }

        // Population diversity
        let unique_prompts = outer_scratchpad.unique_promptgrams();
        prompt.push_str(&format!("Population: {} unique promptgrams tested\n", unique_prompts.len()));
        if unique_prompts.len() > 1 {
            prompt.push_str("  IDs: ");
            prompt.push_str(&unique_prompts.join(", "));
            prompt.push_str("\n");
        }
        prompt.push_str("\n");

        // Extreme meta-lever episodes (what's been tried)
        let extreme_exploration = outer_scratchpad.extreme_lever_episodes("exploration_bias");
        let extreme_structural = outer_scratchpad.extreme_lever_episodes("structural_trust");
        if !extreme_exploration.is_empty() || !extreme_structural.is_empty() {
            prompt.push_str("Extreme lever episodes (what's been explored):\n");
            for (step, val, _) in extreme_exploration.iter().take(3) {
                prompt.push_str(&format!("  Step {}: exploration_bias={:.2}\n", step, val));
            }
            for (step, val, _) in extreme_structural.iter().take(3) {
                prompt.push_str(&format!("  Step {}: structural_trust={:.2}\n", step, val));
            }
            prompt.push_str("\n");
        }

        // Add recent episode summaries
        prompt.push_str("=== RECENT OUTER EPISODES ===\n");
        for (i, summary) in recent_summaries.iter().enumerate() {
            prompt.push_str(&format!("\n--- Episode {} (step {}) [{}] ---\n",
                i + 1, summary.outer_step, summary.selection_mode));
            prompt.push_str(&format!("NDCG: {:.4} â†’ {:.4} (Î”{:+.4})\n",
                summary.baseline_metrics.ndcg,
                summary.final_metrics.ndcg,
                summary.delta.ndcg));
            prompt.push_str(&format!("Failures: {} â†’ {}\n",
                summary.baseline_metrics.failures,
                summary.final_metrics.failures));
            prompt.push_str(&format!("Stability: {} collapses, {:.4} variance, {} oscillations\n",
                summary.stability.collapse_events,
                summary.stability.ndcg_variance,
                summary.stability.oscillations));
            prompt.push_str(&format!("Converged: {}\n", summary.stability.converged));

            // Meta-levers
            let ml = &summary.meta_levers_estimate;
            prompt.push_str(&format!("Meta-levers: struct={:.2} temp={:.2} explore={:.2} depth={:.2} hub={:.2} focus={:.2}\n",
                ml.structural_trust, ml.temporal_horizon, ml.exploration_bias,
                ml.depth_sensitivity, ml.hub_damping, ml.focus_locality));

            // Strategy capsules
            if !summary.strategy_capsules.is_empty() {
                prompt.push_str("Strategy capsules:\n");
                for capsule in summary.strategy_capsules.iter().take(5) {
                    prompt.push_str(&format!("  â€¢ {}\n", capsule));
                }
            }

            // Structural insights
            if !summary.structural_insights.is_empty() {
                prompt.push_str("Structural insights:\n");
                for insight in summary.structural_insights.iter().take(3) {
                    prompt.push_str(&format!("  â€¢ {}\n", insight));
                }
            }

            // Previous proposal (if any)
            if let Some(ref proposal) = summary.proposal {
                prompt.push_str(&format!("Previous L2 proposal: mode={}, confidence={:.2}, {} edits\n",
                    proposal.mode, proposal.confidence, proposal.edits.len()));
            }
        }

        // Add trajectory summary
        if recent_summaries.len() >= 2 {
            let first = recent_summaries.first().unwrap();
            let last = recent_summaries.last().unwrap();
            let trend = last.final_metrics.ndcg - first.final_metrics.ndcg;
            prompt.push_str(&format!("\n=== TRAJECTORY ===\n"));
            prompt.push_str(&format!("Overall trend: {:+.4} NDCG over {} episodes\n", trend, recent_summaries.len()));

            // Check for plateau/collapse using scratchpad methods
            if outer_scratchpad.is_collapse(3, 0.02) {
                prompt.push_str("âš ï¸ COLLAPSE DETECTED - NDCG degrading over last 3 episodes\n");
            } else if outer_scratchpad.is_plateau(3, 0.01) {
                prompt.push_str("âš ï¸ PLATEAU DETECTED - no improvement in last 3 episodes\n");
            } else if trend > 0.02 {
                prompt.push_str("âœ“ IMPROVING - continue current direction\n");
            }

            // Best vs current
            prompt.push_str(&format!("Best NDCG ever: {:.4} (prompt: {})\n",
                outer_scratchpad.best_ndcg, outer_scratchpad.best_prompt_id));
        }

        prompt.push_str("\n=== YOUR TASK ===\n");
        prompt.push_str("Analyze the trajectory and history context, then propose edits to the inner promptgram.\n");
        prompt.push_str("Consider: What patterns recur? What hasn't been tried? What's the risk?\n");
        prompt.push_str("Output your reasoning, then a JSON block with your proposal.\n");

        prompt
    }

    /// Parse the L2 response into an OuterProposal.
    fn parse_l2_response(&self, response: &str) -> Result<OuterProposal, String> {
        // Find JSON block in response
        let json_start = response.find('{')
            .ok_or("No JSON found in L2 response")?;

        // Find matching closing brace
        let mut depth = 0;
        let mut json_end = json_start;
        for (i, c) in response[json_start..].char_indices() {
            match c {
                '{' => depth += 1,
                '}' => {
                    depth -= 1;
                    if depth == 0 {
                        json_end = json_start + i + 1;
                        break;
                    }
                }
                _ => {}
            }
        }

        let json_str = &response[json_start..json_end];

        // Parse JSON
        serde_json::from_str(json_str)
            .map_err(|e| format!("Failed to parse L2 JSON: {} in: {}", e, json_str))
    }
}

/// Result from running the inner loop.
struct InnerRunResult {
    baseline_ndcg: f64,
    final_ndcg: f64,
    episodes: usize,
    mean_confidence: f64,
    strategy_capsules: Vec<String>,
    structural_insights: Vec<String>,
    scratchpad: crate::training::reasoning::Scratchpad,
}

/// Create the meta-promptgram (P_outer) for L2 reasoning.
///
/// This is the prompt that evolves inner prompts.
pub fn create_meta_promptgram() -> Promptgram {
    Promptgram::new("outer_default_v1")
        .with_section("Role", r#"You are a META-OPTIMIZER that evolves prompts for hyperparameter tuning.

You sit at Level 2 of the optimization stack:
- L0: The code ranker being tuned (17 params, NDCG metric)
- L1: Inner optimizer (the prompts you're evolving)
- L2: You - the promptgram optimizer

Your goal: modify the L1 promptgram to improve optimization performance."#, true)

        .with_section("API_contract", r#"You receive:
- Current promptgram (structured with sections: Role, Policy, Heuristics, Style)
- Last N outer episode summaries (each summarizing an inner run)
- Meta-lever estimates (structural_trust, exploration_bias, etc.)
- Strategy capsules from inner runs (the "why" behind changes)

You output JSON with:
- mode: "explore" | "exploit" | "consolidate"
- mode_justification: why this mode
- confidence: 0.0-1.0
- edits: [{section, edit_type, target, content, rationale}]
- expected_effects: what should improve
- hypothesis: what we're testing
- risk_level: "low" | "medium" | "high""#, true)

        .with_section("Policy", r#"### Mode Selection
Choose your mode based on trajectory:
- **explore**: Try something different. Use when plateaued or after success.
- **exploit**: Refine what's working. Use when improving steadily.
- **consolidate**: Lock in gains. Use after significant improvement.

### Edit Constraints
- Only edit mutable sections (Policy, Heuristics, Style)
- Never touch Role, API_contract, or Output_schema
- Small edits (1-2 lines) for exploit mode
- Larger structural changes (new rules, reframing) for explore mode

### Anisotropy
Maintain diversity in your exploration:
- If recent edits were to Policy, try Heuristics next
- If recent changes were conservative, try something bold
- Track what hasn't been tried"#, false)

        .with_section("Heuristics", r#"- Inner runs with high strategy_capsule diversity often find better optima
- Collapse events (NDCG drops > 5%) suggest the prompt is too aggressive
- High oscillation suggests conflicting heuristics in the prompt
- Meta-lever imbalance (all exploration_bias > 0.7) can cause instability
- Prompts that encourage "why" reasoning outperform pure "what" reasoning"#, false)

        .with_section("Output_schema", r#"REASONING:
[Your analysis of the outer trajectory and promptgram performance]

JSON:
{
  "mode": "explore|exploit|consolidate",
  "mode_justification": "...",
  "confidence": 0.7,
  "edits": [
    {
      "section": "Policy|Heuristics|Style",
      "edit_type": "append|replace|delete",
      "target": "text to replace (for replace/delete)",
      "content": "new content",
      "rationale": "why this helps"
    }
  ],
  "expected_effects": ["..."],
  "hypothesis": "what we're testing",
  "risk_level": "low|medium|high"
}"#, true)
}

/// Compute variance of a slice of f64.
fn variance(values: &[f64]) -> f64 {
    if values.is_empty() {
        return 0.0;
    }
    let mean = values.iter().sum::<f64>() / values.len() as f64;
    let sq_diff_sum: f64 = values.iter().map(|v| (v - mean).powi(2)).sum();
    sq_diff_sum / values.len() as f64
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_run_config_inner() {
        let outer = RunConfig {
            run_name: "test_outer".to_string(),
            base_dir: PathBuf::from("training-outer/runs"),
            episodes: 50,
            agent: crate::training::reasoning::Agent::Claude,
            model: None,
            save_interval: 5,
        };

        let inner = outer.inner_run_config(1, 20);
        assert_eq!(inner.run_name, "step_001");
        assert!(inner.base_dir.to_str().unwrap().contains("inner_runs"));
    }

    #[test]
    fn test_variance() {
        let vals = vec![1.0, 2.0, 3.0, 4.0, 5.0];
        let v = variance(&vals);
        assert!((v - 2.0).abs() < 0.001);
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/training_outer/mod.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! L2 Outer Loop: Promptgram Optimization
//!
//! The outer loop sits above the inner reasoning-based optimizer (L1) and evolves
//! the *prompts themselves* rather than just the hyperparameters.
//!
//! ## The 4-Level Stack
//!
//! - **L0 - Environment**: The code ranker, 17 params, NDCG metric
//! - **L1 - Inner Optimizer**: LLM that proposes param changes based on failures
//! - **L2 - Promptgram Optimizer**: This module - evolves the prompts controlling L1
//! - **L3 - Self-hosted Recursion**: L2 eventually learns to edit itself
//!
//! ## Core Concepts
//!
//! - **Promptgram**: A structured prompt treated as a program with sections
//!   (Role, Policy, Heuristics, Output schema, etc.)
//! - **OuterEpisode**: One run of L1 under a candidate promptgram
//! - **Meta-levers**: Latent axes describing optimization behavior
//!   (structural_trust, temporal_horizon, exploration_bias, etc.)
//!
//! ## Directory Structure
//!
//! ```text
//! training-outer/
//!   runs/
//!     0001_outer_run/
//!       config.toml
//!       population/          # Candidate promptgrams
//!       eval/                # Inner run results
//!       outer_scratchpad.md  # Meta-strategy notebook
//!   prompts/
//!     inner/                 # L1 promptgrams
//!     outer/                 # L2 meta-promptgrams
//! ```

pub mod schemas;
pub mod promptgram;
pub mod mesa;

pub use schemas::*;
pub use promptgram::*;
pub use mesa::*;

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/training_outer/promptgram.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Promptgram: treating prompts as structured programs.
//!
//! A promptgram is not a blob of text but a structured program with sections
//! that can be independently evolved by the outer loop.

use std::collections::HashMap;
use std::path::Path;
use serde::{Deserialize, Serialize};

/// A promptgram: a structured prompt treated as a program.
///
/// Each section serves a specific role and can be independently modified
/// by the outer loop optimizer.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct Promptgram {
    /// Unique identifier for this promptgram
    pub id: String,

    /// Parent promptgram ID (if this was derived from another)
    #[serde(default)]
    pub parent_id: Option<String>,

    /// Version number (increments with each edit)
    pub version: usize,

    /// The structured sections of the prompt
    pub sections: HashMap<String, PromptSection>,

    /// Metadata about this promptgram
    pub metadata: PromptgramMetadata,
}

/// A section of a promptgram.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PromptSection {
    /// Section name (Role, Policy, Heuristics, etc.)
    pub name: String,

    /// Section content (markdown/text)
    pub content: String,

    /// Is this section immutable (cannot be edited by L2)?
    #[serde(default)]
    pub immutable: bool,

    /// Tags for categorization
    #[serde(default)]
    pub tags: Vec<String>,
}

/// Metadata about a promptgram.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct PromptgramMetadata {
    /// When this promptgram was created
    pub created_at: i64,

    /// Last modified timestamp
    pub modified_at: i64,

    /// Best NDCG achieved with this promptgram
    pub best_ndcg: f64,

    /// Number of inner runs using this promptgram
    pub run_count: usize,

    /// Human-readable description
    pub description: String,

    /// Lineage: IDs of ancestor promptgrams
    #[serde(default)]
    pub lineage: Vec<String>,
}

/// Standard section names for inner promptgrams (L1).
pub mod sections {
    pub const ROLE: &str = "Role";
    pub const API_CONTRACT: &str = "API_contract";
    pub const POLICY: &str = "Policy";
    pub const HEURISTICS: &str = "Heuristics";
    pub const CURRICULUM: &str = "Curriculum";
    pub const OUTPUT_SCHEMA: &str = "Output_schema";
    pub const STYLE: &str = "Style";
}

impl Promptgram {
    /// Create a new promptgram with the given ID.
    pub fn new(id: impl Into<String>) -> Self {
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map(|d| d.as_secs() as i64)
            .unwrap_or(0);

        Promptgram {
            id: id.into(),
            parent_id: None,
            version: 1,
            sections: HashMap::new(),
            metadata: PromptgramMetadata {
                created_at: now,
                modified_at: now,
                ..Default::default()
            },
        }
    }

    /// Add or update a section.
    pub fn with_section(mut self, name: &str, content: &str, immutable: bool) -> Self {
        self.sections.insert(name.to_string(), PromptSection {
            name: name.to_string(),
            content: content.to_string(),
            immutable,
            tags: vec![],
        });
        self
    }

    /// Get a section by name.
    pub fn get_section(&self, name: &str) -> Option<&PromptSection> {
        self.sections.get(name)
    }

    /// Render the promptgram to a single prompt string.
    ///
    /// Sections are rendered in a canonical order with markdown headers.
    pub fn render(&self) -> String {
        let section_order = [
            sections::ROLE,
            sections::API_CONTRACT,
            sections::POLICY,
            sections::HEURISTICS,
            sections::CURRICULUM,
            sections::OUTPUT_SCHEMA,
            sections::STYLE,
        ];

        let mut output = String::new();

        // Render sections in canonical order
        for section_name in &section_order {
            if let Some(section) = self.sections.get(*section_name) {
                output.push_str(&format!("## {}\n\n", section.name));
                output.push_str(&section.content);
                output.push_str("\n\n");
            }
        }

        // Render any additional sections not in canonical order
        for (name, section) in &self.sections {
            if !section_order.contains(&name.as_str()) {
                output.push_str(&format!("## {}\n\n", section.name));
                output.push_str(&section.content);
                output.push_str("\n\n");
            }
        }

        output.trim().to_string()
    }

    /// Create a child promptgram (fork with new ID, linked lineage).
    pub fn fork(&self, new_id: impl Into<String>) -> Self {
        let new_id = new_id.into();
        let now = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map(|d| d.as_secs() as i64)
            .unwrap_or(0);

        let mut lineage = self.metadata.lineage.clone();
        lineage.push(self.id.clone());

        Promptgram {
            id: new_id,
            parent_id: Some(self.id.clone()),
            version: 1,
            sections: self.sections.clone(),
            metadata: PromptgramMetadata {
                created_at: now,
                modified_at: now,
                best_ndcg: 0.0,
                run_count: 0,
                description: format!("Fork of {}", self.id),
                lineage,
            },
        }
    }

    /// Apply an edit to a section.
    ///
    /// Returns Err if the section is immutable or doesn't exist (for replace/delete).
    pub fn apply_edit(&mut self, edit: &super::PromptEdit) -> Result<(), String> {
        let section = self.sections.get_mut(&edit.section)
            .ok_or_else(|| format!("Section '{}' not found", edit.section))?;

        if section.immutable {
            return Err(format!("Section '{}' is immutable", edit.section));
        }

        match edit.edit_type.as_str() {
            "append" => {
                section.content.push_str("\n\n");
                section.content.push_str(&edit.content);
            }
            "replace" => {
                if edit.target.is_empty() {
                    // Replace entire section
                    section.content = edit.content.clone();
                } else {
                    // Replace specific target text
                    section.content = section.content.replace(&edit.target, &edit.content);
                }
            }
            "delete" => {
                section.content = section.content.replace(&edit.target, "");
            }
            _ => return Err(format!("Unknown edit type: {}", edit.edit_type)),
        }

        self.version += 1;
        self.metadata.modified_at = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .map(|d| d.as_secs() as i64)
            .unwrap_or(0);

        Ok(())
    }

    /// Load a promptgram from a TOML file.
    pub fn load(path: impl AsRef<Path>) -> Result<Self, String> {
        let content = std::fs::read_to_string(path.as_ref())
            .map_err(|e| format!("Failed to read promptgram: {}", e))?;
        toml::from_str(&content)
            .map_err(|e| format!("Failed to parse promptgram: {}", e))
    }

    /// Save the promptgram to a TOML file.
    pub fn save(&self, path: impl AsRef<Path>) -> Result<(), String> {
        let content = toml::to_string_pretty(self)
            .map_err(|e| format!("Failed to serialize promptgram: {}", e))?;
        std::fs::write(path.as_ref(), content)
            .map_err(|e| format!("Failed to write promptgram: {}", e))
    }

    /// Load from markdown with section headers.
    ///
    /// Format:
    /// ```markdown
    /// ## Role
    /// Content here...
    ///
    /// ## Policy
    /// More content...
    /// ```
    pub fn from_markdown(id: &str, content: &str) -> Self {
        let mut promptgram = Self::new(id);
        let mut current_section: Option<String> = None;
        let mut current_content = String::new();

        for line in content.lines() {
            if line.starts_with("## ") {
                // Save previous section if any
                if let Some(section_name) = current_section.take() {
                    let immutable = matches!(
                        section_name.as_str(),
                        "Role" | "API_contract" | "Output_schema"
                    );
                    promptgram = promptgram.with_section(
                        &section_name,
                        current_content.trim(),
                        immutable,
                    );
                    current_content.clear();
                }

                // Start new section
                current_section = Some(line[3..].trim().to_string());
            } else if current_section.is_some() {
                current_content.push_str(line);
                current_content.push('\n');
            }
        }

        // Save last section
        if let Some(section_name) = current_section {
            let immutable = matches!(
                section_name.as_str(),
                "Role" | "API_contract" | "Output_schema"
            );
            promptgram = promptgram.with_section(
                &section_name,
                current_content.trim(),
                immutable,
            );
        }

        promptgram
    }
}

/// Diff two promptgrams section by section.
///
/// Returns a list of differences for L2 to understand what changed.
/// This helps L2 reason about which mutations were effective.
pub fn diff_prompts(a: &Promptgram, b: &Promptgram) -> Vec<PromptDiff> {
    let mut diffs = Vec::new();

    // Check all sections in A
    for (name, section_a) in &a.sections {
        match b.sections.get(name) {
            Some(section_b) => {
                if section_a.content != section_b.content {
                    diffs.push(PromptDiff {
                        section: name.clone(),
                        diff_type: DiffType::Modified,
                        before: Some(section_a.content.clone()),
                        after: Some(section_b.content.clone()),
                        lines_added: count_lines(&section_b.content).saturating_sub(count_lines(&section_a.content)),
                        lines_removed: count_lines(&section_a.content).saturating_sub(count_lines(&section_b.content)),
                    });
                }
            }
            None => {
                diffs.push(PromptDiff {
                    section: name.clone(),
                    diff_type: DiffType::Removed,
                    before: Some(section_a.content.clone()),
                    after: None,
                    lines_added: 0,
                    lines_removed: count_lines(&section_a.content),
                });
            }
        }
    }

    // Check for sections in B but not in A
    for (name, section_b) in &b.sections {
        if !a.sections.contains_key(name) {
            diffs.push(PromptDiff {
                section: name.clone(),
                diff_type: DiffType::Added,
                before: None,
                after: Some(section_b.content.clone()),
                lines_added: count_lines(&section_b.content),
                lines_removed: 0,
            });
        }
    }

    diffs
}

/// A difference between two promptgram sections.
#[derive(Debug, Clone)]
pub struct PromptDiff {
    pub section: String,
    pub diff_type: DiffType,
    pub before: Option<String>,
    pub after: Option<String>,
    pub lines_added: usize,
    pub lines_removed: usize,
}

impl PromptDiff {
    /// Format as a compact summary string.
    pub fn summary(&self) -> String {
        match self.diff_type {
            DiffType::Added => format!("[+{}] {} (+{} lines)", self.section, "added", self.lines_added),
            DiffType::Removed => format!("[-{}] {} (-{} lines)", self.section, "removed", self.lines_removed),
            DiffType::Modified => format!("[~{}] modified (+{}/-{})", self.section, self.lines_added, self.lines_removed),
        }
    }
}

/// Type of difference between sections.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum DiffType {
    Added,
    Removed,
    Modified,
}

fn count_lines(s: &str) -> usize {
    s.lines().count()
}

/// Create baseline inner promptgram (L1 v001).
///
/// This is the seed promptgram - L2 will mutate it over time.
/// Version numbers track lineage, not archetypes.
pub fn baseline_promptgram() -> Promptgram {
    Promptgram::new("inner_v001")
        .with_section(sections::ROLE, r#"You approximate the gradient in concept space.
Given failures and trajectory, propose parameter changes."#, true)

        .with_section(sections::API_CONTRACT, r#"Input:
- NDCG, episode number, trajectory history
- 17 hyperparameters
- Up to 5 ranking failures with context

Output JSON:
- strategy_capsule: intent encoding
- diagnosis: analysis summary
- param_interactions: discovered couplings
- proposed_changes: {param: [direction, magnitude, rationale]}
- structural_insights: beyond-tuning observations
- confidence: 0.0-1.0"#, true)

        .with_section(sections::POLICY, r#"Analyze trajectory state:
- Improving: continue direction
- Degrading: revert or reverse
- Plateaued: orthogonal move

Analyze failures:
- Missing signal vs overwhelming signal
- Parameter interactions"#, false)

        .with_section(sections::HEURISTICS, r#"- NDCG drop >5% = collapse signal
- Temporal and structural signals compete
- High boosts cause tunnel vision
- Low alpha localizes, high alpha globalizes
- Depth penalties break monorepos"#, false)

        .with_section(sections::OUTPUT_SCHEMA, r#"REASONING:
[analysis]

JSON:
{
  "strategy_capsule": "...",
  "diagnosis": "...",
  "param_interactions": [],
  "proposed_changes": {},
  "structural_insights": [],
  "confidence": 0.7
}"#, true)

        .with_section(sections::STYLE, r#"Analytical. Specific. Reference concrete failures."#, false)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_promptgram_render() {
        let pg = Promptgram::new("test")
            .with_section(sections::ROLE, "You are a test", true)
            .with_section(sections::POLICY, "Do the thing", false);

        let rendered = pg.render();
        assert!(rendered.contains("## Role"));
        assert!(rendered.contains("You are a test"));
        assert!(rendered.contains("## Policy"));
    }

    #[test]
    fn test_promptgram_fork() {
        let parent = Promptgram::new("parent")
            .with_section(sections::POLICY, "Original", false);

        let child = parent.fork("child");

        assert_eq!(child.parent_id, Some("parent".to_string()));
        assert_eq!(child.metadata.lineage, vec!["parent"]);
        assert!(child.get_section(sections::POLICY).is_some());
    }

    #[test]
    fn test_promptgram_edit_immutable() {
        let mut pg = Promptgram::new("test")
            .with_section(sections::ROLE, "Original", true);

        let edit = super::super::PromptEdit {
            section: sections::ROLE.to_string(),
            edit_type: "replace".to_string(),
            target: String::new(),
            content: "Modified".to_string(),
            rationale: "test".to_string(),
        };

        assert!(pg.apply_edit(&edit).is_err());
    }

    #[test]
    fn test_from_markdown() {
        let md = r#"## Role
You are a test optimizer.

## Policy
Do smart things.
Make good choices.

## Style
Be brief.
"#;

        let pg = Promptgram::from_markdown("test", md);
        assert!(pg.get_section("Role").is_some());
        assert!(pg.get_section("Policy").is_some());
        assert!(pg.get_section("Style").is_some());
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/training_outer/schemas.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Core schemas for the L2 outer loop.
//!
//! These types define the interface between inner runs (L1) and the outer
//! promptgram optimizer (L2).

use serde::{Deserialize, Serialize};

/// Meta-levers: latent axes describing optimization behavior.
///
/// These are high-level concepts that summarize HOW the optimizer is behaving,
/// not just what parameters it's tuning. They let the outer loop reason in
/// a human-interpretable space.
///
/// Values are normalized to [0.0, 1.0]:
/// - 0.0 = fully one pole
/// - 1.0 = fully the opposite pole
/// - 0.5 = balanced
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct MetaLevers {
    /// Structural vs Contextual Trust
    /// Low = trust git/temporal signals, High = trust graph structure
    pub structural_trust: f64,

    /// Temporal Horizon
    /// Low = focus on recent changes, High = weight historical patterns equally
    pub temporal_horizon: f64,

    /// Exploration vs Exploitation Bias
    /// Low = conservative incremental, High = bold experimental
    pub exploration_bias: f64,

    /// Depth Sensitivity
    /// Low = flat (all depths equal), High = steep penalty for deep files
    pub depth_sensitivity: f64,

    /// Hub Damping
    /// Low = let popular files dominate, High = penalize high-degree nodes
    pub hub_damping: f64,

    /// Focus Locality
    /// Low = broad graph traversal, High = tight local neighborhood
    pub focus_locality: f64,
}

impl MetaLevers {
    /// Estimate meta-levers from current parameters.
    ///
    /// This is a heuristic projection from Î¸ (17D params) to â„“ (6D meta-levers).
    /// Eventually this could be learned, but we start with hand-coded mapping.
    pub fn from_params(params: &crate::training::gridsearch::ParameterPoint) -> Self {
        // structural_trust: high pagerank_alpha = more structural
        let structural_trust = params.pagerank_alpha.clamp(0.0, 1.0);

        // temporal_horizon: inverse of recency decay (fast decay = short horizon)
        let temporal_horizon = 1.0 - (30.0 / params.git_recency_decay_days.max(1.0)).clamp(0.0, 1.0);

        // exploration_bias: estimated from confidence if available, otherwise from param variance
        // For now, assume moderate exploration
        let exploration_bias = 0.5;

        // depth_sensitivity: ratio of root to deep weights
        let depth_ratio = params.depth_weight_root / params.depth_weight_deep.max(0.01);
        let depth_sensitivity = (depth_ratio / 100.0).clamp(0.0, 1.0);

        // hub_damping: inverse of chat multiplier (high multiplier = low damping)
        let hub_damping = 1.0 - (params.pagerank_chat_multiplier / 200.0).clamp(0.0, 1.0);

        // focus_locality: inverse of max_hops and decay
        let focus_locality = (1.0 - params.focus_decay) * (1.0 - params.focus_max_hops / 5.0);
        let focus_locality = focus_locality.clamp(0.0, 1.0);

        MetaLevers {
            structural_trust,
            temporal_horizon,
            exploration_bias,
            depth_sensitivity,
            hub_damping,
            focus_locality,
        }
    }

    /// Format as a compact string for display.
    pub fn summary(&self) -> String {
        format!(
            "struct={:.2} temp={:.2} explore={:.2} depth={:.2} hub={:.2} focus={:.2}",
            self.structural_trust,
            self.temporal_horizon,
            self.exploration_bias,
            self.depth_sensitivity,
            self.hub_damping,
            self.focus_locality,
        )
    }
}

/// Summary of an inner run for L2 consumption.
///
/// This is the canonical interface between L1 and L2. The outer loop doesn't
/// read raw traces - it reads these summaries which compress the essential
/// information about what happened and why.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OuterEpisodeSummary {
    /// Outer step number
    pub outer_step: usize,

    /// ID of the promptgram used for this inner run
    pub prompt_id: String,

    /// Metrics at start of inner run
    pub baseline_metrics: RunMetrics,

    /// Metrics at end of inner run
    pub final_metrics: RunMetrics,

    /// Delta from baseline to final
    pub delta: RunMetrics,

    /// Stability metrics (variance, collapse events)
    pub stability: StabilityMetrics,

    /// Estimated meta-lever position at end of run
    pub meta_levers_estimate: MetaLevers,

    /// Strategy capsules from inner episodes (the "why" of changes)
    /// These encode the intent/reasoning behind parameter moves.
    pub strategy_capsules: Vec<String>,

    /// Notable failure patterns observed
    pub notable_failures: Vec<String>,

    /// Structural insights discovered (beyond parameter tuning)
    pub structural_insights: Vec<String>,

    /// Number of inner episodes run
    pub inner_episodes: usize,

    /// Total duration of inner run (seconds)
    pub duration_secs: f64,

    /// Timestamp when this outer episode completed
    pub timestamp: i64,

    /// L2 proposal that was made (if any) - tracks what L2 decided
    #[serde(default)]
    pub proposal: Option<OuterProposal>,

    /// Selection mode that chose this promptgram (explore/exploit/best)
    #[serde(default)]
    pub selection_mode: String,
}

/// Core metrics for a run.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct RunMetrics {
    /// NDCG@10 score
    pub ndcg: f64,

    /// Number of ranking failures
    pub failures: usize,

    /// Mean confidence of inner optimizer
    pub mean_confidence: f64,
}

impl std::ops::Sub for RunMetrics {
    type Output = RunMetrics;

    fn sub(self, rhs: Self) -> Self::Output {
        RunMetrics {
            ndcg: self.ndcg - rhs.ndcg,
            failures: self.failures.saturating_sub(rhs.failures),
            mean_confidence: self.mean_confidence - rhs.mean_confidence,
        }
    }
}

/// Stability metrics for a run.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct StabilityMetrics {
    /// Number of collapse events (NDCG drops > 5% in one episode)
    pub collapse_events: usize,

    /// NDCG variance across episodes
    pub ndcg_variance: f64,

    /// Did the run converge (stabilize at high NDCG)?
    pub converged: bool,

    /// Number of oscillations (direction changes in NDCG trend)
    pub oscillations: usize,
}

/// A proposed edit to a promptgram section.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PromptEdit {
    /// Which section to edit (Policy, Heuristics, Style, etc.)
    pub section: String,

    /// Type of edit: "append", "replace", "delete"
    pub edit_type: String,

    /// For replace/delete: target text or rule to modify
    #[serde(default)]
    pub target: String,

    /// New content (for append/replace)
    pub content: String,

    /// Rationale for this edit
    pub rationale: String,
}

/// Output from the outer optimizer (L2).
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OuterProposal {
    /// Mode of operation: "explore", "exploit", "consolidate"
    pub mode: String,

    /// Justification for the chosen mode
    pub mode_justification: String,

    /// Confidence in this proposal (0.0 - 1.0)
    pub confidence: f64,

    /// Proposed edits to the promptgram
    pub edits: Vec<PromptEdit>,

    /// Expected effects of these edits
    pub expected_effects: Vec<String>,

    /// What we hope to learn from this experiment
    pub hypothesis: String,

    /// Risk assessment
    pub risk_level: String,
}

/// Accumulated state for the outer loop.
#[derive(Debug, Clone, Default, Serialize, Deserialize)]
pub struct OuterScratchpad {
    /// All outer episode summaries
    pub episodes: Vec<OuterEpisodeSummary>,

    /// Current best promptgram ID
    pub best_prompt_id: String,

    /// Best NDCG achieved
    pub best_ndcg: f64,

    /// Hall of fame: top N promptgrams by final NDCG
    pub hall_of_fame: Vec<(String, f64)>,

    /// Discovered prompt patterns that work
    pub success_patterns: Vec<String>,

    /// Prompt patterns that failed
    pub failure_patterns: Vec<String>,

    /// Meta-insights about promptgram optimization
    pub meta_insights: Vec<String>,
}

impl OuterScratchpad {
    /// Get the last N episode summaries.
    pub fn recent_episodes(&self, n: usize) -> Vec<&OuterEpisodeSummary> {
        self.episodes.iter().rev().take(n).collect()
    }

    /// Check if we're in a plateau (no improvement in last N episodes).
    pub fn is_plateau(&self, n: usize, threshold: f64) -> bool {
        let recent: Vec<_> = self.episodes.iter().rev().take(n).collect();
        if recent.len() < n {
            return false;
        }

        let first_ndcg = recent.last().map(|e| e.final_metrics.ndcg).unwrap_or(0.0);
        let last_ndcg = recent.first().map(|e| e.final_metrics.ndcg).unwrap_or(0.0);

        (last_ndcg - first_ndcg).abs() < threshold
    }

    /// Check if we're in collapse (NDCG degrading).
    pub fn is_collapse(&self, n: usize, threshold: f64) -> bool {
        let recent: Vec<_> = self.episodes.iter().rev().take(n).collect();
        if recent.len() < n {
            return false;
        }

        let first_ndcg = recent.last().map(|e| e.final_metrics.ndcg).unwrap_or(0.0);
        let last_ndcg = recent.first().map(|e| e.final_metrics.ndcg).unwrap_or(0.0);

        last_ndcg < first_ndcg - threshold
    }

    /// Search for failure patterns across all episodes.
    ///
    /// Returns episodes where the pattern appears in notable_failures or strategy_capsules.
    /// L2 can use this to scavenge its own history for recurring issues.
    pub fn search_failures(&self, pattern: &str) -> Vec<(usize, &OuterEpisodeSummary)> {
        let pattern_lower = pattern.to_lowercase();
        self.episodes
            .iter()
            .enumerate()
            .filter(|(_, ep)| {
                // Check notable failures
                ep.notable_failures.iter().any(|f| f.to_lowercase().contains(&pattern_lower))
                    // Check strategy capsules (sometimes failures appear here)
                    || ep.strategy_capsules.iter().any(|c| c.to_lowercase().contains(&pattern_lower))
                    // Check structural insights
                    || ep.structural_insights.iter().any(|i| i.to_lowercase().contains(&pattern_lower))
            })
            .collect()
    }

    /// Get episodes that used a specific promptgram.
    pub fn episodes_with_prompt(&self, prompt_id: &str) -> Vec<&OuterEpisodeSummary> {
        self.episodes
            .iter()
            .filter(|ep| ep.prompt_id == prompt_id)
            .collect()
    }

    /// Compute mean NDCG for a promptgram across all its runs.
    pub fn promptgram_stats(&self, prompt_id: &str) -> Option<PromptgramStats> {
        let episodes: Vec<_> = self.episodes_with_prompt(prompt_id);
        if episodes.is_empty() {
            return None;
        }

        let ndcgs: Vec<f64> = episodes.iter().map(|e| e.final_metrics.ndcg).collect();
        let mean_ndcg = ndcgs.iter().sum::<f64>() / ndcgs.len() as f64;
        let best_ndcg = ndcgs.iter().cloned().fold(0.0, f64::max);
        let worst_ndcg = ndcgs.iter().cloned().fold(1.0, f64::min);

        Some(PromptgramStats {
            prompt_id: prompt_id.to_string(),
            run_count: episodes.len(),
            mean_ndcg,
            best_ndcg,
            worst_ndcg,
            first_step: episodes.first().map(|e| e.outer_step).unwrap_or(0),
            last_step: episodes.last().map(|e| e.outer_step).unwrap_or(0),
        })
    }

    /// Get all unique promptgram IDs used so far.
    pub fn unique_promptgrams(&self) -> Vec<String> {
        let mut ids: Vec<String> = self.episodes
            .iter()
            .map(|e| e.prompt_id.clone())
            .collect();
        ids.sort();
        ids.dedup();
        ids
    }

    /// Find episodes where a specific meta-lever was extreme (>0.8 or <0.2).
    ///
    /// Useful for L2 to understand when certain strategies were tried.
    pub fn extreme_lever_episodes(&self, lever: &str) -> Vec<(usize, f64, &OuterEpisodeSummary)> {
        self.episodes
            .iter()
            .enumerate()
            .filter_map(|(i, ep)| {
                let ml = &ep.meta_levers_estimate;
                let value = match lever {
                    "structural_trust" => ml.structural_trust,
                    "temporal_horizon" => ml.temporal_horizon,
                    "exploration_bias" => ml.exploration_bias,
                    "depth_sensitivity" => ml.depth_sensitivity,
                    "hub_damping" => ml.hub_damping,
                    "focus_locality" => ml.focus_locality,
                    _ => return None,
                };
                if value > 0.8 || value < 0.2 {
                    Some((i, value, ep))
                } else {
                    None
                }
            })
            .collect()
    }
}

/// Statistics for a promptgram's performance across runs.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PromptgramStats {
    pub prompt_id: String,
    pub run_count: usize,
    pub mean_ndcg: f64,
    pub best_ndcg: f64,
    pub worst_ndcg: f64,
    pub first_step: usize,
    pub last_step: usize,
}

/// Configuration for an outer loop run.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OuterConfig {
    /// Number of inner episodes per outer step
    pub inner_episodes: usize,

    /// Which agent to use for L1
    pub inner_agent: String,

    /// Which agent to use for L2
    pub outer_agent: String,

    /// Maximum outer steps
    pub max_outer_steps: usize,

    /// Exploration quota: minimum fraction of steps in explore mode
    pub exploration_quota: f64,

    /// Corpus to use for inner runs
    pub corpus: String,

    /// Path to inner prompt template (contains placeholders like {current_ndcg:.4})
    /// If not set, the promptgram will be rendered and saved to the output directory.
    pub prompt_template_path: Option<String>,

    /// Enable L2 prompt editing. Without this, outer loop just records (Stage 0).
    pub edit_prompts: bool,

    /// Sections of the promptgram that can be edited
    pub editable_sections: Vec<String>,

    /// Sections that are immutable (safety/contract)
    pub immutable_sections: Vec<String>,
}

impl Default for OuterConfig {
    fn default() -> Self {
        OuterConfig {
            inner_episodes: 20,
            inner_agent: "claude".to_string(),
            outer_agent: "codex".to_string(),
            max_outer_steps: 50,
            exploration_quota: 0.2,
            corpus: "curated".to_string(),
            prompt_template_path: Some("training-outer/prompts/inner/v001.md".to_string()),
            edit_prompts: false,
            editable_sections: vec![
                "Policy".to_string(),
                "Heuristics".to_string(),
                "Style".to_string(),
            ],
            immutable_sections: vec![
                "Role".to_string(),
                "API_contract".to_string(),
                "Output_schema".to_string(),
            ],
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_meta_levers_summary() {
        let levers = MetaLevers::default();
        let s = levers.summary();
        assert!(s.contains("struct="));
        assert!(s.contains("explore="));
    }

    #[test]
    fn test_run_metrics_sub() {
        let a = RunMetrics { ndcg: 0.85, failures: 10, mean_confidence: 0.7 };
        let b = RunMetrics { ndcg: 0.80, failures: 15, mean_confidence: 0.6 };
        let delta = a - b;
        assert!((delta.ndcg - 0.05).abs() < 0.001);
    }

    #[test]
    fn test_outer_scratchpad_plateau() {
        let mut pad = OuterScratchpad::default();

        // Add 5 episodes with same NDCG
        for i in 0..5 {
            pad.episodes.push(OuterEpisodeSummary {
                outer_step: i,
                prompt_id: "test".to_string(),
                baseline_metrics: RunMetrics::default(),
                final_metrics: RunMetrics { ndcg: 0.85, failures: 5, mean_confidence: 0.7 },
                delta: RunMetrics::default(),
                stability: StabilityMetrics::default(),
                meta_levers_estimate: MetaLevers::default(),
                strategy_capsules: vec![],
                notable_failures: vec![],
                structural_insights: vec![],
                inner_episodes: 10,
                duration_secs: 60.0,
                timestamp: 0,
                proposal: None,
                selection_mode: "best".to_string(),
            });
        }

        assert!(pad.is_plateau(3, 0.01));
        assert!(!pad.is_collapse(3, 0.01));
    }

    #[test]
    fn test_search_failures() {
        let mut pad = OuterScratchpad::default();

        pad.episodes.push(OuterEpisodeSummary {
            outer_step: 1,
            prompt_id: "v001".to_string(),
            baseline_metrics: RunMetrics::default(),
            final_metrics: RunMetrics::default(),
            delta: RunMetrics::default(),
            stability: StabilityMetrics::default(),
            meta_levers_estimate: MetaLevers::default(),
            strategy_capsules: vec!["boost was too high".to_string()],
            notable_failures: vec!["depth penalty issue".to_string()],
            structural_insights: vec![],
            inner_episodes: 10,
            duration_secs: 60.0,
            timestamp: 0,
            proposal: None,
            selection_mode: "explore".to_string(),
        });

        // Search should find matches
        assert_eq!(pad.search_failures("depth").len(), 1);
        assert_eq!(pad.search_failures("boost").len(), 1);
        assert_eq!(pad.search_failures("nonexistent").len(), 0);
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 src/types.rs 
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

//! Core types for ripmap - the ultra-fast codebase cartographer.
//!
//! This module mirrors the Python grepmap types but optimized for Rust's
//! zero-cost abstractions. Key design decisions:
//! - `Cow<str>` for zero-copy string handling from memory-mapped files
//! - `Arc` for shared ownership of interned strings
//! - Frozen/immutable by default (like Python's frozen dataclasses)

use std::sync::Arc;
use serde::{Deserialize, Deserializer, Serialize, Serializer};

/// Serde serialization helpers for Arc<str> fields
mod arc_str_serde {
    use super::*;

    pub fn serialize<S>(arc: &Arc<str>, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        serializer.serialize_str(arc.as_ref())
    }

    pub fn deserialize<'de, D>(deserializer: D) -> Result<Arc<str>, D::Error>
    where
        D: Deserializer<'de>,
    {
        let s = String::deserialize(deserializer)?;
        Ok(s.into())
    }

    pub fn serialize_opt<S>(arc: &Option<Arc<str>>, serializer: S) -> Result<S::Ok, S::Error>
    where
        S: Serializer,
    {
        match arc {
            Some(s) => serializer.serialize_some(s.as_ref()),
            None => serializer.serialize_none(),
        }
    }

    pub fn deserialize_opt<'de, D>(deserializer: D) -> Result<Option<Arc<str>>, D::Error>
    where
        D: Deserializer<'de>,
    {
        let opt: Option<String> = Option::deserialize(deserializer)?;
        Ok(opt.map(|s| s.into()))
    }
}

/// Detail level for rendering - controls how much information to show.
/// Higher levels = more tokens but more context.
#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord, Hash)]
#[repr(u8)]
pub enum DetailLevel {
    /// Names only: "connect, disconnect, validate"
    Low = 1,
    /// Names + simplified types: "connect(host, port) -> bool"
    Medium = 2,
    /// Full signatures: "connect(host: &str, port: u16) -> Result<Connection>"
    High = 3,
}

impl DetailLevel {
    pub fn value(self) -> u8 {
        self as u8
    }
}

/// Function/method signature information.
/// Extracted from AST to enable detail-level rendering.
#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct SignatureInfo {
    /// Parameters as (name, optional_type) pairs
    pub parameters: Vec<(String, Option<String>)>,
    /// Return type annotation if present
    pub return_type: Option<String>,
    /// Decorators like @property, @staticmethod
    pub decorators: Vec<String>,
    /// Raw signature text for markdown previews
    pub raw: Option<String>,
}

impl SignatureInfo {
    pub fn new() -> Self {
        Self {
            parameters: Vec::new(),
            return_type: None,
            decorators: Vec::new(),
            raw: None,
        }
    }

    /// Render at a given detail level
    pub fn render(&self, detail: DetailLevel) -> String {
        match detail {
            DetailLevel::Low => {
                // Just parameter names, no types
                let names: Vec<_> = self.parameters.iter().map(|(n, _)| n.as_str()).collect();
                format!("({})", names.join(", "))
            }
            DetailLevel::Medium => {
                // Names with simplified types (strip generics)
                let parts: Vec<_> = self
                    .parameters
                    .iter()
                    .map(|(name, typ)| {
                        if let Some(t) = typ {
                            let simple = simplify_type(t);
                            format!("{}: {}", name, simple)
                        } else {
                            name.clone()
                        }
                    })
                    .collect();
                let ret = self.return_type.as_ref().map(|t| format!(" -> {}", simplify_type(t))).unwrap_or_default();
                format!("({}){}", parts.join(", "), ret)
            }
            DetailLevel::High => {
                // Full signature
                let parts: Vec<_> = self
                    .parameters
                    .iter()
                    .map(|(name, typ)| {
                        if let Some(t) = typ {
                            format!("{}: {}", name, t)
                        } else {
                            name.clone()
                        }
                    })
                    .collect();
                let ret = self.return_type.as_ref().map(|t| format!(" -> {}", t)).unwrap_or_default();
                format!("({}){}", parts.join(", "), ret)
            }
        }
    }
}

impl Default for SignatureInfo {
    fn default() -> Self {
        Self::new()
    }
}

/// Simplify a type for MEDIUM detail level.
/// Strips generics and long paths, keeps core type name.
fn simplify_type(t: &str) -> &str {
    // Strip generic params: "Dict[str, int]" -> "Dict"
    if let Some(pos) = t.find('[') {
        return &t[..pos];
    }
    if let Some(pos) = t.find('<') {
        return &t[..pos];
    }
    // Strip module path: "std::collections::HashMap" -> "HashMap"
    if let Some(pos) = t.rfind("::") {
        return &t[pos + 2..];
    }
    if let Some(pos) = t.rfind('.') {
        return &t[pos + 1..];
    }
    t
}

/// Class/struct field information.
#[derive(Debug, Clone, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub struct FieldInfo {
    pub name: String,
    pub type_annotation: Option<String>,
    pub default_value: Option<String>,
}

impl FieldInfo {
    pub fn render(&self, detail: DetailLevel) -> String {
        match detail {
            DetailLevel::Low => self.name.clone(),
            DetailLevel::Medium => {
                if let Some(t) = &self.type_annotation {
                    format!("{}: {}", self.name, simplify_type(t))
                } else {
                    self.name.clone()
                }
            }
            DetailLevel::High => {
                let typ = self.type_annotation.as_deref().unwrap_or("?");
                if let Some(default) = &self.default_value {
                    format!("{}: {} = {}", self.name, typ, default)
                } else {
                    format!("{}: {}", self.name, typ)
                }
            }
        }
    }
}

/// The fundamental unit of code structure - a symbol tag.
/// Represents either a definition ("def") or reference ("ref").
///
/// This is the atom from which all ranking and rendering is built.
/// Frozen/immutable to enable safe sharing across threads.
#[derive(Debug, Clone, PartialEq, Eq, Serialize, Deserialize)]
pub struct Tag {
    /// Relative path for display (e.g., "src/lib.rs")
    #[serde(with = "arc_str_serde")]
    pub rel_fname: Arc<str>,
    /// Absolute path for I/O operations
    #[serde(with = "arc_str_serde")]
    pub fname: Arc<str>,
    /// Line number (1-indexed)
    pub line: u32,
    /// Symbol name (function, class, variable name)
    #[serde(with = "arc_str_serde")]
    pub name: Arc<str>,
    /// "def" for definition, "ref" for reference
    pub kind: TagKind,
    /// AST node type: "function", "class", "method", "call", etc.
    #[serde(with = "arc_str_serde")]
    pub node_type: Arc<str>,
    /// Enclosing scope's name (class or function containing this symbol)
    #[serde(serialize_with = "arc_str_serde::serialize_opt", deserialize_with = "arc_str_serde::deserialize_opt")]
    pub parent_name: Option<Arc<str>>,
    /// Line of the enclosing scope
    pub parent_line: Option<u32>,
    /// Function signature info (for functions/methods)
    pub signature: Option<SignatureInfo>,
    /// Class fields (for classes/structs)
    pub fields: Option<Vec<FieldInfo>>,
    /// Additional metadata from tree-sitter captures (type hints, receivers, etc.)
    /// Keys depend on language and query: "receiver", "var_type", "import_module", etc.
    pub metadata: Option<std::collections::HashMap<String, String>>,
}

/// Tag kind - definition or reference
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum TagKind {
    /// Symbol definition (function def, class def, variable assignment)
    Def,
    /// Symbol reference (function call, variable use)
    Ref,
}

impl TagKind {
    /// Check if this is any kind of definition
    pub fn is_definition(&self) -> bool {
        matches!(self, TagKind::Def)
    }

    /// Check if this is any kind of reference
    pub fn is_reference(&self) -> bool {
        matches!(self, TagKind::Ref)
    }
}

impl Tag {
    /// Check if this is a definition tag
    pub fn is_def(&self) -> bool {
        matches!(self.kind, TagKind::Def)
    }

    /// Check if this is a reference tag
    pub fn is_ref(&self) -> bool {
        matches!(self.kind, TagKind::Ref)
    }
}

/// A tag with its computed importance rank.
/// The rank is a combination of PageRank score and contextual boosts.
#[derive(Debug, Clone)]
pub struct RankedTag {
    /// Combined importance score (PageRank Ã— boosts)
    pub rank: f64,
    /// The underlying tag
    pub tag: Tag,
}

impl RankedTag {
    pub fn new(rank: f64, tag: Tag) -> Self {
        Self { rank, tag }
    }
}

/// Ordering by rank (descending - highest rank first)
impl PartialEq for RankedTag {
    fn eq(&self, other: &Self) -> bool {
        self.rank == other.rank
    }
}

impl Eq for RankedTag {}

impl PartialOrd for RankedTag {
    fn partial_cmp(&self, other: &Self) -> Option<std::cmp::Ordering> {
        Some(self.cmp(other))
    }
}

impl Ord for RankedTag {
    fn cmp(&self, other: &Self) -> std::cmp::Ordering {
        // Reverse order - higher rank comes first
        other.rank.partial_cmp(&self.rank).unwrap_or(std::cmp::Ordering::Equal)
    }
}

/// Configuration for the ranking system.
/// All values are tunable at runtime for experimentation.
#[derive(Debug, Clone)]
pub struct RankingConfig {
    // PageRank settings
    pub pagerank_alpha: f64,
    pub pagerank_chat_multiplier: f64,

    // Depth weights for personalization
    pub depth_weight_root: f64,
    pub depth_weight_moderate: f64,
    pub depth_weight_deep: f64,
    pub depth_weight_vendor: f64,
    pub depth_threshold_shallow: usize,
    pub depth_threshold_moderate: usize,

    // Boost multipliers
    pub boost_mentioned_ident: f64,
    pub boost_mentioned_file: f64,
    pub boost_chat_file: f64,
    pub boost_temporal_coupling: f64,
    pub boost_focus_expansion: f64,
    pub boost_caller_weight: f64,

    // Call graph / focus expansion settings
    pub focus_expansion_max_hops: usize,
    pub focus_expansion_decay: f64,

    // Testâ†”source coupling settings
    // Codex optimization identified this as a missing architectural feature:
    // "path-aware testâ†”crate coupling edges" for surfacing related test files
    pub boost_test_coupling: f64,
    pub test_coupling_min_confidence: f64,

    // Hub damping: penalizes "hub" nodes with excessive incoming edges.
    // Codex identified "degree-normalized hub damping" as needed to prevent
    // utility functions (log(), print()) from dominating rankings.
    // Values:
    //   0.0 = no damping (heavily-called functions get full boost)
    //   1.0 = full damping (caller count is neutralized)
    //   >1.0 = penalty (heavily-called functions are penalized as noise)
    pub hub_damping: f64,

    // Git settings
    pub git_recency_decay_days: f64,
    pub git_recency_max_boost: f64,
    pub git_churn_threshold: usize,
    pub git_churn_max_boost: f64,
    pub git_author_boost: f64,
    pub git_badge_recent_days: u32,
    pub git_badge_churn_commits: usize,

    // Lifecycle phase thresholds
    pub phase_crystal_min_age_days: u32,
    pub phase_crystal_min_quiet_days: u32,
    pub phase_rotting_min_age_days: u32,
    pub phase_rotting_max_quiet_days: u32,
    pub phase_rotting_churn_multiplier: f64,
    pub phase_emergent_max_age_days: u32,

    // Vendor patterns
    pub vendor_patterns: Vec<String>,
}

impl Default for RankingConfig {
    fn default() -> Self {
        Self {
            // PageRank
            pagerank_alpha: 0.85,
            pagerank_chat_multiplier: 100.0,

            // Depth weights
            depth_weight_root: 1.0,
            depth_weight_moderate: 0.5,
            depth_weight_deep: 0.1,
            depth_weight_vendor: 0.01,
            depth_threshold_shallow: 2,
            depth_threshold_moderate: 4,

            // Boosts
            boost_mentioned_ident: 10.0,
            boost_mentioned_file: 5.0,
            boost_chat_file: 20.0,
            boost_temporal_coupling: 3.0,
            boost_focus_expansion: 5.0,
            boost_caller_weight: 2.0,  // Files with heavily-called functions

            // Call graph / focus expansion
            focus_expansion_max_hops: 2,  // BFS depth through call relationships
            focus_expansion_decay: 0.5,   // Weight decay per hop (0.5 = halve each hop)

            // Testâ†”source coupling
            boost_test_coupling: 5.0,      // Test files boost their source files
            test_coupling_min_confidence: 0.5, // Minimum pattern match confidence

            // Hub damping: balance between "called = important" and "utility = noise"
            // 0.0 = no damping (heavily-called functions are boosted)
            // 1.0 = neutralize caller boost entirely
            // Future: Codex may tune this based on repo characteristics
            hub_damping: 0.0,

            // Git
            git_recency_decay_days: 30.0,
            git_recency_max_boost: 10.0,
            git_churn_threshold: 5,
            git_churn_max_boost: 6.0,
            git_author_boost: 1.5,
            git_badge_recent_days: 7,
            git_badge_churn_commits: 10,

            // Lifecycle phases
            phase_crystal_min_age_days: 180,
            phase_crystal_min_quiet_days: 30,
            phase_rotting_min_age_days: 90,
            phase_rotting_max_quiet_days: 14,
            phase_rotting_churn_multiplier: 1.5,
            phase_emergent_max_age_days: 30,

            // Vendor
            vendor_patterns: vec![
                "node_modules".into(),
                "vendor".into(),
                "third_party".into(),
                "__pycache__".into(),
                "site-packages".into(),
                ".git".into(),
                "target".into(),
            ],
        }
    }
}

/// File lifecycle phase - indicates maturity and stability.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash)]
pub enum FilePhase {
    /// Old and stable - settled code, safe to rely on
    Crystal,
    /// Old but recently churning - tech debt surfacing
    Rotting,
    /// New file - still finding its shape
    Emergent,
    /// Normal development - actively being worked on
    Evolving,
}

impl FilePhase {
    pub fn badge(&self) -> &'static str {
        match self {
            FilePhase::Crystal => "crystal",
            FilePhase::Rotting => "rotting",
            FilePhase::Emergent => "emergent",
            FilePhase::Evolving => "evolving",
        }
    }
}

/// Intent classification for focus-aware ranking.
/// Different intents get different ranking recipes.
#[derive(Debug, Clone, Copy, PartialEq, Eq, Hash, Serialize, Deserialize)]
pub enum Intent {
    /// Debugging - favor callers, recent changes
    Debug,
    /// Exploring - neutral graph structure
    Explore,
    /// Extending - favor API surfaces
    Extend,
    /// Refactoring - favor high-churn code
    Refactor,
}

impl Intent {
    /// Get the ranking recipe for this intent
    pub fn recipe(&self) -> IntentRecipe {
        match self {
            Intent::Debug => IntentRecipe {
                recency_weight: 1.5,
                churn_weight: 0.8,
                reverse_edge_bias: 2.0,
            },
            Intent::Explore => IntentRecipe {
                recency_weight: 1.0,
                churn_weight: 1.0,
                reverse_edge_bias: 1.0,
            },
            Intent::Extend => IntentRecipe {
                recency_weight: 1.2,
                churn_weight: 0.5,
                reverse_edge_bias: 0.7,
            },
            Intent::Refactor => IntentRecipe {
                recency_weight: 0.8,
                churn_weight: 2.0,
                reverse_edge_bias: 1.0,
            },
        }
    }
}

/// Ranking weights for a given intent
#[derive(Debug, Clone, Copy)]
pub struct IntentRecipe {
    pub recency_weight: f64,
    pub churn_weight: f64,
    pub reverse_edge_bias: f64,
}

/// Symbol identifier - (file, symbol_name) tuple for graph nodes
pub type SymbolId = (Arc<str>, Arc<str>);

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_detail_levels() {
        assert!(DetailLevel::Low < DetailLevel::Medium);
        assert!(DetailLevel::Medium < DetailLevel::High);
    }

    #[test]
    fn test_signature_render() {
        let sig = SignatureInfo {
            parameters: vec![
                ("host".into(), Some("str".into())),
                ("port".into(), Some("int".into())),
            ],
            return_type: Some("bool".into()),
            decorators: vec![],
            raw: None,
        };

        assert_eq!(sig.render(DetailLevel::Low), "(host, port)");
        assert_eq!(sig.render(DetailLevel::Medium), "(host: str, port: int) -> bool");
        assert_eq!(sig.render(DetailLevel::High), "(host: str, port: int) -> bool");
    }

    #[test]
    fn test_simplify_type() {
        assert_eq!(simplify_type("Dict[str, int]"), "Dict");
        assert_eq!(simplify_type("Vec<String>"), "Vec");
        assert_eq!(simplify_type("std::collections::HashMap"), "HashMap");
        assert_eq!(simplify_type("collections.OrderedDict"), "OrderedDict");
        assert_eq!(simplify_type("int"), "int");
    }

    #[test]
    fn test_ranked_tag_ordering() {
        let tag1 = Tag {
            rel_fname: "a.rs".into(),
            fname: "/a.rs".into(),
            line: 1,
            name: "foo".into(),
            kind: TagKind::Def,
            node_type: "function".into(),
            parent_name: None,
            parent_line: None,
            signature: None,
            fields: None,
            metadata: None,
        };
        let tag2 = tag1.clone();

        let ranked1 = RankedTag::new(0.5, tag1);
        let ranked2 = RankedTag::new(0.8, tag2);

        // Higher rank should come first
        assert!(ranked2 < ranked1);
    }

    #[test]
    fn test_tag_kind_helpers() {
        assert!(TagKind::Def.is_definition());
        assert!(!TagKind::Def.is_reference());
        assert!(TagKind::Ref.is_reference());
        assert!(!TagKind::Ref.is_definition());
    }
}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
